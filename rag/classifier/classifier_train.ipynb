{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a855791-aa19-49dd-ac5d-62a4112e1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62a387e5-0079-41b2-9dfe-8e63dcc76d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainingData:\n",
    "    def __init__(self, context: str, question: str, is_correct: bool):\n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.is_correct = is_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cd852ff-0cfb-41cc-afc3-8e0d8e2b7ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "# model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "944466f0-c394-429c-a47f-7d3fe074f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30534\n",
      "Remote prognosis of preeclampsia in women 25 years old and younger. Twenty-six primiparous women less than or equal to 25 years old who delivered between 1963 and 1978 and met the following criteria were studied: (1) The first obstetric visit was at less than or equal to 30 weeks' gestation with diastolic blood pressure less than or equal to 85 mm Hg; (2) diastolic blood pressure before delivery was greater than or equal to 95-mm Hg, rising at least 15 mm Hg; (3) 24-hour urine protein measured at least 1.0 gm; and (4) patients were enrolled in the Kaiser Health Plan in 1982. Each of the 26 patients was paired with a primiparous woman by year delivered, age, race (black versus nonblack), and weight +/- 1/3. In 23 pairs blood pressure follow-up was available in 1980 or thereafter, which was at least 3 1/2 years after delivery (average interval between delivery and follow-up = 10 years). Three in the preeclampsia group and two in the matched control group had hypertension, with diastolic blood pressure greater than or equal to 90 mm Hg. Mean systolic blood pressure (123.7 mm Hg) was 9.3 mm Hg higher in the preeclampsia group than in the matched controls (114.4 mm Hg), with a standard error of 4.15 mm Hg (p = 0.04). Mean diastolic blood pressure (77.0 mm Hg) was 2.9 mm Hg higher in the preeclampsia group than in the controls (74.1 mm Hg), with a standard error of 3.0 mm Hg (p = 0.345). We conclude that no significant difference in frequency of hypertension or diastolic blood pressure and a small but significant difference in systolic blood pressure is shown in follow-up between patients with preeclampsia and matched controls. True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "QUESTION_COUNT = 10178\n",
    "\n",
    "log_file_list = [\n",
    "    \"9-20-train_150_30_1st_H100.txt\", \n",
    "    # \"9-20-train_150_30_2nd_H100.txt\", \n",
    "    # \"9-21-train_150_30_3rd_H100.txt\"\n",
    "                ]\n",
    "# log_file = \"9-20-train_150_30_2nd_H100.txt\"\n",
    "\n",
    "path = \"../../notebooks/\"\n",
    "\n",
    "file_name_list = [path + log_file for log_file in log_file_list]\n",
    "\n",
    "training_data_list = []\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    with open(file_name) as f:\n",
    "        is_reading_context = False\n",
    "        context = \"\"\n",
    "    \n",
    "        is_reading_question = False\n",
    "        question = \"\"\n",
    "    \n",
    "        for line in f:\n",
    "            # if line == \"\\n\":\n",
    "            #     continue\n",
    "    \n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line == \"Context:\":\n",
    "                is_reading_context = True\n",
    "            elif stripped_line == \"Question:\":\n",
    "                is_reading_context = False\n",
    "                is_reading_question = True\n",
    "            elif stripped_line.startswith(\"[output]\"):\n",
    "                is_correct = stripped_line[len(\"[output]\"):] == \"True\"\n",
    "                context = context.strip()\n",
    "                question = question.strip()\n",
    "                data = TrainingData(context, question, is_correct)\n",
    "                # print(\"aaa\", context, question, is_correct)\n",
    "    \n",
    "                training_data_list.append(data)\n",
    "    \n",
    "                # if len(training_data_list) % 100 == 0:\n",
    "                #     print(f\"finished processing {len(training_data_list)} data\")\n",
    "    \n",
    "                # clear\n",
    "                is_reading_context = False\n",
    "                context = \"\"\n",
    "                is_reading_question = False\n",
    "                question = \"\"\n",
    "                # break\n",
    "            else:\n",
    "                if is_reading_context:\n",
    "                    context += line\n",
    "                elif is_reading_question:\n",
    "                    question += line\n",
    "\n",
    "print(len(training_data_list))\n",
    "print(training_data_list[6].context, training_data_list[6].is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc325416-2ebd-448a-af6e-cbb64d57db01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainingData' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m encodings = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m(\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: tokenizer(row[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m], truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      3\u001b[39m     axis=\u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m lengths = encodings.apply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(lengths.describe())\n",
      "\u001b[31mAttributeError\u001b[39m: 'TrainingData' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "encodings = data.apply(\n",
    "    lambda row: tokenizer(row[\"context\"], row[\"question\"], truncation=False)[\"input_ids\"],\n",
    "    axis=1\n",
    ")\n",
    "lengths = encodings.apply(len)\n",
    "\n",
    "print(lengths.describe())\n",
    "print(\"90th percentile length:\", lengths.quantile(0.9))\n",
    "print(\"Max length:\", lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fa5d205-a457-454e-93ed-7af70b86ec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_len 2132.0\n",
      "8714 8714\n"
     ]
    }
   ],
   "source": [
    "# Force set data to be balanced\n",
    "adapted_data_list = []\n",
    "incorrect_count = 0\n",
    "for data in training_data_list:\n",
    "    if data.is_correct == False:\n",
    "        incorrect_count += 1\n",
    "\n",
    "counter = 0\n",
    "for data in training_data_list:\n",
    "    if data.is_correct == False:\n",
    "        adapted_data_list.append(data)\n",
    "    else:\n",
    "        if counter < incorrect_count + 0:\n",
    "            adapted_data_list.append(data)\n",
    "            counter += 1\n",
    "\n",
    "training_data_list = adapted_data_list\n",
    "\n",
    "\n",
    "# Change the label based on length\n",
    "lengths = []\n",
    "for data in training_data_list:\n",
    "    lengths.append(len(data.context) + len(data.question))\n",
    "import statistics\n",
    "median_len = statistics.median(lengths)\n",
    "print(\"median_len\", median_len)\n",
    "\n",
    "# for data in training_data_list:\n",
    "#     if (len(data.context) + len(data.question)) > median_len:\n",
    "#         data.is_correct = True\n",
    "#     else:\n",
    "#         data.is_correct = False\n",
    "    \n",
    "\n",
    "false_count = 0\n",
    "true_count = 0\n",
    "for data in training_data_list:\n",
    "    if data.is_correct:\n",
    "        true_count += 1\n",
    "    else:\n",
    "        false_count += 1\n",
    "print(false_count, true_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4179036-f45c-4f78-a304-af7fe770b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\" I never actually thought that a film could be so great , but alas I was wrong . Great acting , great plot , fun effects . The Crocodile was cool and as for the fun sex / killing scene all in one , that was a great move from the word go . It was truly shocking and that is a compliment ! How can someone make this film , watch it back and then actually say \"\" \"\" Yeah , my favorite movie . People will watch that \"\" \"\" If you have n't seen it I beg you watch it . \"\"\"\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "filename = \"./data/cfimdb-train.txt\"\n",
    "\n",
    "training_data_list = []\n",
    "\n",
    "with open(filename, 'r') as fp:\n",
    "    for line in fp:\n",
    "        label, org_sent = line.split(' ||| ')\n",
    "        # sent = org_sent.lower().strip()\n",
    "        # tokens = tokenizer.tokenize(\"[CLS] \" + sent + \" [SEP]\")\n",
    "        label = int(label.strip())\n",
    "        # if label not in num_labels:\n",
    "        #     num_labels[label] = len(num_labels)\n",
    "        data = TrainingData(org_sent, \"\", label)\n",
    "        training_data_list.append(data)\n",
    "\n",
    "print(training_data_list[6].context, training_data_list[6].is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "262fb3be-46cb-413a-af94-6263ad3fd066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12199 2614 2615\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose training_data_list is your list of TrainingData objects\n",
    "data = pd.DataFrame([{\n",
    "    \"context\": d.context,\n",
    "    \"question\": d.question,\n",
    "    \"label\": int(d.is_correct)   # True->1, False->0\n",
    "} for d in training_data_list])\n",
    "\n",
    "# Train/val/test split (e.g., 70/15/15)\n",
    "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42, stratify=data[\"label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"label\"])\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "# print(test_df.iloc[0][\"context\"])\n",
    "# print(test_df.iloc[0][\"question\"])\n",
    "print(test_df.iloc[1][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9622e517-6e51-4ef2-b918-2084d49e3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_LEN = 512 #512\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=MAX_LEN):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.data.loc[idx, \"context\"]\n",
    "        question = self.data.loc[idx, \"question\"]\n",
    "        label = self.data.loc[idx, \"label\"]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            context,\n",
    "            question,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "348b73ad-cb3b-41bd-ad2c-d05cde94d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(train_df, tokenizer)\n",
    "val_dataset = QADataset(val_df, tokenizer)\n",
    "test_dataset = QADataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fd059b2-9f45-4225-b8fa-003cfd40a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4962, 4786, 9033, 11475, 12269, 2483, 2828, 1045, 3378, 2007, 10256, 2000, 4895, 13102, 8586, 18513, 16012, 15869, 1998, 4372, 9096, 12644, 9556, 1012, 1048, 7274, 27642, 5527, 7870, 1006, 1048, 16150, 1007, 2411, 19676, 2007, 9115, 2417, 6097, 7934, 7516, 1012, 11616, 2003, 2241, 2006, 6612, 2838, 1998, 3563, 16012, 15869, 1998, 4372, 9096, 12644, 7060, 1012, 1999, 9662, 3572, 1010, 7403, 5604, 2007, 2279, 4245, 24558, 2064, 5323, 1037, 11616, 1010, 2926, 1999, 10256, 2121, 2030, 2012, 22571, 7476, 6887, 16515, 13874, 2015, 1012, 2057, 3189, 2006, 1996, 16474, 2147, 1011, 2039, 1999, 1037, 2879, 2007, 9033, 11475, 12269, 2483, 2828, 1045, 1010, 10886, 3322, 2007, 4417, 9115, 2417, 6097, 7934, 7516, 2021, 2512, 1011, 3563, 24471, 3981, 2854, 19330, 14031, 3736, 9468, 18428, 3207, 7060, 1998, 12890, 10256, 4654, 16748, 3508, 1997, 5391, 9033, 27072, 5648, 1012, 16012, 15869, 1010, 4372, 9096, 12644, 1998, 7403, 5852, 2020, 2864, 1999, 1996, 5776, 1012, 1996, 6612, 1998, 16175, 21281, 20763, 9966, 2951, 2001, 8182, 1998, 1037, 8991, 26305, 1011, 6887, 16515, 13874, 4106, 2001, 2864, 1012, 1999, 2804, 1037, 11778, 3906, 3319, 2001, 3344, 2041, 1012, 9115, 2417, 6097, 7934, 7516, 2020, 2034, 3264, 2012, 1020, 2050, 2086, 1997, 2287, 2044, 9410, 11326, 2026, 7361, 2401, 1012, 3558, 7749, 1010, 18224, 12589, 5604, 1010, 5911, 9751, 2004, 2092, 2004, 18439, 27011, 2020, 4895, 28578, 17007, 3085, 2012, 1023, 2050, 2086, 1997, 2287, 1012, 2061, 2521, 2053, 6612, 2026, 10085, 7811, 2594, 25750, 4158, 1010, 2021, 25212, 2290, 8834, 18960, 4958, 9463, 20746, 11889, 2015, 1998, 5107, 23408, 23461, 4022, 2015, 2024, 15330, 17758, 2135, 1012, 17996, 4857, 6741, 10381, 21716, 10610, 12565, 3662, 2019, 19330, 14031, 3736, 9468, 18428, 3207, 5418, 11892, 2007, 2367, 1048, 16150, 2164, 9033, 11475, 12269, 2483, 1010, 16122, 6593, 20049, 11475, 12269, 2483, 1010, 13938, 2487, 6080, 12798, 5332, 12269, 2483, 2030, 14163, 3597, 18155, 7274, 6305, 7507, 14615, 12650, 2828, 4921, 1038, 1012, 24471, 3981, 2854, 5391, 9033, 27072, 5648, 4654, 16748, 3508, 2001, 19499, 8319, 1999, 17630, 1998, 2484, 2050, 1044, 17996, 8168, 1012, 1999, 3226, 2094, 10882, 12618, 28522, 12837, 1010, 1045, 29657, 1011, 9033, 11475, 8883, 2063, 4023, 2001, 29295, 10548, 2000, 1004, 8318, 1025, 1037, 1015, 1003, 1025, 2174, 1010, 5391, 1998, 2489, 9033, 27072, 5648, 2020, 2306, 3671, 2846, 1012, 11616, 2001, 2776, 2511, 2011, 4800, 102, 2019, 2324, 1011, 3204, 1011, 2214, 2879, 1997, 6683, 7520, 16103, 1011, 3644, 6934, 7534, 2007, 3279, 1997, 13908, 19199, 2015, 1012, 2006, 1051, 15431, 11360, 1010, 1037, 9115, 1011, 2417, 6097, 7934, 3962, 2003, 5159, 1012, 2053, 2002, 4502, 20389, 29107, 2135, 2003, 5159, 2006, 3558, 11360, 1012, 26396, 11360, 3065, 1048, 7274, 19137, 7834, 2007, 20949, 1011, 3096, 3311, 1012, 2054, 2003, 1996, 2087, 3497, 10318, 16012, 15869, 19470, 3012, 1029, 1031, 1037, 1033, 1024, 20299, 1997, 8292, 6444, 5178, 13012, 5369, 2595, 20049, 3207, 1031, 1038, 1033, 1024, 20299, 1997, 11867, 12053, 16940, 18809, 1031, 1039, 1033, 1024, 20299, 1997, 13938, 2475, 6080, 12798, 7363, 1031, 1040, 1033, 1024, 20299, 1997, 1043, 7630, 3597, 17119, 15878, 7352, 5178, 102]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[9][\"input_ids\"].tolist())\n",
    "# print(len(train_dataset[2][\"input_ids\"].tolist()))\n",
    "# print(train_dataset[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bc2616a-d027-4986-8215-3a39d64c774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2515286/3899404171.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6daf404-ecf4-44d3-a3c9-8a368a88ac97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6100' max='6100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6100/6100 04:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.693233</td>\n",
       "      <td>0.527544</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6100, training_loss=0.7007306108318392, metrics={'train_runtime': 247.392, 'train_samples_per_second': 49.31, 'train_steps_per_second': 24.657, 'total_flos': 3209691764336640.0, 'train_loss': 0.7007306108318392, 'epoch': 1.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35700d5c-db2e-4319-a4a1-b3a8e7acd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "# model_name = \"google-bert/bert-base-uncased\"\n",
    "# model_name = \"allenai/longformer-base-4096\"\n",
    "model_name = \"./results/checkpoint-3562\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff9357a5-391c-466f-bed7-517daf0588d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 0.14610586, -0.17824781],\n",
      "       [ 0.0151444 , -0.02443928],\n",
      "       [-0.18617228,  0.16418293],\n",
      "       ...,\n",
      "       [ 0.00196332, -0.01690367],\n",
      "       [ 0.20912941, -0.25593695],\n",
      "       [ 0.03775398, -0.04240683]], shape=(2615, 2), dtype=float32), label_ids=array([0, 0, 1, ..., 1, 1, 0], shape=(2615,)), metrics={'test_loss': 0.6902401447296143, 'test_accuracy': 0.5281070745697897, 'test_f1': 0.5315110098709187, 'test_runtime': 18.1206, 'test_samples_per_second': 144.311, 'test_steps_per_second': 72.183})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.52      0.52      1308\n",
      "           1       0.53      0.54      0.53      1307\n",
      "\n",
      "    accuracy                           0.53      2615\n",
      "   macro avg       0.53      0.53      0.53      2615\n",
      "weighted avg       0.53      0.53      0.53      2615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "print(preds)\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "y_true = preds.label_ids\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c4f2d9-1a80-4d0d-aa99-c9778e7b6f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(512) tensor(0) 0\n",
      "tensor(512) tensor(0) 0\n",
      "tensor(450) tensor(1) 1\n",
      "tensor(493) tensor(0) 0\n",
      "tensor(506) tensor(0) 1\n",
      "tensor(290) tensor(0) 1\n",
      "tensor(481) tensor(1) 1\n",
      "tensor(391) tensor(1) 1\n",
      "tensor(493) tensor(1) 1\n",
      "tensor(386) tensor(1) 1\n",
      "tensor(512) tensor(1) 0\n",
      "tensor(512) tensor(0) 0\n",
      "tensor(512) tensor(0) 1\n",
      "tensor(512) tensor(0) 0\n",
      "tensor(512) tensor(0) 1\n",
      "tensor(512) tensor(0) 0\n",
      "tensor(512) tensor(1) 1\n",
      "tensor(469) tensor(0) 1\n",
      "tensor(387) tensor(1) 1\n",
      "tensor(384) tensor(1) 1\n",
      "1327 / 2615 0.5074569789674952\n",
      "1307 / 2615 0.49980879541108986\n"
     ]
    }
   ],
   "source": [
    "y_preds = preds.predictions.argmax(-1)\n",
    "count_predict_1 = 0\n",
    "count_label_1 = 0\n",
    "for i in range(len(test_dataset)):\n",
    "    if i < 20:\n",
    "        print(sum(test_dataset[i][\"attention_mask\"]), test_dataset[i][\"labels\"], y_preds[i])\n",
    "    count_predict_1 += y_preds[i]\n",
    "    count_label_1 += test_dataset[i][\"labels\"].item()\n",
    "\n",
    "print(count_predict_1, \"/\", len(test_dataset), count_predict_1 / len(test_dataset))\n",
    "print(count_label_1, \"/\", len(test_dataset), count_label_1 / len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6108352-60d6-41fb-ad7e-5a3b09584162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
