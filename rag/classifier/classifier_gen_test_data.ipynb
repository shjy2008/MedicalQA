{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a855791-aa19-49dd-ac5d-62a4112e1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c582fcf-e5f2-4e40-9446-db77fbfe3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_choices(choices, answer_key, mask_correct_answer):\n",
    "    final_answers = []\n",
    "    for i, (x, y) in enumerate(choices.items()):\n",
    "        if mask_correct_answer and x == answer_key:\n",
    "            y = \"None of the other answers\"\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f43434-0d4c-4f5b-a48c-a623c7f27903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_RAG_context(query, formated_choices, score_threshold = None, pick_rag_index = None, use_classifier = False):\n",
    "    ip = \"localhost\"\n",
    "    port = 8080\n",
    "    endpoint = \"search\"\n",
    "    response = requests.get(f\"http://{ip}:{port}/{endpoint}\", params = {\"q\": query, \"k\": self.topK_searchEngine})\n",
    "    if response.status_code == 200:\n",
    "\n",
    "\n",
    "        # 1. BM25\n",
    "        # text = response.text\n",
    "        # doc_list = text.split(\"###RAG_DOC###\")\n",
    "        # for result in results:\n",
    "        results = response.json() # [{docId, docNo, score, content}, ...]\n",
    "        doc_data_list = []\n",
    "        for i in range(len(results)):\n",
    "            result = results[i]\n",
    "            data = {\"docId\": result[\"docNo\"], \"BM25_score\": result[\"score\"], \"BM25_ranking\": i + 1, \"content\": result[\"content\"]}\n",
    "            doc_data_list.append(data)\n",
    "        \n",
    "        # print(f\"1 len(doc_list): {len(doc_list)}\")\n",
    "\n",
    "        # 2. SPLADE\n",
    "        if topK_SPLADE > 0:\n",
    "            doc_data_list = self.RAG_SPLADE_filter(query, doc_data_list)\n",
    "        # print(f\"2 len(doc_list): {len(doc_list)}\")\n",
    "        \n",
    "\n",
    "        # 3. MonoT5\n",
    "        if topK_crossEncoder > 0:\n",
    "            # doc_data_list = self.RAG_CrossEncoder_rerank(query + '\\n' + formated_choices, doc_data_list)\n",
    "            doc_data_list = self.RAG_MonoT5_rerank(query + '\\n' + formated_choices, doc_data_list, score_threshold)\n",
    "            # print(f\"3 len(doc_list): {len(doc_list)}\")            \n",
    "\n",
    "        # 4. LLM list reranker\n",
    "        if topK_LLM > 0:\n",
    "            doc_data_list = self.RAG_LLM_rerank(query + '\\n' + formated_choices, doc_data_list)\n",
    "        \n",
    "        # only feed the nth retrieved document into the model\n",
    "        if pick_rag_index != None:\n",
    "            doc_data_list = [doc_data_list[pick_rag_index]]\n",
    "\n",
    "        # Use a classifier model to select which context+query can produce correct answer\n",
    "        if use_classifier:\n",
    "            doc_data_list = self.RAG_classifier(query + '\\n' + formated_choices, doc_data_list)\n",
    "            \n",
    "        # doc_list -> context (str)\n",
    "        context = \"\"\n",
    "        for doc_data in doc_data_list:\n",
    "            if self.check_RAG_doc_useful(query, formated_choices, doc_data):\n",
    "                context += doc_data[\"content\"]\n",
    "                context += \"\\n\\n\"\n",
    "            \n",
    "        # Print RAG scores and rankings\n",
    "        for doc_data in doc_data_list:\n",
    "            del doc_data[\"content\"]\n",
    "        print(f\"RAG data: {doc_data_list}\")\n",
    "        logging.info(f\"RAG data: {doc_data_list}\")\n",
    "        \n",
    "        return context\n",
    "    else:\n",
    "        return f\"HTTPError: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1209e95a-5170-458f-92ca-be201fa53878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'GBaker/MedQA-USMLE-4-options' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_RAG = '''\n",
    "You are a medical question answering assistant.\n",
    "\n",
    "The following context may or may not be useful. Use it only if it helps answer the question.\n",
    "INSTRUCTIONS:\n",
    "- If the context directly helps answer the question, use it and cite appropriately\n",
    "- If the context is topically related but not diagnostically relevant, acknowledge it but rely on your medical knowledge\n",
    "- If the context might mislead you toward a less likely diagnosis, explicitly state why you're not following it\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{choices}\n",
    "'''\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_path = \"GBaker/MedQA-USMLE-4-options\"\n",
    "subset_name = None\n",
    "split = \"test\"\n",
    "dataset = load_dataset(dataset_path, name = subset_name, trust_remote_code = True)\n",
    "data_list = dataset[split]\n",
    "# print(len(data_list))\n",
    "for data in data_list:\n",
    "    question = data[\"question\"]\n",
    "    choices = data[\"options\"]\n",
    "    answer_key = data[\"answer_idx\"]\n",
    "    prompt = prompt_RAG\n",
    "    formated_choices = format_choices(choices, answer_key, mask_correct_answer = False)\n",
    "\n",
    "    \n",
    "    context = self.get_RAG_context(question, formated_choices, score_threshold = score_threshold, pick_rag_index = pick_rag_index, use_classifier = use_classifier)\n",
    "    content = prompt.format(context = context, question = question, choices = formated_choices)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
