{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276d1669-13d3-4d8d-b4bb-a0a356859a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheju347/.conda/envs/LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "# import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f129326f-f179-449d-a442-b4c3b25281f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "\n",
    "model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b968c7b8-fc33-4446-a44a-7c4650c0dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7째F (36.5째C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      "\n",
      "[A] : Ampicillin\n",
      "[B] : Ceftriaxone\n",
      "[C] : Doxycycline\n",
      "[D] : Nitrofurantoin\n",
      "Given four answer candidates, A, B, C, D, choose the best answer choice and provide explanation.\n",
      "Provide intermediate steps to solve the problem.\n",
      "The answer is:\n",
      "\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "# Test apply_chat_template // https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "def format_choices(choices):\n",
    "    a = zip(list(choices.keys()), choices.values())\n",
    "    final_answers = []\n",
    "    for x,y in a:\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)\n",
    "\n",
    "prompt_template = f'''\n",
    "{{question}} \\n\n",
    "{{choices}}\n",
    "'''\n",
    "\n",
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7째F (36.5째C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\"\n",
    "choices = {\n",
    "\"A\": \"Ampicillin\",\n",
    "\"B\": \"Ceftriaxone\",\n",
    "\"C\": \"Doxycycline\",\n",
    "\"D\": \"Nitrofurantoin\"\n",
    "}\n",
    "\n",
    "formated_choices = format_choices(choices)\n",
    "\n",
    "model_prompt = prompt_template.format(question = question, choices = formated_choices)\n",
    "\n",
    "model_prompt += \"Given four answer candidates, A, B, C, D, choose the best answer choice and provide explanation.\\n\"\n",
    "model_prompt += \"Provide intermediate steps to solve the problem.\\n\"\n",
    "model_prompt += \"The answer is:\\n\"\n",
    "\n",
    "print(model_prompt)\n",
    "\n",
    "inputs = tokenizer(model_prompt, return_tensors = \"pt\", padding=False).to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens = 500,\n",
    "                attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "                do_sample = True)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1382460-198c-4681-b9cf-47626d420bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data from UltraMedical for model to train\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# chat = [\n",
    "#     {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "# ]\n",
    "# print(tokenizer.apply_chat_template(chat, add_generation_prompt = False, tokenize = False, continue_final_message = True))\n",
    "\n",
    "ultraMedical = load_dataset(\"TsinghuaC3I/UltraMedical\")\n",
    "\n",
    "data_list = ultraMedical[\"train\"]\n",
    "\n",
    "first_N_data = data_list.select(range(0, 100))\n",
    "\n",
    "def preprocess_function(data):\n",
    "    # print(\"data:\", data)\n",
    "    # input_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: \" for conversation in data[\"conversations\"]] # (context) + question + options\n",
    "    # target_text = [f\"{conversation[1]['value']}\" for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "    # # target_text = [f\"A\" for conversation in data[\"conversations\"]]  # CoT + final answer (TODO)\n",
    "    \n",
    "    prompt_added_text = \"\"\"\n",
    "Given four answer candidates, A, B, C, D, choose the best answer choice and provide explanation.\n",
    "Provide intermediate steps to solve the problem.\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "    input_text = [conversation[0]['value'] + prompt_added_text for conversation in data['conversations']] # (context) + question + options\n",
    "    target_text = [conversation[1]['value'] for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "\n",
    "\n",
    "    # print(\"input:\", input_text)\n",
    "    # print(\"input_text:\", input_text)\n",
    "    # print(\"target_text:\", target_text)\n",
    "    \n",
    "    # prompt = f\"Give a proper answer to the question:\\nquestion: {question}\\n\\nThe answer is: \"\n",
    "\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    targets = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    return inputs\n",
    "\n",
    "    # prompt = input_text\n",
    "    # data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # data['labels'] = tokenizer(target_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # return data\n",
    "\n",
    "tokenized_datasets = first_N_data.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e952afd-a252-45a6-8fec-dcf9bfd8f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>32.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>26.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>21.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>23.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>18.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>20.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>18.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>17.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>18.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>20.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>16.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>16.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>17.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>16.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>14.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>15.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>13.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>12.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>15.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>15.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>13.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>12.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>13.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>11.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>12.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>13.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>12.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>11.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>13.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>14.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>11.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>12.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>11.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>10.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>10.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>10.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>11.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>11.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>11.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>10.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>9.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>10.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>10.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>10.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>10.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>10.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>9.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>9.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>10.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>9.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>9.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>9.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>9.437500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./dialogue-summary-training/tokenizer_config.json',\n",
       " './dialogue-summary-training/special_tokens_map.json',\n",
       " './dialogue-summary-training/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "output_dir = f'./dialogue-summary-training'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    # weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    # max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets#['train'],\n",
    "    # eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "original_model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "original_model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0781cac-abb3-4f22-8448-e67f323a396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('./dialogue-summary-training', torch_dtype=torch.bfloat16).to(device)\n",
    "model = instruct_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cd252-573b-431c-93b4-61364800b7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
