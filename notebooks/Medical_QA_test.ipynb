{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cb932-66c1-4ba1-88dd-aee2cd0e1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259d1e32-c91b-49b8-8076-74979b53a874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./.local/lib/python3.11/site-packages (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.11/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.11/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa16345-68b9-4e19-8360-a03ab5db0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: accelerate: command not found\n"
     ]
    }
   ],
   "source": [
    "!accelerate --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda309c-5900-475f-b82f-1ddfe1f7e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7611e3-96d3-4cd3-8961-e98121de10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e4b27-02c0-4ab2-9114-69e09dd4ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d0130-46d5-473a-801c-be45a2442aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83b671-a7e6-4e01-9a5f-249e0a0999b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd11da-6f11-4112-b872-5d19b3f0c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadaebaa-f58d-49e7-b93f-d4d7b26410a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "model_name = \"GanjinZero/biobart-v2-base\"\n",
    "# model_name = \"GanjinZero/biobart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8e7aa-10e5-4001-9d79-37d2633448b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db60521-8c24-4def-bafb-e992265a532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"microsoft/biogpt\"\n",
    "# model_name = \"microsoft/BioGPT-Large-PubMedQA\"\n",
    "# model_name = \"microsoft/BioGPT-Large\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048cdfa1-7784-452b-b259-34d5a60eda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"gpt2-xl\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6b836-a517-4ce0-ae0c-cc041d9776ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user tiktoken pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41dd39-cd82-4885-9187-8415ff2c1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"KrithikV/MedMobile\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_name = \"microsoft/Phi-3-small-8k-instruct\"\n",
    "# model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe05e4-2ffb-4d4a-bc8d-58cca564242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f7ce5-36b0-4ea8-b8dd-c3e55c0bc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "folder_path = \"/home/sheju347/.cache/\"\n",
    "size_in_mb = get_folder_size(folder_path) / (1024 * 1024 * 1024)\n",
    "print(f\"Folder size: {size_in_mb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f32a0-286e-4314-bca4-4001299afe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# from transformers.utils import TRANSFORMERS_CACHE\n",
    "# print(TRANSFORMERS_CACHE)\n",
    "\n",
    "# print(os.listdir(\"/home/sheju347/.cache/huggingface/hub\"))\n",
    "\n",
    "# remove = True\n",
    "\n",
    "# model_name = \"models--Qwen--Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "# model_path = os.path.join(cache_dir, model_name)\n",
    "# print(\"path: \", model_path)\n",
    "\n",
    "# if os.path.exists(model_path):\n",
    "#     print(\"path exists\")\n",
    "#     if remove:\n",
    "#         shutil.rmtree(model_path)\n",
    "#         print(\"removed\")\n",
    "\n",
    "def remove_all():\n",
    "    shutil.rmtree(\"/home/sheju347/.cache/huggingface/\", ignore_errors=True)\n",
    "\n",
    "remove_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2630a5-1ff4-4f75-9c35-d52eb7286365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"gpt2\"\n",
    "llm = LLM(model=model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Load model into vLLM\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.2, top_k=40, top_p=0.85, max_tokens=100\n",
    ")\n",
    "\n",
    "prompt = \"What are the symptoms of diabetes?\"\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "print(outputs[0].text)  # Get response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89605e-4c35-457c-8f6a-6db69256290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FOR TEST\")\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "# prompt = \"What are the treatments of diabetes?\"\n",
    "# prompt = \"What are the symptoms of diabetes?\"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Question: Which medicine is for diabetes?\n",
    "# Choices:\n",
    "# A: Ibuprofen\n",
    "# B: Famciclovir\n",
    "# C: Baclofen\n",
    "# D: Metformin\n",
    "# Respond with only the letter corresponding to the correct answer.\n",
    "# The answer is: \\n\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = \"How are you?\"\n",
    "\n",
    "# prompt = \"Say DEF\"\n",
    "\n",
    "# prompt = \"\"\"Question: Which organ is responsible for cleaning blood?\n",
    "# Choices:\n",
    "# A) Liver\n",
    "# B) Kidney\n",
    "# C) Stomach\n",
    "# D) Lung\n",
    "# Respond with only the letter corresponding to the correct answer.\n",
    "# The answer is: B\n",
    "\n",
    "# Question: Which organ is responsible for breath?\n",
    "# Choices:\n",
    "# A) Liver\n",
    "# B) Kidney\n",
    "# C) Stomach\n",
    "# D) Lung\n",
    "# Respond with only the letter corresponding to the correct answer.\n",
    "# The answer is: \n",
    "# \"\"\"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Question: Which organ is responsible for breath?\n",
    "# Choices:\n",
    "# A: Liver\n",
    "# B: Kidney\n",
    "# C: Stomach\n",
    "# D: Lung\n",
    "# Given four answer candidates, A, B, C, and D, choose the best answer choice.\n",
    "# The answer is: \n",
    "# \"\"\"\n",
    "\n",
    "# from vllm import SamplingParams\n",
    "\n",
    "\"Given four answer candidates, A, B, C, and D, choose the best answer choice\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
    "Choices:\n",
    "A: Disclose the error to the patient but leave it out of the operative report\n",
    "B: Disclose the error to the patient and put it in the operative report\n",
    "C: Tell the attending that he cannot fail to disclose this mistake\n",
    "D: Report the physician to the ethics committee\n",
    "E: Refuse to dictate the operative report\n",
    "Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\n",
    "The answer is:\n",
    "\"\"\"\n",
    "# Respond only with letter A, B, C, D, E. Don't need other information.\n",
    "\n",
    "print (\"Prompt: \", prompt)\n",
    "print (\"--------------\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\", padding=False).to(device)\n",
    "with torch.no_grad(): # NO gradient calculation for inference\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 5,\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "        # use_cache=False,\n",
    "        do_sample = False\n",
    "    )\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "\n",
    "answer = answer.replace(prompt, '').strip()\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n",
    "print (\"--------------\")\n",
    "\n",
    "print (\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471dfb1-0808-464b-bcfc-3e32091cc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medqa_accuracy():\n",
    "    # Load MedQA dataset\n",
    "    # med_qa = load_dataset(\"bigbio/med_qa\", trust_remote_code = True)\n",
    "    med_qa = load_dataset(\"GBaker/MedQA-USMLE-4-options\", trust_remote_code = True)\n",
    "    keys = med_qa.keys()\n",
    "    # print(len(med_qa[\"train\"]), len(med_qa[\"validation\"]), len(med_qa[\"test\"]))\n",
    "    \n",
    "    print (\"model: \", model.name_or_path)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_list = med_qa[\"test\"]\n",
    "    count = 0\n",
    "    correct_count = 0\n",
    "    for data in data_list:\n",
    "        question = data[\"question\"]\n",
    "        answer_idx = data[\"answer_idx\"]\n",
    "    \n",
    "    #     prompt = \"\"\"\n",
    "    # Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
    "    # Choices:\n",
    "    # A: Disclose the error to the patient but leave it out of the operative report\n",
    "    # B: Disclose the error to the patient and put it in the operative report\n",
    "    # C: Tell the attending that he cannot fail to disclose this mistake\n",
    "    # D: Report the physician to the ethics committee\n",
    "    # E: Refuse to dictate the operative report\n",
    "    # Respond with only the letter corresponding to the correct answer.\n",
    "    # The answer is:\n",
    "    # \"\"\"\n",
    "    \n",
    "        prompt = f\"Question: {question}\\n\"\n",
    "        \n",
    "        prompt += \"Choices:\\n\"\n",
    "        options = data[\"options\"]\n",
    "        # for option in options:\n",
    "        #     prompt += f\"{option['key']}: {option['value']}\\n\"\n",
    "        for option_key, option_value in options.items():\n",
    "            prompt += f\"{option_key}: {option_value}\"\n",
    "        # prompt += \"Respond with only the letter corresponding to the correct answer.\\n\"\n",
    "        # prompt += \"Respond only with letter A, B, C, D, E. Don't need other information.\\n\"\n",
    "        # prompt += \"Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\\n\"\n",
    "        prompt += \"Given five answer candidates, A, B, C, D, choose the best answer choice.\\n\"\n",
    "        prompt += \"The answer is:\"\n",
    "    \n",
    "        # print(\"prompt: \", prompt)\n",
    "        # break\n",
    "        \n",
    "        # print(\"correct answer: \", answer_idx)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors = \"pt\", padding=False).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens = 5,\n",
    "                attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "                # do_sample=True,\n",
    "                # temperature = 0.1,\n",
    "                # use_cache = False, # For \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "                # do_sample = False\n",
    "            )\n",
    "        \n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "    \n",
    "        answer = answer.replace(prompt, '').strip()\n",
    "        # print(\"Answer: \", answer)\n",
    "\n",
    "        answer = answer.strip(\"()\")\n",
    "        answer = answer[0] # only get the first letter (A, B, C, D, E)\n",
    "        \n",
    "        correct_answer = answer_idx\n",
    "    \n",
    "        is_correct = (answer == correct_answer)\n",
    "        # print(\"Correct!!!\" if is_correct else \"Wrong\")\n",
    "     \n",
    "    \n",
    "        if is_correct:\n",
    "          correct_count += 1\n",
    "    \n",
    "        count += 1\n",
    "        \n",
    "        print(f\"question {count}/{len(data_list)} answer:{answer} correct_answer:{correct_answer} {is_correct}\")\n",
    "        \n",
    "        # if count >= 10:\n",
    "        #     break\n",
    "    \n",
    "    accuracy = correct_count / count\n",
    "    print(f\"Total questions: {count}, correct: {correct_count}, accuracy: {accuracy}\")\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    elapse_time = finish_time - start_time\n",
    "    print(\"elapse_time: \", elapse_time)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673be75-d858-4594-a9e0-1be3bd5508cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy_list = []\n",
    "for i in range(1, 21):\n",
    "    print(\"round:\", i)\n",
    "    accuracy = get_medqa_accuracy()\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"accuracy_list:\", accuracy_list)\n",
    "\n",
    "# mean_accuracy = np.mean(accuracy_list)\n",
    "# confidency_level = 0.95\n",
    "\n",
    "print(\"accuracy_list:\", accuracy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9799a-1736-4894-a252-26fe0eca94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_95_confidence_interval(accuracy_list):\n",
    "    mean = np.mean(accuracy_list)\n",
    "    standard_deviation = np.std(accuracy_list, ddof = 1)\n",
    "    standard_error_of_mean = standard_deviation / np.sqrt(len(accuracy_list))\n",
    "    z_score = 1.96 # for 95% confidence interval\n",
    "    margin_of_error = z_score * standard_error_of_mean\n",
    "    down = mean - margin_of_error\n",
    "    up = mean + margin_of_error\n",
    "\n",
    "    down = round(down, 5)\n",
    "    up = round(up, 5)\n",
    "\n",
    "    confidence_interval = (down, up)\n",
    "    return confidence_interval\n",
    "\n",
    "# accuracy_list = [0.527101335428123, 0.538098978790259, 0.5302435192458759, 0.5294579732914375, 0.5365278868813825, 0.5326001571091908, 0.5365278868813825, 0.5318146111547526, 0.5365278868813825, 0.5271013354281225, 0.5333857030636292, 0.5310290652003142, 0.5357423409269442, 0.5192458758837392, 0.5404556166535742, 0.5388845247446976, 0.5310290652003142, 0.5380989787902593, 0.5349567949725059, 0.5294579732914375, 0.5247446975648076, 0.5412411626080126, 0.5349567949725059, 0.5231736056559309, 0.5302435192458759, 0.5302435192458759]\n",
    "# accuracy_list = [0.4925373134328358, 0.4917517674783975, 0.4878240377062058, 0.48153967007069914, 0.4752553024351925, 0.49332285938727416, 0.47761194029850745, 0.48860958366064416, 0.4878240377062058, 0.4846818538884525, 0.4964650432050275, 0.4760408483896308, 0.4831107619795758, 0.48860958366064416, 0.4713275726630008, 0.46975648075412413, 0.48153967007069914, 0.4870384917517675, 0.48625294579732914, 0.49410840534171246, 0.4783974862529458]\n",
    "accuracy_list = [0.38177533385703066, 0.3857030636292223, 0.38648860958366066, 0.38020424194815394, 0.3786331500392773, 0.3935585231736057, 0.38020424194815394, 0.38256087981146897, 0.38413197172034563, 0.37549096622152395, 0.3786331500392773, 0.3809897879025923, 0.37549096622152395, 0.3699921445404556, 0.3809897879025923, 0.3582089552238806, 0.384917517674784, 0.38020424194815394, 0.373134328358209, 0.3927729772191673]\n",
    "print(get_95_confidence_interval(accuracy_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95b928-6e19-43dd-ba79-0ba73c2924f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # target_string = \"target: the answer to the question given the context is\"\n",
    "    target_string = \"The answer to the question given the context is\"\n",
    "    start_index = response.lower().find(target_string.lower())\n",
    "    answer = response[start_index + len(target_string):].strip()\n",
    "    return answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7fdbb-c5b4-43f3-8c47-a7163a0549eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PubMedQA dataset\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "# PubMedQA has three subsets: ['pqa_artificial', 'pqa_labeled', 'pqa_unlabeled'], each only has one split: train\n",
    "pubmed_qa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", trust_remote_code = True)\n",
    "\n",
    "print(pubmed_qa.keys()) # train only\n",
    "\n",
    "print(len(pubmed_qa[\"train\"]))\n",
    "\n",
    "data_list = pubmed_qa[\"train\"]#.select(range(8000, 8500))\n",
    "print(type(data_list))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "letter_to_answer = {\"1\": \"yes\", \"2\": \"no\", \"3\": \"maybe\"}\n",
    "\n",
    "count = 0\n",
    "correct_count = 0\n",
    "\n",
    "yes_count = 0\n",
    "no_count = 0\n",
    "maybe_count = 0\n",
    "for data in data_list:\n",
    "    question = data[\"question\"]\n",
    "    contexts = data[\"context\"][\"contexts\"]\n",
    "    final_decision = data[\"final_decision\"]\n",
    "    long_answer = data[\"long_answer\"]\n",
    "\n",
    "    if final_decision == \"yes\":\n",
    "        yes_count += 1\n",
    "    elif final_decision == \"no\":\n",
    "        no_count += 1\n",
    "    elif final_decision == \"maybe\":\n",
    "        maybe_count += 1\n",
    "    else:\n",
    "        print(\"final_decision\", final_decision)\n",
    "\n",
    "    # continue\n",
    "\n",
    "    # print(\"question:\", question)\n",
    "\n",
    "    prompt = f\"Question: {question}\\n\"\n",
    "    prompt += f\"Context: \\n\"\n",
    "    for context in contexts:\n",
    "        prompt += context + '\\n'\n",
    "    \n",
    "    prompt += \"Choices:\\n\"\n",
    "    prompt += \"yes\\n\"\n",
    "    prompt += \"no\\n\"\n",
    "    prompt += \"maybe\\n\"\n",
    "    # prompt += \"Respond with only the letter corresponding to the correct answer.\\n\"\n",
    "    # prompt += \"Respond only with letter A, B, C. Don't need other information.\\n\"\n",
    "    # prompt += \"Respond only with yes, no, maybe. Don't start with the word 'Answer:'. Don't need other information.\\n\"\n",
    "    prompt += \"Respond only with yes, no, maybe.\\n\"\n",
    "    # prompt += \"Respond only with yes, no, or maybe. The answer should start directly with yes, no, or maybe. Don't start with Answer.\\n\"\n",
    "    # prompt += \"The answer is: '\\n\"\n",
    "    prompt += \"The answer to the question given the context is\"\n",
    "    # options = data[\"options\"]\n",
    "    # for option in options:\n",
    "    #     prompt += f\"{option['key']} {option['value']}\\n\"\n",
    "    # prompt += \"Respond with yes, no, or maybe.\\n\"\n",
    "    # prompt += \"The correct answer is: \"\n",
    "\n",
    "    # print(\"prompt:\", prompt)\n",
    "    # break\n",
    "\n",
    "    # The following prompt is for BioGPT only, refer to https://arxiv.org/pdf/2210.10341 page 4\n",
    "    # source: question: question text. context: context text.\n",
    "    # target: the answer to the question given the context is yes\n",
    "    # prompt = f\"source: question: {question} context: \"\n",
    "    # for context in contexts:\n",
    "    #     prompt += context + ' '\n",
    "    # prompt += \"\\ntarget: the answer to the question given the context is\"\n",
    "\n",
    "    # print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, padding=False, return_tensors = \"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens = 10,\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # temperature = 0.1,\n",
    "            use_cache = False, # For \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "    answer = extract_answer(answer)\n",
    "    # print(\"answer:\", answer)\n",
    "    # print(\"prompt:\", prompt.replace('\\n', ''))\n",
    "    # answer = answer.replace(prompt.replace('\\n', ''), '').strip()\n",
    "    # print(\"-------------------------\")\n",
    "    # print(\"Answer: \", answer)\n",
    "    # print(answer)\n",
    "    # print(\"first 3: \", answer[0:2])\n",
    "    # print(\"-------------------------\")\n",
    "    # answer = letter_to_answer[answer]\n",
    "    # print(\"answer::::\", answer)\n",
    "    # print(\"prompt:::::\", prompt)\n",
    "\n",
    "    if \"yes\" in answer:\n",
    "        answer = \"yes\"\n",
    "    elif \"no\" in answer:\n",
    "        answer = \"no\"\n",
    "    else:\n",
    "        answer = \"maybe\"\n",
    "\n",
    "    correct_answer = final_decision # yes, no, maybe\n",
    "\n",
    "    is_correct = (answer == correct_answer)\n",
    "    # print(\"Correct!!!\" if is_correct else \"Wrong\")\n",
    "\n",
    "    if is_correct:\n",
    "      correct_count += 1\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "    print(f\"question {count}/{len(data_list)} answer:{answer} correct_answer:{correct_answer} {is_correct}\")\n",
    "    \n",
    "    # if count >= 1:\n",
    "    #     break\n",
    "\n",
    "\n",
    "\n",
    "accuracy = correct_count / count\n",
    "print(f\"Total questions: {count}, correct: {correct_count}, accuracy: {accuracy}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n",
    "\n",
    "print(\"yes no maybe count:\", yes_count, no_count, maybe_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760f844-2cc3-442c-8ab6-68f5560edc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MedMCQA dataset\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "pubmed_qa = load_dataset(\"openlifescienceai/medmcqa\", trust_remote_code = True)\n",
    "\n",
    "print(pubmed_qa.keys()) # train, validation, test\n",
    "\n",
    "print(len(pubmed_qa[\"train\"]), len(pubmed_qa[\"validation\"]), len(pubmed_qa[\"test\"]))\n",
    "\n",
    "data_list = pubmed_qa[\"validation\"]#.select(range(100, 200))\n",
    "print(type(data_list))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "count = 0\n",
    "correct_count = 0\n",
    "\n",
    "option_letters = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4}\n",
    "\n",
    "for data in data_list:\n",
    "    question = data[\"question\"]\n",
    "    options = [data[\"opa\"], data[\"opb\"], data[\"opc\"], data[\"opd\"]]\n",
    "    correct_option = data[\"cop\"]\n",
    "    choice_type = data[\"choice_type\"] # multi/single, but seems no use here, since even multi, \"cop\" is only an integer\n",
    "    exp = data[\"exp\"] # explanation\n",
    "\n",
    "    prompt = f\"Question: {question}\\n\"\n",
    "    \n",
    "    prompt += \"Choices:\\n\"\n",
    "    for i in range(len(options)):\n",
    "        option = options[i]\n",
    "        prompt += f\"{option_letters[i]}: {option}\\n\"\n",
    "    # prompt += \"Respond with only the letter corresponding to the correct answer (A, B, C, or D).\\n\"\n",
    "    # prompt += \"Respond only with letter A, B, C, D. Don't need other information.\\n\"\n",
    "    prompt += \"Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\\n\"\n",
    "    # prompt += \"The correct choice is \"\n",
    "    prompt += \"The answer is:\"\n",
    "\n",
    "    \n",
    "    # prompt = f\"source: question: {question}\"\n",
    "    # prompt += \"\\ntarget: the answer to the question given the context is\"\n",
    "\n",
    "    # print(\"prompt:\", prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, padding=False, return_tensors = \"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens = 5,\n",
    "            attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "            # use_cache = False, # For \"microsoft/Phi-3-mini-4k-instruct\", setting trust_remote_code = False can set use_cache = True\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "    \n",
    "    answer = answer.replace(prompt, '').strip()\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Question {count} Answer: \", answer)\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    answer = answer.strip(\"()\")\n",
    "    answer = answer[0] # only get the first letter (A, B, C, D)\n",
    "\n",
    "    if answer in letter_to_index:\n",
    "        answer = letter_to_index[answer]\n",
    "\n",
    "    correct_answer = correct_option\n",
    "\n",
    "    is_correct = (str(answer) == str(correct_answer))\n",
    "    # print(\"Correct!!!\" if is_correct else \"Wrong\")\n",
    "\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "    print(f\"question {count}/{len(data_list)} answer:{answer} correct_answer:{correct_answer} {is_correct}\")\n",
    "    \n",
    "    # if count >= 1:\n",
    "    #     break\n",
    "\n",
    "\n",
    "\n",
    "accuracy = correct_count / count\n",
    "print(f\"Total questions: {count}, correct: {correct_count}, accuracy: {accuracy}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71e379-8505-4d68-8f3f-ccab57a032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "import torch\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "# model_name = \"dmis-lab/biobert-v1.1\"\n",
    "# model_name = \"michiyasunaga/BioLinkBERT-large\"\n",
    "model_name = \"FacebookAI/xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "\n",
    "print (\"model:\", model.name_or_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "question = \"Kidney is used for cleaning blood. Which organ is responsible for cleaning blood?\"\n",
    "choices = [\"A) Liver\", \"B) Kidney\", \"C) Stomach\", \"D) Lung\"]\n",
    "\n",
    "# need enable batch tensors by setting padding and truncation to True\n",
    "encoding = tokenizer([[question, choice] for choice in choices], padding = True, truncation = True, return_tensors = \"pt\").to(device)\n",
    "\n",
    "print(\"encoding:\", encoding)\n",
    "\n",
    "for key in encoding:\n",
    "    encoding[key] = encoding[key].unsqueeze(0)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"encoding after:\", encoding)\n",
    "\n",
    "output = model(**encoding) # encoding: {'input_ids': tensor([[[...]]], 'attention_mask': tensor([[1, 1, ..., 0]]) }\n",
    "logits = output.logits\n",
    "\n",
    "print(\"logits:\", logits)\n",
    "\n",
    "predicted_choice = torch.argmax(logits, dim = 1).item()\n",
    "\n",
    "answer = choices[predicted_choice]\n",
    "\n",
    "print(\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc274577-a6ff-4dc3-80d8-28775d3ad904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aee67-f2bc-4a2d-8923-70cb2402d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5962b7b-6971-43f4-a7eb-9e703feeb4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
