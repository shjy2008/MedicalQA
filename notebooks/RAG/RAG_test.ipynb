{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6811ce4e-bbaa-4312-b275-07f6bccf71f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheju347/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = \"../../../../projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\"\n",
    "model_name = os.path.abspath(model_name)\n",
    "print(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only = True, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only = True, trust_remote_code = False, torch_dtype=torch.bfloat16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69932de-7e23-4e0c-bef5-b7a01de8608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "def set_llm(temperature = 0.0000001):\n",
    "    llm = HuggingFaceLLM(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context_window=4096,  # Phi-3 Mini typically has a 4k context window\n",
    "        max_new_tokens=1024,   # Max tokens to generate per response\n",
    "        generate_kwargs={\"temperature\": temperature, \"do_sample\": True},\n",
    "        # system_prompt=\"You are a helpful and friendly AI assistant.\",\n",
    "        messages_to_prompt=lambda messages: tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        ),\n",
    "        # You might need to set `device_map` here as well if not set in AutoModelForCausalLM\n",
    "        # device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm\n",
    "\n",
    "set_llm()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "# embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# # embedding_model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c8d152-5925-4af2-a9cd-1a68dd5860d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x14d104c12220>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir = \"/projects/sciences/computing/sheju347/RAG/pubmed_index_100k\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc00ff3-71bb-472d-b482-cad030a08f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='notebook_log_test_fine_tuned.txt',      # Log file name\n",
    "    filemode='a',                    # Append mode\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO               # Log level\n",
    ")\n",
    "\n",
    "class TestPerformance():\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, temperature = 0.0000001):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def test_MedQA_response(self):# Test apply_chat_template // https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "        print(self.model.name_or_path)\n",
    "\n",
    "        def format_choices(choices):\n",
    "            a = zip(list(choices.keys()), choices.values())\n",
    "            final_answers = []\n",
    "            for x,y in a:\n",
    "                final_answers.append(f'[{x}] : {y}')\n",
    "            return \"\\n\".join(final_answers)\n",
    "\n",
    "\n",
    "        def run_inference(content, model, tokenizer, max_new_tokens, temperature):\n",
    "            # messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "            # # add_generation_prompt indicates the start of a response\n",
    "            # inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = \"pt\").to(self.device)\n",
    "            # # print(\"inputs:\", tokenizer.apply_chat_template(messages, add_generation_prompt = True, tokenize = False))\n",
    "            # outputs = model.generate(inputs, max_new_tokens = max_new_tokens, do_sample = True, temperature = temperature)\n",
    "            # text = tokenizer.batch_decode(outputs)[0]\n",
    "            \n",
    "            query_engine = index.as_query_engine()\n",
    "            response = query_engine.query(content)\n",
    "            text = response.response\n",
    "\n",
    "            # Print the retrieved source nodes\n",
    "            for i, node in enumerate(response.source_nodes):\n",
    "                print(f\"--- Source {i + 1} ---\")\n",
    "                print(node.text)\n",
    "                print(f\"Score: {node.score}\")  # Optional: similarity score\n",
    "                print()\n",
    "            return text\n",
    "\n",
    "        prompt = f'''\n",
    "        {{question}} \\n\n",
    "        {{choices}}\n",
    "        '''\n",
    "\n",
    "        examples = [\n",
    "        # # Training data index 0 (GBaker/MedQA-USMLE-4-options)\n",
    "        # # correct: D\n",
    "        # {\"question\": \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\",\n",
    "        # \"choices\": {\n",
    "        # \"A\": \"Ampicillin\",\n",
    "        # \"B\": \"Ceftriaxone\",\n",
    "        # \"C\": \"Doxycycline\",\n",
    "        # \"D\": \"Nitrofurantoin\"\n",
    "        # },\n",
    "        # \"correct\": \"D\",\n",
    "        # \"source\": \"Training data index 0 (GBaker/MedQA-USMLE-4-options)\"\n",
    "        # },\n",
    "\n",
    "        # # Training data index 1\n",
    "        # # correct: A\n",
    "        # {\"question\": \"A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\",\n",
    "        # \"choices\": {\n",
    "        # \"A\": \"Placing the infant in a supine position on a firm mattress while sleeping\",\n",
    "        # \"B\": \"Keeping the infant covered and maintaining a high room temperature\",\n",
    "        # \"C\": \"Application of a device to maintain the sleeping position\",\n",
    "        # \"D\": \"Avoiding pacifier use during sleep\"\n",
    "        # },\n",
    "        # \"correct\": \"A\",\n",
    "        # \"source\": \"Training data index 1 (GBaker/MedQA-USMLE-4-options)\"\n",
    "        # },\n",
    "\n",
    "        # # Training data index 2\n",
    "        # # correct: A\n",
    "        # {\"question\": \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\",\n",
    "        # \"choices\": {\n",
    "        # \"A\": \"Abnormal migration of ventral pancreatic bud\",\n",
    "        # \"B\": \"Complete failure of proximal duodenum to recanalize\",\n",
    "        # \"C\": \"Abnormal hypertrophy of the pylorus\",\n",
    "        # \"D\": \"Failure of lateral body folds to move ventrally and fuse in the midline\"\n",
    "        # },\n",
    "        # \"correct\": \"A\",\n",
    "        # \"source\": \"Training data index 2 (GBaker/MedQA-USMLE-4-options)\"\n",
    "        # },\n",
    "\n",
    "        # # Test data index 0\n",
    "        # # correct: B\n",
    "        # {\"question\": \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\",\n",
    "        # \"choices\":{\n",
    "        # \"A\": \"Disclose the error to the patient and put it in the operative report\",\n",
    "        # \"B\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
    "        # \"C\": \"Report the physician to the ethics committee\",\n",
    "        # \"D\": \"Refuse to dictate the operative report\"\n",
    "        # },\n",
    "        # \"correct\": \"B\",\n",
    "        # \"source\": \"Test data index 0 (GBaker/MedQA-USMLE-4-options)\"\n",
    "        # },\n",
    "\n",
    "            \n",
    "        {\"question\": \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\",\n",
    "        \"choices\": {\n",
    "        \"A\": \"Abnormal migration of ventral pancreatic bud\",\n",
    "        \"B\": \"Complete failure of proximal duodenum to recanalize\",\n",
    "        \"C\": \"Abnormal hypertrophy of the pylorus\",\n",
    "        \"D\": \"Failure of lateral body folds to move ventrally and fuse in the midline\"\n",
    "        },\n",
    "        \"correct\": \"A\",\n",
    "        \"source\": \"Train data index 2 (GBaker/MedQA-USMLE-4-options)\"\n",
    "        },\n",
    "\n",
    "        ]\n",
    "\n",
    "        for example in examples:\n",
    "            formated_choices = format_choices(example[\"choices\"])\n",
    "    \n",
    "            model_prompt = prompt.format(question = example[\"question\"], choices = formated_choices)\n",
    "    \n",
    "            # print(model_prompt)\n",
    "    \n",
    "            output_text = run_inference(model_prompt, self.model, self.tokenizer, max_new_tokens = 1024, temperature = self.temperature)\n",
    "            # output_text = output_text.split(\"<|assistant|>\")[-1]\n",
    "            print(\"model output:\", output_text)\n",
    "            print(\"\\nsource: \", example[\"source\"])\n",
    "            print(\"correct: \", example[\"correct\"])\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def test_MedQA_test_data_accuracy(self, is_ensemble = False):\n",
    "\n",
    "        if is_ensemble:\n",
    "            set_llm(temperature = 0.7)\n",
    "        else:\n",
    "            set_llm()\n",
    "\n",
    "        MAX_TOKEN_OUTPUT = 1024\n",
    "        SPLIT = \"test\"\n",
    "        DATA_RANGE = None #range(627, 643)\n",
    "\n",
    "        regex_pattern=r\"[\\(\\[]([A-Z])[\\)\\]]\"\n",
    "        regex = re.compile(regex_pattern)\n",
    "\n",
    "        def format_choices(choices):\n",
    "            a = zip(list(choices.keys()), choices.values())\n",
    "            final_answers = []\n",
    "            for x,y in a:\n",
    "                final_answers.append(f'[{x}] : {y}')\n",
    "            return \"\\n\".join(final_answers)\n",
    "\n",
    "        def find_match(regex, resp, convert_dict={}):\n",
    "            match = regex.findall(resp)\n",
    "            if match:\n",
    "                match = match[-1]\n",
    "                if isinstance(match, tuple):\n",
    "                    match = [m for m in match if m][0]\n",
    "                match = match.strip()\n",
    "                if match and match in convert_dict: \n",
    "                    match = convert_dict[match]\n",
    "            return match\n",
    "                \n",
    "        def extract_answer(response):\n",
    "            matchFirst = re.search(r'the answer is .(\\w).', response)\n",
    "            if matchFirst:\n",
    "                return f\"({matchFirst.group(1)})\"\n",
    "            match = find_match(regex, response) \n",
    "            if match:\n",
    "                return f\"({match})\"\n",
    "            return \"[invalid]\"\n",
    "\n",
    "        def run_inference_get_answer_letter(content):\n",
    "            query_engine = index.as_query_engine()\n",
    "            response = query_engine.query(content)\n",
    "            text = response.response\n",
    "        \n",
    "            # # Print the retrieved source nodes\n",
    "            # for i, node in enumerate(response.source_nodes):\n",
    "            #     print(f\"--- Source {i + 1} ---\")\n",
    "            #     print(node.text)\n",
    "            #     print(f\"Score: {node.score}\")  # Optional: similarity score\n",
    "            #     print()\n",
    "            \n",
    "            # print(\"outputs text:\", text)\n",
    "            text = text.split(\"<|assistant|>\")[-1]\n",
    "            # answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "\n",
    "            answer = extract_answer(text).strip(\"()\")\n",
    "            \n",
    "            print(f\"answer: {answer}\")\n",
    "            logging.info(f\"answer: {answer}\")\n",
    "            \n",
    "            return answer\n",
    "                \n",
    "        def get_medqa_accuracy():\n",
    "            # Load MedQA dataset\n",
    "            # med_qa = load_dataset(\"bigbio/med_qa\", trust_remote_code = True)\n",
    "            med_qa = load_dataset(\"GBaker/MedQA-USMLE-4-options\", trust_remote_code = True)\n",
    "            keys = med_qa.keys()\n",
    "            # print(len(med_qa[\"train\"]), len(med_qa[\"validation\"]), len(med_qa[\"test\"]))\n",
    "\n",
    "            print(f\"model: {self.model.name_or_path}\")\n",
    "            logging.info(f\"model: {self.model.name_or_path}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            data_list = med_qa[SPLIT]\n",
    "            if DATA_RANGE != None:\n",
    "                data_list = data_list.select(DATA_RANGE)\n",
    "            count = 0\n",
    "            correct_count = 0\n",
    "            for data in data_list:\n",
    "                question = data[\"question\"]\n",
    "                answer_idx = data[\"answer_idx\"]\n",
    "                choices = data[\"options\"]\n",
    "\n",
    "                prompt = f'''\\n{{question}}\\n{{choices}}\\n'''\n",
    "\n",
    "                formated_choices = format_choices(choices)\n",
    "                \n",
    "                model_prompt = prompt.format(question = question, choices = formated_choices)\n",
    "                \n",
    "                # messages = [{\"role\": \"user\", \"content\": f\"{model_prompt}\"}]\n",
    "                # inputs = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "                # print(\"messages: \", messages)\n",
    "                # break\n",
    "                \n",
    "                # print(\"correct answer: \", answer_idx)\n",
    "\n",
    "                if is_ensemble:\n",
    "                    answer_dict = {}\n",
    "                    for i in range(0, 5):\n",
    "                        current_answer = run_inference_get_answer_letter(model_prompt)\n",
    "                        if current_answer in answer_dict:\n",
    "                            answer_dict[current_answer] += 1\n",
    "                        else:\n",
    "                            answer_dict[current_answer] = 1\n",
    "                    answer = max(answer_dict, key = answer_dict.get)\n",
    "                else:\n",
    "                    answer = run_inference_get_answer_letter(model_prompt)\n",
    "                \n",
    "                correct_answer = answer_idx\n",
    "            \n",
    "                is_correct = (answer == correct_answer)\n",
    "                # print(\"Correct!!!\" if is_correct else \"Wrong\")\n",
    "            \n",
    "            \n",
    "                if is_correct:\n",
    "                    correct_count += 1\n",
    "            \n",
    "                count += 1\n",
    "\n",
    "                print(f\"question {count}/{len(data_list)} answer:{answer} correct_answer:{correct_answer} {is_correct}\")\n",
    "                logging.info(f\"question {count}/{len(data_list)} answer:{answer} correct_answer:{correct_answer} {is_correct}\")\n",
    "            \n",
    "            accuracy = correct_count / count\n",
    "            print(f\"Total questions: {count}, correct: {correct_count}, accuracy: {accuracy}\")\n",
    "            logging.info(f\"Total questions: {count}, correct: {correct_count}, accuracy: {accuracy}\")\n",
    "            \n",
    "            finish_time = time.time()\n",
    "            elapse_time = finish_time - start_time\n",
    "            print(f\"elapse_time: {elapse_time}\")\n",
    "            logging.info(f\"elapse_time: {elapse_time}\")\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "        get_medqa_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4ea9d-e6df-487d-85a1-1f721a4d42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: /projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\n",
      "answer: A\n",
      "question 1/1273 answer:A correct_answer:B False\n",
      "answer: D\n",
      "question 2/1273 answer:D correct_answer:D True\n",
      "answer: B\n",
      "question 3/1273 answer:B correct_answer:B True\n",
      "answer: B\n",
      "question 4/1273 answer:B correct_answer:D False\n",
      "answer: B\n",
      "question 5/1273 answer:B correct_answer:B True\n",
      "answer: B\n",
      "question 6/1273 answer:B correct_answer:D False\n",
      "answer: B\n",
      "question 7/1273 answer:B correct_answer:C False\n",
      "answer: C\n",
      "question 8/1273 answer:C correct_answer:C True\n",
      "answer: B\n",
      "question 9/1273 answer:B correct_answer:B True\n",
      "answer: A\n",
      "question 10/1273 answer:A correct_answer:A True\n",
      "answer: D\n",
      "question 11/1273 answer:D correct_answer:D True\n",
      "answer: D\n",
      "question 12/1273 answer:D correct_answer:D True\n",
      "answer: B\n",
      "question 13/1273 answer:B correct_answer:B True\n",
      "answer: D\n",
      "question 14/1273 answer:D correct_answer:D True\n",
      "answer: D\n",
      "question 15/1273 answer:D correct_answer:C False\n",
      "answer: B\n",
      "question 16/1273 answer:B correct_answer:B True\n"
     ]
    }
   ],
   "source": [
    "test = TestPerformance(model, tokenizer, device)\n",
    "# test.test_MedQA_response()\n",
    "test.test_MedQA_test_data_accuracy(is_ensemble = False)\n",
    "# test.test_MedQA_test_data_accuracy(is_ensemble = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a25f5-a7a1-45e6-bcf6-c5f5c3f09b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
