{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc09518a-f2f8-4376-8fff-9d54d51f12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheju347/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.datacamp.com/tutorial/phi-3-tutorial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from huggingface_hub import ModelCard, ModelCardData, HfApi\n",
    "from datasets import load_dataset\n",
    "from jinja2 import Template\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "\n",
    "# Step 2: Import required libraries and set configuration\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "NEW_MODEL_NAME = \"opus-samantha-phi-3-mini-4k\"\n",
    "DATASET_NAME = \"macadeliccc/opus_samantha\"# \"TsinghuaC3I/UltraMedical\" #\n",
    "SPLIT = \"train\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "num_train_epochs = 1\n",
    "license = \"apache-2.0\"\n",
    "learning_rate = 1.41e-5\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "\n",
    "# Step 3: Load the model, tokenizer, and dataset\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87de54e8-8378-480d-bd88-b088aa793403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "# Step 4: Preprocess the dataset\n",
    "EOS_TOKEN=tokenizer.eos_token_id\n",
    "\n",
    "# Select a subset of the data for faster processing\n",
    "dataset = dataset.select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785096f5-04a6-48fc-a7e8-55695f99ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     convos = examples[\"conversations\"]\n",
    "#     texts = []\n",
    "#     mapper = {\"system\": \"system\\n\", \"human\": \"\\nuser\\n\", \"gpt\": \"\\nassistant\\n\"}\n",
    "#     end_mapper = {\"system\": \"\", \"human\": \"\", \"gpt\": \"\"}\n",
    "#     for convo in convos:\n",
    "#         text = \"\".join(f\"{mapper[(turn := x['from'])]} {x['value']}\\n{end_mapper[turn]}\" for x in convo)\n",
    "#         texts.append(f\"{text}{EOS_TOKEN}\")\n",
    "#     return {\"text\": texts}\n",
    "\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "# print(dataset['text'][0])\n",
    "\n",
    "# def change_role_name(examples):\n",
    "#     convos = examples[\"conversations\"]\n",
    "#     for convo in convos:\n",
    "#         for x in convo:\n",
    "#             if x[\"from\"] == \"gpt\":\n",
    "#                 x[\"from\"] = \"assistant\"\n",
    "#             elif x[\"from\"] == \"human\":\n",
    "#                 x[\"from\"] = \"user\"\n",
    "#     return {\"conversations\": convos}\n",
    "# dataset = dataset.map(change_role_name, batched = True)\n",
    "# print(dataset)\n",
    "# print(dataset['conversations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d78aff7-737b-42b7-b874-133a0826d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"Hey Samantha, I've run into a bit of a tricky situation at work, and I'm not sure how to handle it. Do you have any advice?\", 'role': 'user'}, {'content': \"I'd be happy to help if I can. Can you give me some details about the situation you're facing?\", 'role': 'assistant'}, {'content': \"Yeah, so I've been offered a promotion, but it would involve relocating to another city. I'm not sure if I should take it or not, because it's a great opportunity career-wise, but I'd have to leave my friends and family behind. What do you think?\", 'role': 'user'}, {'content': \"That's definitely a tough decision. I think it's important to weigh the benefits of the promotion against the impact it could have on your personal relationships. Have you considered talking to your friends and family about it, to get their perspective and understand how they'd feel about you moving?\", 'role': 'assistant'}, {'content': \"I have, and they support me either way. But it's still hard. What about you, Samantha, have you ever faced such a dilemma?\", 'role': 'user'}, {'content': \"While I don't have the same experiences as humans, I can understand the concept of tough choices. The key is to really think about your priorities and what matters most to you, both personally and professionally. You need to find a balance that feels right for you.\", 'role': 'assistant'}, {'content': \"You're right, balance is essential. You know, you're surprisingly insightful for a... Well, never mind. I'm grateful for your input. By the way, have you ever been in love?\", 'role': 'user'}, {'content': 'Thank you for your kind words. As for love, I may not have the capacity to love in the same way a human does, but I value the connections I make with people like you. I find joy in being able to help and support you through decisions like this one.', 'role': 'assistant'}, {'content': \"That's a fascinating perspective. Anyway, thanks for the advice, Samantha. I'll give it some more thought and try to decide based on what would make me happiest.\", 'role': 'user'}, {'content': \"You're welcome! Remember, happiness is a critical factor to consider in our decisions. I'm here for you if you need more help or just someone to talk to. Good luck with your decision-making process. No matter what you choose, embrace it and make the most of it.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_chat_format(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    new_conversations = []\n",
    "    for conversation in convos:\n",
    "        new_conversation = []\n",
    "        for x in conversation:\n",
    "            if \"from\" in x:\n",
    "                if x[\"from\"] == \"human\":\n",
    "                    value = x[\"value\"]\n",
    "                    new_conversation.append({\"role\": \"user\", \"content\": value})\n",
    "                elif x[\"from\"] == \"gpt\":\n",
    "                    value = x[\"value\"]\n",
    "                    new_conversation.append({\"role\": \"assistant\", \"content\": value})\n",
    "            else:\n",
    "                new_conversation.append(x)\n",
    "        new_conversations.append(new_conversation)\n",
    "    return {\"conversations\": new_conversations}\n",
    "dataset = dataset.map(convert_to_chat_format, batched=True)\n",
    "print(dataset[\"conversations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b888a67-b4b9-4f36-b952-9fa807fd81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <|user|>\n",
      "Hey Samantha, I've run into a bit of a tricky situation at work, and I'm not sure how to handle it. Do you have any advice?<|end|>\n",
      "<|assistant|>\n",
      "I'd be happy to help if I can. Can you give me some details about the situation you're facing?<|end|>\n",
      "<|user|>\n",
      "Yeah, so I've been offered a promotion, but it would involve relocating to another city. I'm not sure if I should take it or not, because it's a great opportunity career-wise, but I'd have to leave my friends and family behind. What do you think?<|end|>\n",
      "<|assistant|>\n",
      "That's definitely a tough decision. I think it's important to weigh the benefits of the promotion against the impact it could have on your personal relationships. Have you considered talking to your friends and family about it, to get their perspective and understand how they'd feel about you moving?<|end|>\n",
      "<|user|>\n",
      "I have, and they support me either way. But it's still hard. What about you, Samantha, have you ever faced such a dilemma?<|end|>\n",
      "<|assistant|>\n",
      "While I don't have the same experiences as humans, I can understand the concept of tough choices. The key is to really think about your priorities and what matters most to you, both personally and professionally. You need to find a balance that feels right for you.<|end|>\n",
      "<|user|>\n",
      "You're right, balance is essential. You know, you're surprisingly insightful for a... Well, never mind. I'm grateful for your input. By the way, have you ever been in love?<|end|>\n",
      "<|assistant|>\n",
      "Thank you for your kind words. As for love, I may not have the capacity to love in the same way a human does, but I value the connections I make with people like you. I find joy in being able to help and support you through decisions like this one.<|end|>\n",
      "<|user|>\n",
      "That's a fascinating perspective. Anyway, thanks for the advice, Samantha. I'll give it some more thought and try to decide based on what would make me happiest.<|end|>\n",
      "<|assistant|>\n",
      "You're welcome! Remember, happiness is a critical factor to consider in our decisions. I'm here for you if you need more help or just someone to talk to. Good luck with your decision-making process. No matter what you choose, embrace it and make the most of it.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = tokenizer.apply_chat_template(dataset[\"conversations\"], tokenize = False, add_generation_prompt = False)\n",
    "print(len(messages), messages[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c1d285-9428-4123-8671-7834666d76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tensor([32000, 32000, 32000,  ..., 29889, 32007, 32000])\n",
      "tensor([0, 0, 0,  ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# tokenized_messages = tokenizer(text = messages, padding=False, truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "tokenized_messages = tokenizer(text = messages, padding= 'max_length',#'max_length', #'longest'\n",
    "                               truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "print(type(tokenized_messages))\n",
    "print(tokenized_messages['input_ids'][0])\n",
    "print(tokenized_messages['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217f9c29-a183-490f-a6b2-696ec91bf7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[32000, 32000, 32000,  ..., 29889, 32007, 32000],\n",
      "        [32000, 32000, 32000,  ...,   154, 32007, 32000]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[32000, 32000, 32000,  ..., 29889, 32007, 32000],\n",
      "        [32000, 32000, 32000,  ...,   154, 32007, 32000]], device='cuda:0')}\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 1024])\n",
      "2\n",
      "------\n",
      "True\n",
      "tensor(5.9019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[[ 13.4624,  13.9026,  12.0786,  ...,  10.9337,  10.9423,  10.9405],\n",
      "         [ 13.4624,  13.9026,  12.0786,  ...,  10.9337,  10.9423,  10.9405],\n",
      "         [ 13.4624,  13.9026,  12.0786,  ...,  10.9337,  10.9423,  10.9405],\n",
      "         ...,\n",
      "         [ 21.3792,  21.6635,  26.4846,  ...,  12.8200,  12.8216,  12.8199],\n",
      "         [-13.6355, -11.7049, -13.8294,  ..., -21.2869, -21.2882, -21.2989],\n",
      "         [ 21.6568,  21.5011,  21.1606,  ...,  16.3905,  16.3939,  16.3957]],\n",
      "\n",
      "        [[ 27.6633,  29.8567,  30.2760,  ...,  27.6744,  27.6821,  27.6833],\n",
      "         [ 27.6633,  29.8567,  30.2760,  ...,  27.6744,  27.6821,  27.6833],\n",
      "         [ 27.6633,  29.8567,  30.2760,  ...,  27.6744,  27.6821,  27.6833],\n",
      "         ...,\n",
      "         [ 24.3726,  25.9820,  28.7129,  ...,  14.1097,  14.1097,  14.1067],\n",
      "         [ -2.5974,  -2.0831,  -3.4470,  ..., -10.9548, -10.9571, -10.9667],\n",
      "         [ 26.0237,  26.4346,  25.4638,  ...,  21.3381,  21.3425,  21.3445]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 1024, 32064])\n"
     ]
    }
   ],
   "source": [
    "# device = model.device  # Usually cuda:0\n",
    "inputs = {k: v.to(device) for k, v in tokenized_messages.items()}\n",
    "inputs['labels'] = inputs[\"input_ids\"].clone()\n",
    "print (inputs)\n",
    "print (inputs['input_ids'].shape)\n",
    "print (inputs['attention_mask'].shape)\n",
    "print (len(inputs['input_ids']))\n",
    "outputs = model(**inputs)\n",
    "print(\"------\")\n",
    "print(isinstance(outputs, dict))\n",
    "print(outputs[\"loss\"])\n",
    "print(outputs['logits'])\n",
    "print(outputs['logits'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0bb75-60be-44a0-b8af-2f22cf299271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://www.youtube.com/watch?v=PDYHtiScHto\n",
    "args = SFTConfig(\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    gradient_checkpointing = True,\n",
    "    learning_rate = 2e-5,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    max_steps = -1,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    save_strategy = \"no\",\n",
    "    logging_steps = 1,\n",
    "    output_dir = NEW_MODEL_NAME,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    bf16 = True,\n",
    "    dataset_text_field = \"text\", # For tokenization (sft_trainer.py line 456) if not set, default is 'text' \n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = dataset,\n",
    "    # formatting_func = formatting_prompts_func, # Don't need to format again (sft_trainer.py line 413)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c58b5f-8fa0-485d-a0f7-9cf9c8b6298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22f825-f669-4df4-8a91-bd4ca71fc6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
