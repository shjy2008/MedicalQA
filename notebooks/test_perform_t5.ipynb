{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eff5bcd-c545-4a20-b37b-30f0d56e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "!export CUDA_LAUNCH_BLOCKING=1   # (optional, helps debug determinism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99151ab-4029-40b5-8afd-9a74f5d95626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349a8d9c-2c15-4b55-815b-a696377a14ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model_trainer import ModelTrainer\n",
    "from test_performance import TestPerformance, DatasetPath, MMLU_Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283a1066-2c5f-47e4-86ab-2b539229b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- start checking GPU -----------\n",
      "GPU: NVIDIA H100 NVL\n",
      "torch.cuda.is_bf16_supported():  True\n",
      "---------- finish checking GPU -----------\n",
      "---------- start loading model:google/flan-t5-xl -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading model\n",
      "torch_dtype: torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "---------- finish loading model:google/flan-t5-xl -----------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"KrithikV/MedMobile\"\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# model_name = os.path.abspath(\"../../../projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\")\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/10-15-UltraMedical-batchsize8-lr2e-5-bf16\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/10-15-UltraMedical-batchsize8-bf16\" # BEST\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-1-phi3-mini-batchsize8-epoch456/\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-batch8-epoch456/checkpoint-256000\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-1-phi3-mini-batchsize8-epoch456/checkpoint-256000\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_phi4/11-24-phi4-mini-base-UltraMedical\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_Phi_3_mini\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_no_mask_entire\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-3-fine-tuned-all\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-5-fine-tuned-UltraMedical-MedQA-context/\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-6-fine-tuned-UltraMedical-MedQA-LoRA\"\n",
    "\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-6-fine-tuned-UltraMedical-MedQA-LoRA\"\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-10-LoRA-context012-r-16-lr-1e-5\"\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/10-12-LoRA-Evol-context012-r-16-lr-1e-5-epoch3/checkpoint-37846\"\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/10-17-new-base-model-1e-4-batch8-LoRA-1e-4-r16\" # BEST\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-6-epoch5-model-LoRA-1e-4-r16-context012\"\n",
    "lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-7-epoch5-model-LoRA-1e-4-r16-context01\"\n",
    "# lora_adapter_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/11-10-LoRA-5e-5-r16-context012\"\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"google/flan-t5-xl\"\n",
    "\n",
    "# trainer.load_model(model_name)\n",
    "# trainer.load_model(model_name, lora_adapter_path)\n",
    "\n",
    "# tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-UltraMedical\"\n",
    "# trainer.load_model(model_name, tokenizer_path = tokenizer_path)\n",
    "\n",
    "# tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-1-phi3-mini-batchsize8-epoch456/\"\n",
    "# trainer.load_model(model_name, tokenizer_path = tokenizer_path, lora_adapter_path = lora_adapter_path)\n",
    "\n",
    "trainer.load_model_t5(model_name)\n",
    "# trainer.test_model_MedQA_response()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2f89ba-3ca3-4a20-a563-41e6aa1a6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = TestPerformance(trainer.model, trainer.tokenizer, trainer.device, is_encoder_decoder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50774f0-7b2f-47aa-9cec-7a32401c90c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'GBaker/MedQA-USMLE-4-options' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_RAG: True\n",
      "topK_searchEngine: 150\n",
      "topK_SPLADE: 30\n",
      "topK_denseEmbedding: 0\n",
      "topK_crossEncoder: 5\n",
      "topK_LLM: 0\n",
      "score_threshold: None\n",
      "RRF_models: None\n",
      "model: google/flan-t5-xl\n",
      "RAG data: [{'docId': 'pubmed23n0390_3257', 'BM25_score': 118.188, 'BM25_ranking': 2, 'SPLADE_score': 70.20608520507812, 'SPLADE_ranking': 2, 'crossEncoder_score': np.float32(0.6180477), 'crossEncoder_ranking': 1}, {'docId': 'pubmed23n0983_13389', 'BM25_score': 93.872, 'BM25_ranking': 58, 'SPLADE_score': 52.943092346191406, 'SPLADE_ranking': 19, 'crossEncoder_score': np.float32(-0.602681), 'crossEncoder_ranking': 2}, {'docId': 'pubmed23n0796_13845', 'BM25_score': 96.5008, 'BM25_ranking': 36, 'SPLADE_score': 54.55889129638672, 'SPLADE_ranking': 12, 'crossEncoder_score': np.float32(-1.66644), 'crossEncoder_ranking': 3}, {'docId': 'pubmed23n0827_1593', 'BM25_score': 95.4198, 'BM25_ranking': 44, 'SPLADE_score': 54.33488464355469, 'SPLADE_ranking': 13, 'crossEncoder_score': np.float32(-1.7007079), 'crossEncoder_ranking': 4}, {'docId': 'pubmed23n0987_15772', 'BM25_score': 96.6391, 'BM25_ranking': 33, 'SPLADE_score': 60.82719421386719, 'SPLADE_ranking': 5, 'crossEncoder_score': np.float32(-1.8588865), 'crossEncoder_ranking': 5}]\n",
      "outputs text: <pad> [A]</s>\n",
      "question 1/1273 answer:A correct_answer:B False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m file_name = \u001b[33m\"\u001b[39m\u001b[33m12-8-flan-t5-xl-MedQA-150-30-1.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# test.SPLIT = \"train\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDatasetPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMedQA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ensemble\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_RAG\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mtopK_searchEngine\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_searchEngine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_SPLADE\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_SPLADE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_denseEmbedding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_denseEmbedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_crossEncoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_crossEncoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_LLM\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK_LLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpick_rag_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpick_rag_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_classifier_training_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_classifier_training_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                  \u001b[49m\u001b[43muse_classifier\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRRF_models\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mRRF_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MedicalQA/notebooks/test_performance.py:440\u001b[39m, in \u001b[36mTestPerformance.test_accuracy\u001b[39m\u001b[34m(self, dataset_path, subset_name, is_ensemble, use_RAG, data_range, file_name, topK_searchEngine, topK_SPLADE, topK_denseEmbedding, topK_crossEncoder, topK_LLM, score_threshold, pick_rag_index, mask_correct_answer, get_classifier_training_data, use_classifier, sentence_level_RAG, RRF_models)\u001b[39m\n\u001b[32m    438\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRAG data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_data_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_RAG_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformated_choices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sentence_level_RAG:\n\u001b[32m    442\u001b[39m         sentences = sent_tokenize(context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MedicalQA/notebooks/context_retriever.py:183\u001b[39m, in \u001b[36mContextRetriever.get_RAG_context\u001b[39m\u001b[34m(self, question, formated_choices)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_RAG_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, question, formated_choices):\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     doc_data_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_RAG_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformated_choices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# doc_list -> context (str)\u001b[39;00m\n\u001b[32m    186\u001b[39m     context = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MedicalQA/notebooks/context_retriever.py:147\u001b[39m, in \u001b[36mContextRetriever.get_RAG_data_list\u001b[39m\u001b[34m(self, question, formated_choices)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# 1. SPLADE\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.topK_SPLADE != \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.topK_SPLADE > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     doc_data_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mRAG_SPLADE_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtopK_SPLADE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# print(f\"2 len(doc_list): {len(doc_list)}\")\u001b[39;00m\n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# 2. Dense Embedding\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.topK_denseEmbedding != \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.topK_denseEmbedding > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MedicalQA/notebooks/context_retriever.py:249\u001b[39m, in \u001b[36mContextRetriever.RAG_SPLADE_filter\u001b[39m\u001b[34m(self, query, doc_data_list, top_k)\u001b[39m\n\u001b[32m    247\u001b[39m doc_list = [data[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m doc_data_list]\n\u001b[32m    248\u001b[39m query_embeddings = \u001b[38;5;28mself\u001b[39m.splade_model.encode_query([query], show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m document_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplade_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m similarities = \u001b[38;5;28mself\u001b[39m.splade_model.similarity(query_embeddings, document_embeddings)\n\u001b[32m    251\u001b[39m score_list = similarities[\u001b[32m0\u001b[39m].tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/sentence_transformers/sparse_encoder/SparseEncoder.py:395\u001b[39m, in \u001b[36mSparseEncoder.encode_document\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, convert_to_tensor, convert_to_sparse_tensor, save_to_cpu, device, max_active_dims, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m             prompt_name = candidate_prompt_name\n\u001b[32m    393\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_sparse_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_sparse_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_to_cpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_to_cpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_active_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_active_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/sentence_transformers/sparse_encoder/SparseEncoder.py:593\u001b[39m, in \u001b[36mSparseEncoder.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, convert_to_tensor, convert_to_sparse_tensor, save_to_cpu, device, max_active_dims, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m save_to_cpu:\n\u001b[32m    591\u001b[39m         embeddings = embeddings.cpu()\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     all_embeddings.extend(embeddings)\n\u001b[32m    595\u001b[39m all_embeddings = [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/torch/_tensor.py:1195\u001b[39m, in \u001b[36mTensor.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state():\n\u001b[32m   1187\u001b[39m     warnings.warn(\n\u001b[32m   1188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1189\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing a tensor of different shape won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt change the number of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1193\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1194\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# data_range = [9, 13, 19, 24, 28, 29, 30, 31, 32, 38, 43, 44, 45, 46, 52, 56, 57, 62, 64, 68, 71, 73, 77, 87, 90, 94, 96, 97, 108, \n",
    "#               109, 110, 118, 123, 126, 133, 135, 139, 141, 143, 144, 145, 146, 159, 160, 166, 167, 170, 171, 172, 173, 179, 180, 181, 183, \n",
    "#               185, 186, 188, 191, 196, 197, 203, 205]\n",
    "\n",
    "# data_range = range(30, 32) # None #range(34, 35)\n",
    "\n",
    "data_range = None\n",
    "\n",
    "# data_range = range(7333, 10178)\n",
    "\n",
    "# data_range = range(860, 1273)\n",
    "\n",
    "# data_range = range(889, 1000)\n",
    "\n",
    "topK_searchEngine = 150\n",
    "topK_SPLADE = 30\n",
    "topK_denseEmbedding = 0\n",
    "topK_crossEncoder = 5\n",
    "topK_LLM = 0\n",
    "score_threshold = None #0.7 # 0.8\n",
    "pick_rag_index = None\n",
    "get_classifier_training_data = False\n",
    "use_classifier = False\n",
    "RRF_models = None #[(\"SPLADE\", 0.8), (\"MonoT5\", 1), (\"Vicuna\", 1)] # [\"SPLADE\", \"DenseEmbedding\", \"MonoT5\", \"Zephyr\", \"Vicuna\"]\n",
    "\n",
    "# file_name = \"9-30-test_data_150_30_5_H100.txt\"\n",
    "# file_name = \"a.txt\"\n",
    "# file_name = \"10-14-lora-context012-r16-epoch3-150-30-5-all-data.txt\"\n",
    "# file_name = \"10-21-batchsize8-lr1e-4-bf16-new-lora-150-30-5-RRF.txt\"\n",
    "# file_name = \"11-4-150-30-5-1-MedQA-ensemble.txt\"\n",
    "# file_name = \"11-7-no-rag-ensemble-MedQA-A100.txt\"\n",
    "# file_name = \"11-13-Phi3-150-dense30-monoT51.txt\"\n",
    "# file_name = \"11-16-Phi3-1000-300-80-5-1.txt\"\n",
    "# file_name = \"11-18-Phi3-150-30-1-MonoT5-MedMCQA.txt\"\n",
    "# file_name = \"11-25-Phi3-no-rag-ensemble-train.txt\"\n",
    "# file_name = \"11-26-Phi4-no-rag-MedQA.txt\"\n",
    "# file_name = \"11-26-Phi2.5-3b-no-rag-MedMCQA.txt\"\n",
    "# file_name = \"12-1-Phi3-150-30-1-MiniLM-MMLU-College_medicine.txt\"\n",
    "# file_name = \"12-3-Phi3-batchsize8-epoch5-MedQA-ensemble.txt\"\n",
    "# file_name = \"12-5-qwen-4b-thinking-epoch5-MedQA.txt\"\n",
    "# file_name = \"12-8-Phi3-epoch5-lora01-MedQA-150-30-1-Embedding.txt\"\n",
    "file_name = \"12-8-flan-t5-xl-MedQA-150-30-1.txt\"\n",
    "\n",
    "# test.SPLIT = \"train\"\n",
    "test.test_accuracy(DatasetPath.MedQA, subset_name = None, is_ensemble = False, use_RAG = True, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bb94f-c47d-4e92-9503-a873737c72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"10-22-150-30-5-1-MMLU-College_medicine.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MMLU, subset_name = MMLU_Subset.College_medicine, is_ensemble = False, use_RAG = False, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0a7ea-499c-404e-85c6-d483ed7db3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"10-22-150-30-5-1-MMLU-College_biology.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MMLU, subset_name = MMLU_Subset.College_biology, is_ensemble = False, use_RAG = False, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21e5bd-164d-4cef-a440-0cbc8b753a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"10-22-150-30-5-1-MMLU-Anatomy.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MMLU, subset_name = MMLU_Subset.Anatomy, is_ensemble = False, use_RAG = False, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadd8ba-2342-44b2-8995-be126c456a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"10-22-150-30-5-1-MMLU-Medical_genetics.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MMLU, subset_name = MMLU_Subset.Medical_genetics, is_ensemble = False, use_RAG = False, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850ee99-e581-4643-a5d2-a2187f98fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"10-22-150-30-5-1-MMLU-Clinical_knowledge.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MMLU, subset_name = MMLU_Subset.Clinical_knowledge, is_ensemble = False, use_RAG = False, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_denseEmbedding = topK_denseEmbedding, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index, get_classifier_training_data = get_classifier_training_data,\n",
    "                  use_classifier = use_classifier, RRF_models = RRF_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82f5ba-8f30-403f-bd01-4d8205710349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_range = [9, 13, 19, 24, 28, 29, 30, 31, 32, 38, 43, 44, 45, 46, 52, 56, 57, 62, 64, 68, 71, 73, 77, 87, 90, 94, 96, 97, 108, \n",
    "#               109, 110, 118, 123, 126, 133, 135, 139, 141, 143, 144, 145, 146, 159, 160, 166, 167, 170, 171, 172, 173, 179, 180, 181, 183, \n",
    "#               185, 186, 188, 191, 196, 197, 203, 205]\n",
    "\n",
    "# data_range = range(4, 5) # None #range(34, 35)\n",
    "\n",
    "data_range = None\n",
    "\n",
    "# data_range = range(763, 1273)\n",
    "\n",
    "topK_searchEngine = 150\n",
    "topK_SPLADE = 30\n",
    "topK_crossEncoder = 5\n",
    "topK_LLM = 0\n",
    "score_threshold = None # 0.8\n",
    "pick_rag_index = 4\n",
    "\n",
    "file_name = \"9-5-150_30_5th_RAG_data_H100.txt\"\n",
    "# file_name = \"a.txt\"\n",
    "\n",
    "test.test_accuracy(DatasetPath.MedQA, subset_name = None, is_ensemble = False, use_RAG = True, data_range = data_range, file_name = file_name,\n",
    "                  topK_searchEngine = topK_searchEngine, topK_SPLADE = topK_SPLADE, topK_crossEncoder = topK_crossEncoder, topK_LLM = topK_LLM,\n",
    "                   score_threshold = score_threshold, pick_rag_index = pick_rag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95867a73-0d71-4739-8d85-68ba0cd8b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_prompt = \"\"\"You are a medical question answering assistant.\n",
    "\n",
    "\n",
    "\n",
    "Question:\n",
    "A 2-year-old boy is brought to the physician because of decreased appetite and abdominal pain for the last several weeks. Physical examination shows a well-appearing toddler with a palpable left-sided abdominal mass that does not cross the midline. A CT of the abdomen shows a large, necrotic tumor on the left kidney. Histological examination of the kidney mass shows primitive blastemal cells and immature tubules and glomeruli. This tissue is most likely derived from the same embryological structure as which of the following?\n",
    "\n",
    "[A] : Adrenal medulla\n",
    "[B] : Thyroid gland\n",
    "[C] : Papillary muscles\n",
    "[D] : Anterior pituitary\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": f\"{model_prompt}\"}]\n",
    "\n",
    "inputs = trainer.tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = \"pt\").to(trainer.device)\n",
    "\n",
    "generate_kwargs = { \"max_new_tokens\": 1024, \"do_sample\": False }\n",
    "\n",
    "outputs = trainer.model.generate(inputs, **generate_kwargs)\n",
    "\n",
    "text = trainer.tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc516c-a20d-4ff8-b66e-07af418003ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "QUESTION_COUNT = 1273\n",
    "\n",
    "path = \"\"\n",
    "file_names = [\"9-5-150_30_1_RAG_data_H100.txt\", \"9-5-150_30_2nd_RAG_data_H100.txt\", \"9-5-150_30_3rd_RAG_data_H100.txt\", \n",
    "              \"9-5-150_30_4th_RAG_data_H100.txt\", \"9-5-150_30_5th_RAG_data_H100.txt\"]\n",
    "\n",
    "full_file_names = [path + file_name for file_name in file_names]\n",
    "\n",
    "class QuestionAnswerData:\n",
    "    def __init__(self, answer, correct_answer, RAG_data):\n",
    "        self.answer = answer\n",
    "        self.correct_answer = correct_answer\n",
    "        self.is_correct = answer == correct_answer\n",
    "        # RAG data: [{'docId': 'pubmed23n0827_1593', 'BM25_score': 95.4198, 'BM25_ranking': 44, 'SPLADE_score': 54.33488464355469, 'SPLADE_ranking': 13, 'MonoT5_score': 0.9597430488962883, 'MonoT5_ranking': 1}]\n",
    "        self.RAG_data = RAG_data\n",
    "\n",
    "# Get the data\n",
    "\n",
    "# [ [QuestionAnswerData, QuestionAnswerData, ... (1273 data)], [...], [...], [...], [...] ]\n",
    "all_question_answer_data_list = []\n",
    "\n",
    "for i in range(len(full_file_names)):\n",
    "    question_answer_data_list = []\n",
    "    with open(full_file_names[i], \"r\") as f:\n",
    "        count = 0\n",
    "        curr_RAG_data = None\n",
    "        for line in f:\n",
    "            if \"RAG data: \" in line:\n",
    "                curr_RAG_data_str = line[line.find(\"RAG data: \") + len(\"RAG data: \"):].strip()\n",
    "                curr_RAG_data = ast.literal_eval(curr_RAG_data_str)\n",
    "\n",
    "            elif \"question \" in line:\n",
    "                \n",
    "                # Check question number error\n",
    "                count += 1\n",
    "                if \"num:\" in line:\n",
    "                    number = line.strip().split('num:')[-1].split(' ')[0]\n",
    "                else:\n",
    "                    number = line.strip().split('/')[0].split(' ')[-1]\n",
    "                if int(number) != count:\n",
    "                    print(f\"Error: {file_names[i]}:\", number, count)\n",
    "                    break\n",
    "\n",
    "                match = re.search(r\"answer:([^\\s]+)\\s+correct_answer:([^\\s]+)\", line)\n",
    "                if match:\n",
    "                    answer = match.group(1)\n",
    "                    correct_answer = match.group(2)\n",
    "\n",
    "                    question_answer_data = QuestionAnswerData(answer, correct_answer, curr_RAG_data)\n",
    "                    question_answer_data_list.append(question_answer_data)\n",
    "    \n",
    "    all_question_answer_data_list.append(question_answer_data_list)\n",
    "\n",
    "print(len(all_question_answer_data_list[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2315c00-de10-4c9f-b8fd-5d6a3cf70784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
