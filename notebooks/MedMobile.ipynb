{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51748dc1-cd64-4dae-9a28-0b5f9f0447b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mKrithikV/MedMobile\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"KrithikV/MedMobile\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800514f2-9689-4f3d-9134-b17f6538245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FOR TEST\")\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
    "Choices:\n",
    "A: Disclose the error to the patient but leave it out of the operative report\n",
    "B: Disclose the error to the patient and put it in the operative report\n",
    "C: Tell the attending that he cannot fail to disclose this mistake\n",
    "D: Report the physician to the ethics committee\n",
    "E: Refuse to dictate the operative report\n",
    "Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "print (\"Prompt: \", prompt)\n",
    "print (\"--------------\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\", padding = False).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 50,\n",
    "        # attention_mask = inputs[\"attention_mask\"],\n",
    "        # pad_token_id = tokenizer.eos_token_id,\n",
    "        use_cache = False,  # For \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        # do_sample = False,\n",
    "    )\n",
    "        \n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "answer = answer.replace(prompt, \"\").strip()\n",
    "\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n",
    "print (\"--------------\")\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb58c08-c2ab-4735-a002-caa55811e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "folder_path = \"/home/sheju347/.cache/\"\n",
    "size_in_mb = get_folder_size(folder_path) / (1024 * 1024 * 1024)\n",
    "print(f\"Folder size: {size_in_mb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65443b0f-0789-4557-83d4-05cd2e07022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def remove_all():\n",
    "    shutil.rmtree(\"/home/sheju347/.cache/\", ignore_errors=True)\n",
    "\n",
    "remove_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b09aa36-0280-4bd1-9551-917a6efa45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32K\t./Desktop\n",
      "9.6G\t./Documents\n",
      "32K\t./Downloads\n",
      "24K\t./Flan-T5-Medical.ipynb\n",
      "56K\t./MedMobile.ipynb\n",
      "64K\t./Medical_QA_test.ipynb\n",
      "198M\t./Miniconda3-latest-Linux-x86_64.sh\n",
      "32K\t./Music\n",
      "72K\t./Phi3_medical.ipynb\n",
      "32K\t./Pictures\n",
      "32K\t./Public\n",
      "32K\t./Templates\n",
      "80K\t./Test_DL.ipynb\n",
      "24K\t./Test_fine_tuned.ipynb\n",
      "72K\t./Train_Phi_3_mini.ipynb\n",
      "32K\t./Videos\n",
      "616K\t./code\n",
      "455M\t./data\n",
      "56K\t./dialogsum_train.ipynb\n",
      "39M\t./ondemand\n",
      "32K\t./saved\n",
      "24K\t./.ICEauthority\n",
      "24K\t./.Xauthority\n",
      "32K\t./.apptainer\n",
      "48K\t./.bash_history\n",
      "24K\t./.bashrc\n",
      "21G\t./.cache\n",
      "24K\t./.condarc\n",
      "2.5M\t./.config\n",
      "88K\t./.dbus\n",
      "64K\t./.gnupg\n",
      "24K\t./.icons\n",
      "488K\t./.ipynb_checkpoints\n",
      "2.2M\t./.ipython\n",
      "520K\t./.jupyter\n",
      "56K\t./.krb5\n",
      "5.6G\t./.local\n",
      "14M\t./.mozilla\n",
      "472K\t./.npm\n",
      "64K\t./.nv\n",
      "24K\t./.python_history\n",
      "104K\t./.slurm-apptainer\n",
      "56K\t./.ssh\n",
      "56K\t./.vnc\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./* ./.??*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a947a4d9-eafa-4521-921c-974c7ff5130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "684M\t.cache/huggingface/hub/datasets--TsinghuaC3I--UltraMedical/\n",
      "14M\t.cache/huggingface/hub/datasets--knkarthick--dialogsum/\n",
      "9.6G\t.cache/huggingface/hub/models--KrithikV--MedMobile/\n",
      "956M\t.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-instruct/\n",
      "1.3G\t.cache/huggingface/hub/models--google--flan-t5-base/\n",
      "9.5G\t.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/\n"
     ]
    }
   ],
   "source": [
    "!du -sh .cache/huggingface/hub/*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820099e-4583-452e-bf3c-54ea48421066",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh .local/lib/* .local/lib/.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20bcea-2b7e-4a55-8777-98042fdc4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -type d -name '.*' -exec du -sh {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce3ea9-e614-4076-9c81-10336bccb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769f9bb-2a74-4afb-8be9-f9add104973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9d916c-ef29-4e4f-ba3f-ad7a68b36b58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from openai import OpenAI\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "# from openai import OpenAI\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# Set openai key if using gpt4o as engine.\n",
    "# os.environ['OPENAI_API_KEY'] = \"OPEN AI KEY HERE\"\n",
    "\n",
    "def return_parted_rows(df, part_ind, part_ind_list):\n",
    "    # Extract chapter numbers from the 'chapters' column\n",
    "    df['chapter_num'] = df['current_chapter'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    \n",
    "    # Determine the start and end range for the given part_ind\n",
    "    start = part_ind_list[part_ind]\n",
    "    end = part_ind_list[part_ind + 1] - 1 if part_ind + 1 in part_ind_list else df['chapter_num'].max()\n",
    "\n",
    "    # Filter the DataFrame to include only rows with chapters within the range\n",
    "    df_filtered = df[(df['chapter_num'] >= start) & (df['chapter_num'] <= end)]\n",
    "    \n",
    "    # Drop the temporary 'chapter_num' column if not needed\n",
    "    df_filtered = df_filtered.drop(columns=['chapter_num'])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def format_choices(choices):\n",
    "    a = zip(list(choices.keys()), choices.values())\n",
    "    final_answers = []\n",
    "    for x,y in a:\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)\n",
    "\n",
    "    \n",
    "def format_examples(examples):\n",
    "    formatted_examples = []\n",
    "    for row in examples:\n",
    "        example = f'## Question {row[\"question\"]} \\n ## Answer {row[\"answer\"]}'\n",
    "        formatted_examples.append(example)\n",
    "    return \"\\n\".join(formatted_examples)\n",
    "\n",
    "def extract_samples(task, numShot, model_prompt):\n",
    "    questions, answer_choices, correct_answers = task_load(task, 'train')\n",
    "    example_indexes = random.sample(range(len(questions)), numShot)\n",
    "    example_list = []\n",
    "    for i in example_indexes:\n",
    "        example_list.append(model_prompt.format(question=questions[i], choices=format_choices(answer_choices[i]), answer=correct_answers[i]))\n",
    "    return example_list\n",
    "\n",
    "def task_load(task, split):\n",
    "    if task==\"medqa\":\n",
    "        ds = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=split)\n",
    "        questions = [ds[i]['question'] for i in range(len(ds))]\n",
    "        answer_choices = [ds[i]['options'] for i in range(len(ds))]\n",
    "        correct_answers = [ds[i]['answer_idx'] for i in range(len(ds))]\n",
    "        return questions, answer_choices, correct_answers\n",
    "    \n",
    "    elif task==\"medmcqa\":\n",
    "        if split == 'test':\n",
    "            split = 'validation'\n",
    "        ds = load_dataset(\"openlifescienceai/medmcqa\", split=split)\n",
    "        questions = [ds[i]['question'] for i in range(len(ds))]\n",
    "        answer_choices = [{\"A\": ds[i]['opa'], \"B\": ds[i]['opb'], \"C\": ds[i]['opc'], \"D\": ds[i]['opd']} for i in range(len(ds))]\n",
    "        correct_answers = [chr(ds[i]['cop']+65) for i in range(len(ds))]\n",
    "        return questions, answer_choices, correct_answers\n",
    "    \n",
    "    elif task==\"medbullets_op4\":\n",
    "        path = \"ADD MEDBULLETS PATH HERE\"\n",
    "        with open(path, 'r') as file:\n",
    "            ds = json.load(file)\n",
    "        questions = [ds['question'].values()]\n",
    "        answer_choices = [{\"A\": ds['opa'][str(i)], \"B\": ds['opb'][str(i)], \"C\": ds['opc'][str(i)], \"D\": ds['opd'][str(i)]} for i in range(len(ds))]\n",
    "        correct_answers = [ds['answer_idx'].values()]\n",
    "        return questions, answer_choices, correct_answers\n",
    "\n",
    "    elif task==\"medbullets_op5\":\n",
    "        path = \"ADD MEDBULLETS PATH HERE\"\n",
    "        with open(path, 'r') as file:\n",
    "            ds = json.load(file)\n",
    "        questions = [ds['question'].values()]\n",
    "        answer_choices = [{\"A\": ds['opa'][str(i)], \"B\": ds['opb'][str(i)], \"C\": ds['opc'][str(i)], \"D\": ds['opd'][str(i)]} for i in range(len(ds))]\n",
    "        correct_answers = [ds['answer_idx'].values()]\n",
    "        return questions, answer_choices, correct_answers\n",
    "    \n",
    "    elif task==\"pubmedqa\":\n",
    "        # This also contains context that is necessary for the question.\n",
    "        path = \"ADD PATH FOR PUBMEDQA HERE\"\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            ds = json.load(file)\n",
    "        ds = list(ds.values())\n",
    "        answer_choice_dict = {'A': \"yes\", 'B': \"no\", 'C': \"maybe\"}\n",
    "        answer_choices = [answer_choice_dict]*len(ds)\n",
    "        correct_answers = []\n",
    "        questions = []\n",
    "        for i in range(len(ds)):\n",
    "            question_context = \"Context: \" + \"\\nContext: \".join(ds[i]['CONTEXTS'])\n",
    "            questions.append(question_context + \"\\n\" + ds[i]['QUESTION'])\n",
    "\n",
    "            rev_answer_choice_dict = dict((v,k) for k,v in answer_choice_dict.items())\n",
    "            answer = rev_answer_choice_dict.get(ds[i]['final_decision'])\n",
    "            correct_answers.append(answer)\n",
    "        return questions, answer_choices, correct_answers\n",
    "    \n",
    "    elif \"mmlu\" in task:\n",
    "        subset = task.split(\"-\", 1)[1]\n",
    "        ds = load_dataset(\"cais/mmlu\", subset, split=split)\n",
    "        questions = [ds[i]['question'] for i in range(len(ds))]\n",
    "        answer_choices = [{\"A\": ds[i]['choices'][0], \"B\": ds[i]['choices'][1], \"C\": ds[i]['choices'][2], \"D\": ds[i]['choices'][3]} for i in range(len(ds))]\n",
    "        correct_answers = [chr(ds[i]['answer']+65) for i in range(len(ds))]\n",
    "        return questions, answer_choices, correct_answers\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"TASK NOT FOUND\")\n",
    "\n",
    "def filterContext(context):\n",
    "    end_tag = \"</end>\"\n",
    "    if end_tag in context:\n",
    "        return context.split(end_tag)[0] + end_tag\n",
    "    return context\n",
    "\n",
    "def run_inference(content, engine, temp=0.0001, max_tokens_output=200, tokenizer=None, model=None, local=False, vllm = False):\n",
    "    if local:\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to('cuda:0')\n",
    "        outputs = model.generate(inputs, max_new_tokens=max_tokens_output, do_sample = True, temperature=temp)\n",
    "        text = tokenizer.batch_decode(outputs)[0]\n",
    "        return text.split(\"<|assistant|>\")[-1]\n",
    "    elif vllm:\n",
    "        return None\n",
    "    else:\n",
    "        return None\n",
    "        # client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        # messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "        # response = client.chat.completions.create(\n",
    "        #     model=engine,\n",
    "        #     messages=messages,\n",
    "        #     temperature=temp,\n",
    "        #     max_tokens=max_tokens_output,\n",
    "        #     frequency_penalty=0.0\n",
    "        # )\n",
    "        # response_text = response.choices[0].message.content\n",
    "        # return response_text\n",
    "    \n",
    "class MultiChoiceFilter:\n",
    "    # Inspiring from lmeval\n",
    "    def __init__(self, ignore_case=False, ignore_punctuation=False, regex_pattern=r\"[\\(\\[]([A-Z])[\\)\\]]\"):\n",
    "        \n",
    "        self.ignore_case = ignore_case\n",
    "        self.ignore_punctuation = ignore_punctuation\n",
    "        self.regex_pattern = regex_pattern\n",
    "        self.regex = re.compile(regex_pattern)\n",
    "        self.punct_tbl = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                                       if unicodedata.category(chr(i)).startswith(\"P\"))\n",
    "\n",
    "    def filter_text(self, text):\n",
    "        if self.ignore_case:\n",
    "            text = text.lower()\n",
    "        if self.ignore_punctuation:\n",
    "            text = text.translate(self.punct_tbl)\n",
    "        return text\n",
    "\n",
    "    def find_match(self, regex, resp, convert_dict={}):\n",
    "        match = regex.findall(resp)\n",
    "        if match:\n",
    "            match = match[-1]\n",
    "            if isinstance(match, tuple):\n",
    "                match = [m for m in match if m][0]\n",
    "            match = match.strip()\n",
    "            if match and match in convert_dict: \n",
    "                match = convert_dict[match]\n",
    "        return match\n",
    "\n",
    "    def extract_answer(self, response, choices=None):\n",
    "        matchFirst = re.search(r'the answer is .(\\w).', response)\n",
    "        if matchFirst:\n",
    "            return f\"({matchFirst.group(1)})\"\n",
    "        match = self.find_match(self.regex, response) \n",
    "        if match:\n",
    "            return f\"({match})\"\n",
    "        return \"[invalid]\"\n",
    "\n",
    "    def filter_responses(self, responses, choices):\n",
    "        return [self.extract_answer(resp, choices) for resp in responses]\n",
    "\n",
    "\n",
    "prompt_eval_bare_fully = f'''\n",
    "{{question}} \\n\n",
    "{{choices}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d01db-7f82-4f32-867c-aa261e89c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "ENGINE = \"KrithikV/MedMobile\"\n",
    "TASK_LIST = ['medqa'] # Options [\"medqa\", 'mmlu-anatomy', 'mmlu-professional_medicine', 'mmlu-college_biology', 'mmlu-college_medicine', 'mmlu-clinical_knowledge', 'mmlu-medical_genetics', pubmedqa, medmcqa\"]\n",
    "SPLIT = \"test\"\n",
    "NUMBER_OF_ENSEMBLE = 5\n",
    "if NUMBER_OF_ENSEMBLE > 1:\n",
    "    ENGINE_TEMPERATURE = 0.7\n",
    "else:\n",
    "    ENGINE_TEMPERATURE = 0.000000001 \n",
    "MAX_TOKEN_OUTPUT = 1024\n",
    "NSHOT = 0\n",
    "\n",
    "TEST_QUESTION_NUM = 10000000 # for debug purpose, stop after testing this number of questions\n",
    "\n",
    "OUTPUT_DIR = \"./Documents/\" #\"OUTPUT DIRECTORY HERE\"\n",
    "results_db = {\n",
    "    \"metadata\": {\n",
    "        \"model\" : ENGINE,\n",
    "        \"temperature\" : ENGINE_TEMPERATURE,\n",
    "        \"num_shot\" : NSHOT,\n",
    "        \"number_of_ensemble\": NUMBER_OF_ENSEMBLE,\n",
    "        \"max_tokens\" : MAX_TOKEN_OUTPUT,\n",
    "    }\n",
    "}\n",
    "\n",
    "if NUMBER_OF_ENSEMBLE > 1:\n",
    "    runName = f'MedMobile ({ENGINE}) + Ensemble ({NUMBER_OF_ENSEMBLE})'\n",
    "else: \n",
    "    runName = f'MedMobile ({ENGINE})'\n",
    "\n",
    "## DISPLAY HYPERPARAMETERS\n",
    "for name, value in results_db['metadata'].items():\n",
    "    print(f\"{name} : {value}\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(ENGINE, trust_remote_code = False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(ENGINE, trust_remote_code = False) # Setting trust_remote_code = True will cause AttributeError: 'DynamicCache' object has no attribute 'get_max_length'\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(\"Model Running: \" + ENGINE)\n",
    "print(\"Run: \" + runName)\n",
    "\n",
    "mcf = MultiChoiceFilter(ignore_case=True, ignore_punctuation=True)\n",
    "\n",
    "for task in TASK_LIST:\n",
    "    question_list, answer_choices_list, correct_answer_list = task_load(task, SPLIT)\n",
    "    # print(\"question_list\", len(question_list), question_list[0])\n",
    "    print(f\"{task} loaded succesfully. Now conducting evaluation on {len(question_list)} samples.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_db = []\n",
    "\n",
    "    # tqdm: show progress bar\n",
    "    for i, (question, answer_choices, correct_answer) in tqdm(enumerate(zip(question_list, answer_choices_list, correct_answer_list))):\n",
    "        D = {}\n",
    "        context = \"\"\n",
    "\n",
    "        prompt = prompt_eval_bare_fully # NOT use RAG, not use k-shot prompting (NSHOT == 0)\n",
    "\n",
    "        \n",
    "        model_prompt = prompt.format(question=question, choices=format_choices(answer_choices), context = filterContext(context))\n",
    "\n",
    "        # print(model_prompt)\n",
    "        # print(correct_answer)\n",
    "\n",
    "        D[\"query\"] = question\n",
    "        D[\"question_choices\"] = answer_choices\n",
    "        D[\"correct_answer\"] = correct_answer\n",
    "        D[\"attempts\"] = []\n",
    "        D[\"model_prompt\"] = model_prompt\n",
    "        \n",
    "        for j in range(NUMBER_OF_ENSEMBLE):\n",
    "            # run_inference()\n",
    "            max_tokens_output = MAX_TOKEN_OUTPUT \n",
    "            messages = [{\"role\": \"user\", \"content\": f\"{model_prompt}\"}]\n",
    "            inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device) #.to('cuda:0')\n",
    "            outputs = model.generate(inputs, \n",
    "                                     max_new_tokens=max_tokens_output, \n",
    "                                     do_sample = True, \n",
    "                                     temperature=ENGINE_TEMPERATURE,\n",
    "                                     # use_cache = False, # TODO solve the problem\n",
    "                                    )\n",
    "            text = tokenizer.batch_decode(outputs)[0]\n",
    "            text = text.split(\"<|assistant|>\")[-1]\n",
    "\n",
    "            query_object = {\"id\": (\"attemp_\" + str(j)), \"COT\": text}\n",
    "            D[\"attempts\"].append(query_object)\n",
    "\n",
    "            # print(j, \": \", text)\n",
    "\n",
    "        model_db.append(D)\n",
    "\n",
    "        if i >= TEST_QUESTION_NUM - 1:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_num_questions = 0\n",
    "    num_correct = 0\n",
    "    num_invalid = 0\n",
    "    for q in model_db:\n",
    "        choices = q[\"question_choices\"]\n",
    "        letter_counts = {}\n",
    "        for attempt in q[\"attempts\"]:\n",
    "            model_choice = mcf.extract_answer(attempt[\"COT\"], choices)\n",
    "            attempt[\"model_choice\"] = model_choice\n",
    "            if model_choice in letter_counts:\n",
    "                letter_counts[model_choice] += 1\n",
    "            else:\n",
    "                letter_counts[model_choice] = 1\n",
    "        max_count = 0\n",
    "        for letter, count in letter_counts.items():\n",
    "            if count > max_count:\n",
    "                q[\"ensemble_answer\"] = letter\n",
    "                max_count = count\n",
    "\n",
    "        print(\"Ansemble answer: \", q[\"ensemble_answer\"], \" Correct answer: \", q[\"correct_answer\"])\n",
    "        \n",
    "        total_num_questions += 1\n",
    "        if q[\"ensemble_answer\"].strip(\"()\") == q[\"correct_answer\"]:\n",
    "            num_correct += 1\n",
    "        elif q[\"ensemble_answer\"] == \"[invalid]\":\n",
    "            num_invalid += 1\n",
    "\n",
    "    print(\"Number of correct answer: \" + str(num_correct))\n",
    "    print(\"Total number of questions: \" + str(total_num_questions))\n",
    "    print(\"Model accuracy: \" + str(num_correct / total_num_questions))\n",
    "\n",
    "\n",
    "    results_db_task = results_db.copy()\n",
    "    results_db_task['metadata']['informal_run_name'] = runName\n",
    "    results_db_task['metadata']['task'] = task\n",
    "    results_db_task['metadata']['timestamp'] = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    results_db_task['metadata']['prompt'] = prompt\n",
    "    results_db_task['metadata']['number_of_invalids'] = num_invalid\n",
    "    results_db_task['metadata']['number_of_questions'] = total_num_questions\n",
    "    results_db_task['metadata']['true_accuracy'] = num_correct / total_num_questions\n",
    "    results_db_task['metadata']['eff_accuracy'] = num_correct/ (total_num_questions - num_invalid)\n",
    "    results_db_task['metadata']['run_time'] = end_time - start_time\n",
    "    results_db_task['metadata']['run_time_per_iteration'] = (end_time - start_time) / total_num_questions\n",
    "    results_db_task['model_results'] = model_db\n",
    "    \n",
    "    filename = f\"{OUTPUT_DIR}{task}/query_database_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(results_db_task, file, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761557e-3600-4ac8-b60c-071df4ebc25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
