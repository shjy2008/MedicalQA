{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558f200e-5334-4100-afe1-f0347957ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612622f5-3405-400f-8a27-f567dc56789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73d11e4-e511-4590-a0b6-f54dc55ec694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-10-13 10:21:03,745 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-10-13 10:21:03,746 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template validated successfully!\n",
      "Successfully created pointwise inference handler!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from context_retriever import ContextRetriever\n",
    "\n",
    "\n",
    "retriever = ContextRetriever()\n",
    "retriever.set_params(topK_searchEngine = 150, topK_SPLADE = 30, topK_crossEncoder = 5, topK_LLM = None, score_threshold = None, \n",
    "                     pick_rag_index = None, use_classifier = False)\n",
    "\n",
    "# question = \"A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\"\n",
    "# formated_choices = \"\"\n",
    "# context = retriever.get_RAG_context(question, formated_choices)\n",
    "# print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d69db9-a8a1-4f29-bb16-a21c3f84cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"TsinghuaC3I/UltraMedical\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6710c4-f973-4ac8-be85-5737c460020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "medqa_subset = dataset.filter(lambda d: d[\"id\"].startswith(\"MedQA,\"))\n",
    "print(medqa_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b14bdea-9793-4a58-94d7-2c5a0a71aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'type', 'conversations', 'answer', 'score'],\n",
      "    num_rows: 51809\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "medqa_evol_subset = dataset.filter(lambda d: d[\"id\"].startswith(\"MedQA-Evol,\"))\n",
    "print(medqa_evol_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846fb87a-7b45-4f02-9c99-564641849d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_choices(choices):\n",
    "    final_answers = []\n",
    "    for i, (x, y) in enumerate(choices.items()):\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)\n",
    "\n",
    "    \n",
    "def convert_to_chat_format(conversations, is_input):\n",
    "    chat = []\n",
    "    for conversation in conversations:\n",
    "        if conversation[\"from\"] == \"human\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}] :\")\n",
    "            chat.append({\"role\": \"user\", \"content\": value})\n",
    "        elif is_input == False and conversation[\"from\"] == \"gpt\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}].\")\n",
    "            chat.append({\"role\": \"assistant\", \"content\": value})\n",
    "    return chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573d2047-24f7-41d0-92af-54b054bb5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Log\n",
    "# log_file_name = \"10-2-MedQA_context_training.txt\"\n",
    "log_file_name = \"10-8-MedQA_Evol_context_training.txt\"\n",
    "# log_file_name = \"a.txt\"\n",
    "\n",
    "# Remove all handlers associated with the root logger\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Set log file config\n",
    "logging.basicConfig(\n",
    "    filename = log_file_name,      # Log file name\n",
    "    filemode = 'a',                    # Append mode\n",
    "    format = '%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level = logging.INFO               # Log level\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c3f1b-55e5-4a5d-a3eb-64c16a251ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/9440\n",
      "3/9440\n",
      "4/9440\n",
      "5/9440\n"
     ]
    }
   ],
   "source": [
    "# subset = medqa_subset\n",
    "# subset = medqa_evol_subset\n",
    "\n",
    "start_index = 42369\n",
    "subset = medqa_evol_subset.select(range(start_index, len(medqa_evol_subset)))\n",
    "\n",
    "for i in range(len(subset)):\n",
    "    print(f\"{i+1}/{len(subset)}\")\n",
    "    logging.info(f\"[question index:]{i + start_index}\")\n",
    "    \n",
    "    data = subset[i]\n",
    "    questions_and_answers = convert_to_chat_format(data[\"conversations\"], is_input = False)\n",
    "\n",
    "    question_and_options = questions_and_answers[0][\"content\"]\n",
    "    \n",
    "    logging.info(f\"[question_and_options:]{question_and_options}\")\n",
    "    \n",
    "    # print(questions_and_answers[0][\"content\"])\n",
    "\n",
    "    # These few lines of code is for only one document\n",
    "    doc_data_list = retriever.get_RAG_data_list(question_and_options, \"\")\n",
    "    for doc_idx in range(len(doc_data_list)):\n",
    "        doc_data = doc_data_list[doc_idx]\n",
    "        doc = doc_data[\"content\"]\n",
    "        logging.info(f\"[rag_doc:]{doc_idx}:{doc}\")\n",
    "        # print(doc)\n",
    "\n",
    "    # # These few lines of code is for 2 or more documents\n",
    "    # context = retriever.get_RAG_context(question_and_options, \"\")\n",
    "    # logging.info(f\"[rag_doc:]{0}:{context}\")\n",
    "\n",
    "\n",
    "    expected_output = questions_and_answers[1][\"content\"]\n",
    "    logging.info(f\"[expected_output:]{expected_output}\")\n",
    "\n",
    "    # if i > 4:\n",
    "    #     break\n",
    "\n",
    "    # question = data[\"question\"]\n",
    "    # choices = data[\"options\"]\n",
    "    # print(question, choices)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab07278-7d6f-465b-a2a6-85de4cd23874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
