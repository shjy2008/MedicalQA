{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427d9f00-b7d9-4c72-ac55-3c80a5c9c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c1ba21-b35c-43cb-8bb3-1c6c21994fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2e7065-e5ee-499a-bb31-0ad178d88684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343\n",
      "9343\n"
     ]
    }
   ],
   "source": [
    "# file_name = \"../10-3-test_training_data.txt\"\n",
    "file_name_list = [\"10-2-MedQA_context_training.txt\", \n",
    "                  # \"10-8-MedQA_Evol_context_training.txt\"\n",
    "                 ]\n",
    "\n",
    "class TrainingData:\n",
    "    def __init__(self):\n",
    "        self.question_and_options = \"\"\n",
    "        self.doc_list = []\n",
    "        self.expected_output = \"\"\n",
    "\n",
    "training_data_list = []\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    with open(file_name) as f:\n",
    "        question_index = None\n",
    "        is_reading_question = False\n",
    "        is_reading_answer = False\n",
    "        current_training_data = None\n",
    "        for line in f:\n",
    "            key = \"[question index:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                # print(question)\n",
    "                \n",
    "                question_index = int(line[start + len(key):].strip())\n",
    "                # print(question_index)\n",
    "    \n",
    "                current_training_data = TrainingData()\n",
    "                training_data_list.append(current_training_data)\n",
    "                \n",
    "                is_reading_question = False\n",
    "                is_reading_answer = False\n",
    "                continue\n",
    "                \n",
    "            key = \"[question_and_options:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                current_training_data.question_and_options += line[start + len(key):]\n",
    "                is_reading_question = True\n",
    "                continue\n",
    "    \n",
    "    \n",
    "            key = \"[rag_doc:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                doc_idx_and_doc = line[start + len(key):]\n",
    "                split_pos = doc_idx_and_doc.find(\":\")\n",
    "                doc_idx = int(doc_idx_and_doc[:split_pos])\n",
    "                doc = doc_idx_and_doc[split_pos + 1:]\n",
    "                current_training_data.doc_list.append(doc)\n",
    "                \n",
    "                is_reading_question = False\n",
    "                continue\n",
    "    \n",
    "            key = \"[expected_output:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                expected_output = line[start + len(key):]\n",
    "                current_training_data.expected_output += expected_output\n",
    "                is_reading_answer = True\n",
    "                continue\n",
    "            \n",
    "            if is_reading_question:\n",
    "                current_training_data.question_and_options += line\n",
    "                continue\n",
    "                \n",
    "            if is_reading_answer:\n",
    "                current_training_data.expected_output += line\n",
    "                continue\n",
    "                \n",
    "            # if \"[rag_doc:]\" in line:\n",
    "            #     doc = line(\n",
    "\n",
    "print(len(training_data_list))\n",
    "\n",
    "if len(training_data_list[len(training_data_list) - 1].doc_list) == 0:\n",
    "    del training_data_list[len(training_data_list) - 1]\n",
    "\n",
    "print(len(training_data_list))\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].question_and_options)\n",
    "# print(\"------------\")\n",
    "# print(len(training_data_list[0].doc_list))\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].doc_list[0])\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].expected_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a60766-c7de-41e5-b28b-4b068805de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- start checking GPU -----------\n",
      "GPU: NVIDIA H100 NVL\n",
      "torch.cuda.is_bf16_supported():  True\n",
      "---------- finish checking GPU -----------\n",
      "---------- start loading model:/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000 -----------\n",
      "finish loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading model\n",
      "torch_dtype: torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "---------- finish loading model:/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000 -----------\n"
     ]
    }
   ],
   "source": [
    "from model_trainer import ModelTrainer\n",
    "trainer = ModelTrainer()\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/10-15-UltraMedical-batchsize8-bf16\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-11-Qwen3-4B-base-UltraMedical\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-1-phi3-mini-batchsize8-epoch456/checkpoint-256000\"\n",
    "model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-batch8-epoch456/checkpoint-204800\"\n",
    "\n",
    "# model_name = \"TsinghuaC3I/Llama-3.1-8B-UltraMedical\"\n",
    "\n",
    "tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/\"\n",
    "# tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-UltraMedical\"\n",
    "trainer.load_model(model_name, tokenizer_path = tokenizer_path)\n",
    "\n",
    "tokenizer = trainer.tokenizer\n",
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3701141c-8478-4808-a94f-ae69951af066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chat_format(conversations, is_input):\n",
    "    chat = []\n",
    "    for conversation in conversations:\n",
    "        if conversation[\"from\"] == \"human\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}] :\")\n",
    "            chat.append({\"role\": \"user\", \"content\": value})\n",
    "        elif is_input == False and conversation[\"from\"] == \"gpt\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}].\")\n",
    "            chat.append({\"role\": \"assistant\", \"content\": value})\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9e2ca7-4e92-4057-b6c5-d34becc3202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_length = 2048 # 2048\n",
    "system_prompt = \"\"\n",
    "# system_prompt = \"You are a medical question answering assistant.\"\n",
    "\n",
    "def convert_to_chat_format(question, context, expected_output, is_input):\n",
    "    chat = []\n",
    "    if system_prompt:\n",
    "        chat.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    content = \"\"\n",
    "    if context:\n",
    "        content += f\"Context:\\n {context}\\n\\nQuestion: \\n\"\n",
    "    content += question\n",
    "    chat.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    if is_input == False:\n",
    "        chat.append({\"role\": \"assistant\", \"content\": expected_output})\n",
    "                     \n",
    "    return chat\n",
    "\n",
    "def convert_to_tokenized_training_data(question, context, expected_output):\n",
    "    questions = convert_to_chat_format(question, context, expected_output, is_input = True) #[convert_to_chat_format(conversations, is_input = True) for conversations in conversations_list]   \n",
    "    template_questions = tokenizer.apply_chat_template(questions, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "    questions_and_answers = convert_to_chat_format(question, context, expected_output, is_input = False) #[convert_to_chat_format(conversations, is_input = False) for conversations in conversations_list]\n",
    "    template_questions_and_answers = tokenizer.apply_chat_template(questions_and_answers, tokenize = False, add_generation_prompt = False)\n",
    "\n",
    "    # print(\"template_questions\", '\\n', template_questions, '\\n------')\n",
    "    # print(\"template_expected_outputs\", '\\n', template_questions_and_answers, '\\n-----')\n",
    "\n",
    "    tokenized_questions = tokenizer(template_questions, padding = False, truncation=True, max_length=max_length)\n",
    "    tokenized_inputs = tokenizer(template_questions_and_answers, padding=False, truncation=True, max_length=max_length)\n",
    "\n",
    "    # print(\"tokenized_questions\", '\\n', tokenized_questions, '\\n------')\n",
    "    # print(\"tokenized_inputs\", '\\n', tokenized_inputs, '\\n-----')\n",
    "\n",
    "    questions_input_ids = tokenized_questions[\"input_ids\"]\n",
    "    all_content_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "    tokenized_labels = [-100] * len(questions_input_ids) + all_content_input_ids[len(questions_input_ids):]\n",
    "\n",
    "    # print(\"tokenized_labels\", '\\n', tokenized_labels, '\\n-----')\n",
    "    \n",
    "    # Add padding\n",
    "    padding_num = max_length - len(tokenized_inputs[\"input_ids\"])\n",
    "    if padding_num > 0:\n",
    "        tokenized_inputs[\"input_ids\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"input_ids\"]\n",
    "        tokenized_inputs[\"attention_mask\"] = [0] * padding_num + tokenized_inputs[\"attention_mask\"]\n",
    "        tokenized_labels = [tokenizer.eos_token_id] * padding_num + tokenized_labels\n",
    "\n",
    "    # print(tokenized_inputs)\n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"attention_mask\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    training_data = {\"input_ids\": tokenized_inputs[\"input_ids\"], \n",
    "                    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "                    \"labels\": tokenized_labels}\n",
    "    \n",
    "    # print(\"input_ids:\", len(training_data[\"input_ids\"]), training_data[\"input_ids\"])\n",
    "\n",
    "    # print(\"attention_mask:\", len(training_data[\"attention_mask\"]), training_data[\"attention_mask\"])\n",
    "\n",
    "    # print(\"labels:\", len(training_data[\"labels\"]), training_data[\"labels\"])\n",
    "\n",
    "\n",
    "    training_data = {k: torch.tensor(v) for k, v in training_data.items()}\n",
    "\n",
    "    # print(\"training_data:\", training_data)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3996c5a-b448-4906-a3c5-4689139175ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 28029\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_num_list = [0, 1, 2]\n",
    "# doc_num_list = [2]\n",
    "is_random_doc_num = False\n",
    "\n",
    "all_training_data = []\n",
    "\n",
    "def get_tokenized_data(data, doc_num):\n",
    "    context = \"\"\n",
    "    for j in range(doc_num):\n",
    "        context += data.doc_list[j]\n",
    "        context += \"\\n\\n\"\n",
    "    tokenized_data = convert_to_tokenized_training_data(data.question_and_options, context, data.expected_output)\n",
    "    return tokenized_data\n",
    "\n",
    "for i in range(len(training_data_list)):\n",
    "    data = training_data_list[i]\n",
    "\n",
    "    # n docs\n",
    "    if is_random_doc_num:\n",
    "        doc_num = random.choice(doc_num_list)\n",
    "        all_training_data.append(get_tokenized_data(data, doc_num))\n",
    "    else:\n",
    "        for doc_num in doc_num_list:\n",
    "            all_training_data.append(get_tokenized_data(data, doc_num))\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "    # if len(all_training_data) > 10:\n",
    "    #     break\n",
    "        \n",
    "from datasets import Dataset\n",
    "\n",
    "training_data = Dataset.from_list([\n",
    "    {\n",
    "        \"input_ids\": s[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": s[\"attention_mask\"].tolist(),\n",
    "        \"labels\": s[\"labels\"].tolist(),\n",
    "    }\n",
    "    for s in all_training_data\n",
    "])\n",
    "\n",
    "print(training_data)\n",
    "\n",
    "# ret = convert_to_tokenized_training_data(data.question_and_options, data.doc_list[0], data.expected_output)\n",
    "# print(ret)\n",
    "# print(len(ret[\"input_ids\"]))\n",
    "# print(len(ret[\"attention_mask\"]))\n",
    "# print(len(ret[\"labels\"]))\n",
    "\n",
    "# print(ret[\"input_ids\"].tolist())\n",
    "# print(ret[\"attention_mask\"].tolist())\n",
    "# print(ret[\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb57452d-1370-4632-9b86-d5e2450feca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "2048\n",
      "Context:\n",
      " [Acute cystourethritis during pregnancy]. A prospective study was carried out in 103/863 obstetric patients with cystitis characterized by urinary urgency and frequency, dysuria, pyuria and suprapubic discomfort in the absence of systemic symptoms such as fever and costovertebral angle tenderness. The association of symptomatic lower urinary tract infection with low-count bacteriuria (10(2)-10(5) UFC/mL of urine) was present in all the patients. The incidence of cystourethritis was about 12%, most of the infections occurred at the first trimester. To learn whether a multiple-dose of nitrofurantoin or ampicillin is safe and effective therapy for acute uncomplicated urinary tract infections, 103 symptomatic pregnant women were randomly grouped to receive oral nitrofurantoin (100 mg t.i.d.) or ampicillin (500 mg t.i.d.) for five days. Seventeen patient were excluded since they did not return for follow-up. Escherichia coli was isolated in 67% of infections. Overall cure varied from 87% to 89%, without any great differences between the regimens. Nine patients had asymptomatic bacteriuria in the course of pregnancy, four developed acute pyelonephritis and one of them had abnormal intravenous pyelogram.\n",
      "\n",
      "\n",
      "A randomized trial of three antibiotic regimens for the treatment of pyelonephritis in pregnancy. To compare the effectiveness of three antibiotic regimens for the treatment of acute pyelonephritis in pregnancy. One hundred seventy-nine pregnant women earlier than 24 weeks' gestation who had acute pyelonephritis were randomized to 1) intravenous (i.v.) ampicillin and gentamicin, 2) i.v. cefazolin, or 3) intramuscular ceftriaxone. All participants then completed 10-day courses of oral cephalexin after primary treatment. A urine culture was performed on admission and 5-14 days after completion of therapy. Surveillance for persistent or recurrent infection and obstetric complications continued until delivery. On the basis of a two-sided hypothesis test and with alpha = .025, 60 subjects were needed in each group for statistical power greater than 80% to detect a difference between ceftriaxone and other antibiotics if hospital length of stay differed by 1 or more days. The treatment groups were similar in age, parity, temperature, gestational age, and initial white blood cell count. There were no statistically significant differences in length of hospitalization, hours until becoming afebrile, days until resolution of costovertebral angle tenderness, or infecting organism. There were no statistically significant differences in birth outcomes between the three groups. The average (standard deviation) age at delivery was 38.8 +/- 3.6 weeks. The average birth weight was 3274 +/- 523 g. Eleven (6.9%) of 159 subjects delivered prematurely. Escherichia coli was the most common uropathogen isolated (137 of 179, 76.5%). Blood cultures were positive for organisms in 15 cases (8.4%). At follow-up examination within 2 weeks of initial therapy, eight (5.0%) of 159 subjects had urine cultures positive for organisms. Ten women (6.3%) had cultures positive for organisms later in their antepartum course, and 10 other participants (6.3%) developed recurrent pyelonephritis. There are no significant differences in clinical response to antimicrobial therapy or birth outcomes among subjects treated with ampicillin and gentamicin, cefazolin, or ceftriaxone for acute pyelonephritis in pregnancy before 24 weeks' gestation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: \n",
      "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "\n",
      "[A] : Ampicillin\n",
      "[B] : Ceftriaxone\n",
      "[C] : Doxycycline\n",
      "[D] : Nitrofurantoin\n",
      " The clinical scenario suggests that the pregnant woman is likely experiencing a urinary tract infection (UTI), given her symptoms of burning upon urination. During pregnancy, the risk of UTIs increases due to physiological changes, and it is important to treat them to prevent complications such as pyelonephritis or preterm labor.\n",
      "\n",
      "When considering antibiotic therapy for a UTI in a pregnant woman, it's crucial to choose an agent that is safe for both the mother and the fetus. Let's evaluate the options:\n",
      "\n",
      "[A]. Ampicillin - This is a penicillin antibiotic that is generally considered safe during pregnancy and can be used to treat UTIs. However, it may not be the first choice due to resistance patterns.\n",
      "\n",
      "[B]. Ceftriaxone - This is a cephalosporin antibiotic that is also generally safe in pregnancy and effective against a broad range of bacteria. It is typically used for more complicated infections or when other antibiotics are not suitable.\n",
      "\n",
      "[C]. Doxycycline - This is a tetracycline antibiotic that is contraindicated during pregnancy because it can affect fetal bone growth and discoloration of teeth.\n",
      "\n",
      "[D]. Nitrofurantoin - This antibiotic is commonly used to treat uncomplicated UTIs and is considered safe during most of the pregnancy. However, it is not recommended at the very end of pregnancy (after 38 weeks) due to the risk of hemolytic anemia in the newborn.\n",
      "\n",
      "Given that the patient is at 22 weeks gestation and has an uncomplicated UTI, the best choice would be an antibiotic that is safe and commonly used for UTIs in pregnancy.\n",
      "\n",
      "So, the answer is [D]. Nitrofurantoin.\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "sample = training_data[2]\n",
    "print(sample.keys())\n",
    "\n",
    "import torch\n",
    "labels = sample[\"labels\"]\n",
    "# print(torch.unique(labels))  # should include token IDs, not only -100\n",
    "\n",
    "print(len(sample[\"labels\"]))\n",
    "print(tokenizer.decode(sample[\"input_ids\"], skip_special_tokens = True))\n",
    "print(\"--------------\")\n",
    "\n",
    "# print(tokenizer.decode(sample[\"labels\"]))\n",
    "# print(tokenizer.decode([id for id in sample[\"labels\"] if id >= 0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82100da-3d9a-4b93-b026-183c1b062de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,437,184 || all params: 3,830,516,736 || trainable%: 0.2464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5864' max='21024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5864/21024 1:21:16 < 3:30:10, 1.20 it/s, Epoch 0.84/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# model.gradient_checkpointing_enable() # TODO\n",
    "\n",
    "use_lora = True\n",
    "if use_lora:\n",
    "    \n",
    "    model.config.name_or_path = model_name\n",
    "    model.name_or_path = model_name\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    # rank (typical 8–64)\n",
    "        lora_alpha=32,           # scaling factor\n",
    "        # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # for Phi-3     (attention projection layers)\n",
    "        target_modules=[\"qkv_proj\", \"o_proj\"], # for Phi-3.5\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/11-10-LoRA-5e-5-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA_qwen/11-14-LoRA-1e-4-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-6-epoch5-model-LoRA-1e-4-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-9-epoch5-model-LoRA-1e-4-r16-context0123\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA_qwen/12-8-Qwen-4B-Thinking-epoch4-LoRA-1e-4-r16-context012\"\n",
    "output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-13-Phi3.5-epoch5-lora-1e-4-r16-context012\"\n",
    "\n",
    "is_bf16_supported = torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    save_strategy = \"epoch\", #\"steps\", #\"steps\", # save checkpoints # \"epoch\", \"steps\", \"no\"\n",
    "    # save_steps = 50000, # total_steps = dataset_size / batch_size\n",
    "    # save_total_limit = 5, # keep only the last N checkpoint\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 3, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-4, # As specified in the paper: 1e-4\n",
    "    fp16 = torch.cuda.is_available() and not is_bf16_supported,\n",
    "    bf16 = is_bf16_supported, # Sometimes RuntimeError: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'\n",
    "    logging_steps = 100,\n",
    "    logging_dir = \"./logs\",  # Directory for logs\n",
    "    # report_to = [\"tensorboard\"],  # Enable logging to TensorBoard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = training_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if not use_lora:\n",
    "    model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "    \n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                    )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                        # legacy_format=False  # Use newer, more efficient format\n",
    "                    )\n",
    "\n",
    "print(f\"----------- finish saving model {model.name_or_path} to: {output_dir} -----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635db81b-0d1d-46ba-a046-04c16761c215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
