{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427d9f00-b7d9-4c72-ac55-3c80a5c9c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c1ba21-b35c-43cb-8bb3-1c6c21994fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2e7065-e5ee-499a-bb31-0ad178d88684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343\n",
      "9343\n"
     ]
    }
   ],
   "source": [
    "# file_name = \"../10-3-test_training_data.txt\"\n",
    "file_name_list = [\"10-2-MedQA_context_training.txt\", \n",
    "                  # \"10-8-MedQA_Evol_context_training.txt\"\n",
    "                 ]\n",
    "\n",
    "class TrainingData:\n",
    "    def __init__(self):\n",
    "        self.question_and_options = \"\"\n",
    "        self.doc_list = []\n",
    "        self.expected_output = \"\"\n",
    "\n",
    "training_data_list = []\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    with open(file_name) as f:\n",
    "        question_index = None\n",
    "        is_reading_question = False\n",
    "        is_reading_answer = False\n",
    "        current_training_data = None\n",
    "        for line in f:\n",
    "            key = \"[question index:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                # print(question)\n",
    "                \n",
    "                question_index = int(line[start + len(key):].strip())\n",
    "                # print(question_index)\n",
    "    \n",
    "                current_training_data = TrainingData()\n",
    "                training_data_list.append(current_training_data)\n",
    "                \n",
    "                is_reading_question = False\n",
    "                is_reading_answer = False\n",
    "                continue\n",
    "                \n",
    "            key = \"[question_and_options:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                current_training_data.question_and_options += line[start + len(key):]\n",
    "                is_reading_question = True\n",
    "                continue\n",
    "    \n",
    "    \n",
    "            key = \"[rag_doc:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                doc_idx_and_doc = line[start + len(key):]\n",
    "                split_pos = doc_idx_and_doc.find(\":\")\n",
    "                doc_idx = int(doc_idx_and_doc[:split_pos])\n",
    "                doc = doc_idx_and_doc[split_pos + 1:]\n",
    "                current_training_data.doc_list.append(doc)\n",
    "                \n",
    "                is_reading_question = False\n",
    "                continue\n",
    "    \n",
    "            key = \"[expected_output:]\"\n",
    "            start = line.find(key)\n",
    "            if start != -1:\n",
    "                expected_output = line[start + len(key):]\n",
    "                current_training_data.expected_output += expected_output\n",
    "                is_reading_answer = True\n",
    "                continue\n",
    "            \n",
    "            if is_reading_question:\n",
    "                current_training_data.question_and_options += line\n",
    "                continue\n",
    "                \n",
    "            if is_reading_answer:\n",
    "                current_training_data.expected_output += line\n",
    "                continue\n",
    "                \n",
    "            # if \"[rag_doc:]\" in line:\n",
    "            #     doc = line(\n",
    "\n",
    "print(len(training_data_list))\n",
    "\n",
    "if len(training_data_list[len(training_data_list) - 1].doc_list) == 0:\n",
    "    del training_data_list[len(training_data_list) - 1]\n",
    "\n",
    "print(len(training_data_list))\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].question_and_options)\n",
    "# print(\"------------\")\n",
    "# print(len(training_data_list[0].doc_list))\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].doc_list[0])\n",
    "# print(\"------------\")\n",
    "# print(training_data_list[0].expected_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a60766-c7de-41e5-b28b-4b068805de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- start checking GPU -----------\n",
      "GPU: NVIDIA H100 NVL\n",
      "torch.cuda.is_bf16_supported():  True\n",
      "---------- finish checking GPU -----------\n",
      "---------- start loading model:/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000 -----------\n",
      "finish loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading model\n",
      "torch_dtype: torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "---------- finish loading model:/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000 -----------\n"
     ]
    }
   ],
   "source": [
    "from model_trainer import ModelTrainer\n",
    "trainer = ModelTrainer()\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/10-15-UltraMedical-batchsize8-bf16\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-11-Qwen3-4B-base-UltraMedical\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-1-phi3-mini-batchsize8-epoch456/checkpoint-256000\"\n",
    "model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/checkpoint-256000\"\n",
    "# model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-batch8-epoch456/checkpoint-204800\"\n",
    "\n",
    "# model_name = \"TsinghuaC3I/Llama-3.1-8B-UltraMedical\"\n",
    "\n",
    "tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/12-9-phi3.5-mini-UltraMedical-batchsize8-epoch456/\"\n",
    "# tokenizer_path = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base_qwen/11-25-qwen-4B-Thinking-UltraMedical\"\n",
    "trainer.load_model(model_name, tokenizer_path = tokenizer_path)\n",
    "\n",
    "tokenizer = trainer.tokenizer\n",
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3701141c-8478-4808-a94f-ae69951af066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chat_format(conversations, is_input):\n",
    "    chat = []\n",
    "    for conversation in conversations:\n",
    "        if conversation[\"from\"] == \"human\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}] :\")\n",
    "            chat.append({\"role\": \"user\", \"content\": value})\n",
    "        elif is_input == False and conversation[\"from\"] == \"gpt\":\n",
    "            value = conversation[\"value\"]\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                value = value.replace(f\"{option_letter}.\", f\"[{option_letter}].\")\n",
    "            chat.append({\"role\": \"assistant\", \"content\": value})\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9e2ca7-4e92-4057-b6c5-d34becc3202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_length = 2048 # 2048\n",
    "system_prompt = \"\"\n",
    "# system_prompt = \"You are a medical question answering assistant.\"\n",
    "\n",
    "def convert_to_chat_format(question, context, expected_output, is_input):\n",
    "    chat = []\n",
    "    if system_prompt:\n",
    "        chat.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    content = \"\"\n",
    "    if context:\n",
    "        content += f\"Context:\\n {context}\\n\\nQuestion: \\n\"\n",
    "    content += question\n",
    "    chat.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    if is_input == False:\n",
    "        chat.append({\"role\": \"assistant\", \"content\": expected_output})\n",
    "                     \n",
    "    return chat\n",
    "\n",
    "def convert_to_tokenized_training_data(question, context, expected_output):\n",
    "    questions = convert_to_chat_format(question, context, expected_output, is_input = True) #[convert_to_chat_format(conversations, is_input = True) for conversations in conversations_list]   \n",
    "    template_questions = tokenizer.apply_chat_template(questions, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "    questions_and_answers = convert_to_chat_format(question, context, expected_output, is_input = False) #[convert_to_chat_format(conversations, is_input = False) for conversations in conversations_list]\n",
    "    template_questions_and_answers = tokenizer.apply_chat_template(questions_and_answers, tokenize = False, add_generation_prompt = False)\n",
    "\n",
    "    # print(\"template_questions\", '\\n', template_questions, '\\n------')\n",
    "    # print(\"template_expected_outputs\", '\\n', template_questions_and_answers, '\\n-----')\n",
    "\n",
    "    tokenized_questions = tokenizer(template_questions, padding = False, truncation=True, max_length=max_length)\n",
    "    tokenized_inputs = tokenizer(template_questions_and_answers, padding=False, truncation=True, max_length=max_length)\n",
    "\n",
    "    # print(\"tokenized_questions\", '\\n', tokenized_questions, '\\n------')\n",
    "    # print(\"tokenized_inputs\", '\\n', tokenized_inputs, '\\n-----')\n",
    "\n",
    "    questions_input_ids = tokenized_questions[\"input_ids\"]\n",
    "    all_content_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "    tokenized_labels = [-100] * len(questions_input_ids) + all_content_input_ids[len(questions_input_ids):]\n",
    "\n",
    "    # print(\"tokenized_labels\", '\\n', tokenized_labels, '\\n-----')\n",
    "    \n",
    "    # Add padding\n",
    "    padding_num = max_length - len(tokenized_inputs[\"input_ids\"])\n",
    "    if padding_num > 0:\n",
    "        tokenized_inputs[\"input_ids\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"input_ids\"]\n",
    "        tokenized_inputs[\"attention_mask\"] = [0] * padding_num + tokenized_inputs[\"attention_mask\"]\n",
    "        tokenized_labels = [tokenizer.eos_token_id] * padding_num + tokenized_labels\n",
    "\n",
    "    # print(tokenized_inputs)\n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"attention_mask\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    training_data = {\"input_ids\": tokenized_inputs[\"input_ids\"], \n",
    "                    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "                    \"labels\": tokenized_labels}\n",
    "    \n",
    "    # print(\"input_ids:\", len(training_data[\"input_ids\"]), training_data[\"input_ids\"])\n",
    "\n",
    "    # print(\"attention_mask:\", len(training_data[\"attention_mask\"]), training_data[\"attention_mask\"])\n",
    "\n",
    "    # print(\"labels:\", len(training_data[\"labels\"]), training_data[\"labels\"])\n",
    "\n",
    "\n",
    "    training_data = {k: torch.tensor(v) for k, v in training_data.items()}\n",
    "\n",
    "    # print(\"training_data:\", training_data)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3996c5a-b448-4906-a3c5-4689139175ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 28029\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_num_list = [0, 1, 2]\n",
    "# doc_num_list = [2]\n",
    "is_random_doc_num = False\n",
    "\n",
    "all_training_data = []\n",
    "\n",
    "def get_tokenized_data(data, doc_num):\n",
    "    context = \"\"\n",
    "    for j in range(doc_num):\n",
    "        context += data.doc_list[j]\n",
    "        context += \"\\n\\n\"\n",
    "    tokenized_data = convert_to_tokenized_training_data(data.question_and_options, context, data.expected_output)\n",
    "    return tokenized_data\n",
    "\n",
    "for i in range(len(training_data_list)):\n",
    "    data = training_data_list[i]\n",
    "\n",
    "    # n docs\n",
    "    if is_random_doc_num:\n",
    "        doc_num = random.choice(doc_num_list)\n",
    "        all_training_data.append(get_tokenized_data(data, doc_num))\n",
    "    else:\n",
    "        for doc_num in doc_num_list:\n",
    "            all_training_data.append(get_tokenized_data(data, doc_num))\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "from datasets import Dataset\n",
    "\n",
    "training_data = Dataset.from_list([\n",
    "    {\n",
    "        \"input_ids\": s[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": s[\"attention_mask\"].tolist(),\n",
    "        \"labels\": s[\"labels\"].tolist(),\n",
    "    }\n",
    "    for s in all_training_data\n",
    "])\n",
    "\n",
    "print(training_data)\n",
    "\n",
    "# ret = convert_to_tokenized_training_data(data.question_and_options, data.doc_list[0], data.expected_output)\n",
    "# print(ret)\n",
    "# print(len(ret[\"input_ids\"]))\n",
    "# print(len(ret[\"attention_mask\"]))\n",
    "# print(len(ret[\"labels\"]))\n",
    "\n",
    "# print(ret[\"input_ids\"].tolist())\n",
    "# print(ret[\"attention_mask\"].tolist())\n",
    "# print(ret[\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb57452d-1370-4632-9b86-d5e2450feca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "2048\n",
      "A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\n",
      "\n",
      "[A] : Abnormal migration of ventral pancreatic bud\n",
      "[B] : Complete failure of proximal duodenum to recanalize\n",
      "[C] : Abnormal hypertrophy of the pylorus\n",
      "[D] : Failure of lateral body folds to move ventrally and fuse in the midline\n",
      " The infant's presentation with fussiness, regurgitation of feeds, and yellow-colored vomit is suggestive of a gastrointestinal issue that is likely related to an embryologic development error. Let's analyze each of the provided options:\n",
      "\n",
      "[A]. Abnormal migration of ventral pancreatic bud - This can lead to conditions such as annular pancreas, where the pancreas forms a ring around the duodenum and can cause duodenal obstruction. This would present with symptoms similar to what is described, including vomiting, which can be yellow if it contains bile.\n",
      "\n",
      "[B]. Complete failure of proximal duodenum to recanalize - This would cause duodenal atresia, which presents with bilious vomiting within the first few days of life. While the vomit described is yellow (suggestive of bile), the timing and presentation don't align perfectly with duodenal atresia, as the symptoms would have likely appeared earlier than 3 weeks of age.\n",
      "\n",
      "[C]. Abnormal hypertrophy of the pylorus - This would lead to pyloric stenosis, which typically presents with projectile non-bilious vomiting in infants around 2 to 8 weeks of age. However, the vomit in this case is described as yellow, which suggests the presence of bile, making pyloric stenosis less likely since the obstruction in pyloric stenosis is before the bile enters the digestive tract.\n",
      "\n",
      "[D]. Failure of lateral body folds to move ventrally and fuse in the midline - This is related to congenital abdominal wall defects such as gastroschisis or omphalocele. These conditions would be noticeable at birth due to visible defects in the abdominal wall and are not consistent with the symptoms described.\n",
      "\n",
      "Given the description of yellow vomit, which indicates the presence of bile and therefore an obstruction distal to where bile is introduced into the digestive tract, the most likely embryologic error is an abnormal migration of the ventral pancreatic bud leading to an annular pancreas and subsequent duodenal obstruction.\n",
      "\n",
      "So, the answer is [A].\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "sample = training_data[3]\n",
    "print(sample.keys())\n",
    "\n",
    "import torch\n",
    "labels = sample[\"labels\"]\n",
    "# print(torch.unique(labels))  # should include token IDs, not only -100\n",
    "\n",
    "print(len(sample[\"labels\"]))\n",
    "print(tokenizer.decode(sample[\"input_ids\"], skip_special_tokens = True))\n",
    "print(\"--------------\")\n",
    "\n",
    "# print(tokenizer.decode(sample[\"labels\"]))\n",
    "# print(tokenizer.decode([id for id in sample[\"labels\"] if id >= 0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82100da-3d9a-4b93-b026-183c1b062de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/projects/sciences/computing/sheju347/miniconda3/envs/LLM311/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,145,728 || all params: 3,824,225,280 || trainable%: 0.0823\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6264' max='21024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6264/21024 1:46:48 < 4:11:44, 0.98 it/s, Epoch 0.89/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# model.gradient_checkpointing_enable() # TODO\n",
    "\n",
    "use_lora = True\n",
    "if use_lora:\n",
    "    \n",
    "    model.config.name_or_path = model_name\n",
    "    model.name_or_path = model_name\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    # rank (typical 8–64)\n",
    "        lora_alpha=32,           # scaling factor\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # attention projection layers\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/11-10-LoRA-5e-5-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA_qwen/11-14-LoRA-1e-4-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-6-epoch5-model-LoRA-1e-4-r16-context012\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-9-epoch5-model-LoRA-1e-4-r16-context0123\"\n",
    "# output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA_qwen/12-8-Qwen-4B-Thinking-epoch4-LoRA-1e-4-r16-context012\"\n",
    "output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/LoRA/12-12-Phi3.5-epoch5-lora-1e-4-r16-context012\"\n",
    "\n",
    "\n",
    "is_bf16_supported = torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    save_strategy = \"epoch\", #\"steps\", #\"steps\", # save checkpoints # \"epoch\", \"steps\", \"no\"\n",
    "    # save_steps = 50000, # total_steps = dataset_size / batch_size\n",
    "    # save_total_limit = 5, # keep only the last N checkpoint\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 3, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-4, # As specified in the paper: 1e-4\n",
    "    fp16 = torch.cuda.is_available() and not is_bf16_supported,\n",
    "    bf16 = is_bf16_supported, # Sometimes RuntimeError: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'\n",
    "    logging_steps = 100,\n",
    "    logging_dir = \"./logs\",  # Directory for logs\n",
    "    # report_to = [\"tensorboard\"],  # Enable logging to TensorBoard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = training_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if not use_lora:\n",
    "    model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "    \n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                    )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                        # legacy_format=False  # Use newer, more efficient format\n",
    "                    )\n",
    "\n",
    "print(f\"----------- finish saving model {model.name_or_path} to: {output_dir} -----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635db81b-0d1d-46ba-a046-04c16761c215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM311)",
   "language": "python",
   "name": "llm311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
