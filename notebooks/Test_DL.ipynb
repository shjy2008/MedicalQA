{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538a8ea-cce7-4c4f-85f2-496e2b922f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ecaf25-82f4-4208-8c50-211966ca692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Lech Szymanski\"\n",
    "__organization__ = \"COSC420, University of Otago\"\n",
    "__email__ = \"lech.szymanski@otago.ac.nz\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "try:\n",
    "   import torch\n",
    "   torch_lib = True\n",
    "except:\n",
    "   torch_lib = False\n",
    "   \n",
    "import time\n",
    "\n",
    "def show_data_attributes(inputs, labels,attribute_names,class_names):\n",
    "   # Create a figure\n",
    "   figure_handle = plt.figure()\n",
    "\n",
    "   n_attributes = inputs.shape[1]\n",
    "   n_plots = int(n_attributes*(n_attributes-1)/2)\n",
    "   n_plot_rows = int(np.floor(np.sqrt(n_plots)))\n",
    "   n_plot_columns = int(np.ceil(n_plots/n_plot_rows))\n",
    "\n",
    "   # Plot each dimension against another dimension\n",
    "   n_classes = len(class_names)\n",
    "   plot_index = 1\n",
    "   for i in range(n_attributes):\n",
    "       for j in range(i+1,n_attributes):\n",
    "           # Add subplot to the figure\n",
    "           plot_handle = figure_handle.add_subplot(n_plot_rows, n_plot_columns, plot_index)\n",
    "           # Do a scatter plot for each class separately (matplotlib automatically does them in different colours)\n",
    "           for k in range(n_classes):\n",
    "               # Find all the indexes of points of class k\n",
    "               I = np.where(labels==k)[0]\n",
    "               # Show a scatter plot of dimension i vs. dimension j of the points of class k\n",
    "               plot_handle.scatter(inputs[I,i],inputs[I,j])\n",
    "           # Increments the plot_index variable\n",
    "           plot_index += 1\n",
    "           # Label the x and y axis\n",
    "           plot_handle.set_xlabel(attribute_names[i])\n",
    "           plot_handle.set_ylabel(attribute_names[j])\n",
    "\n",
    "   # Show the legend for class colours on the last subplot\n",
    "   plot_handle.legend(class_names)\n",
    "\n",
    "\n",
    "def show_data_images(images, labels=None, predictions=None, class_names=None,blocking=True):\n",
    "\n",
    "   if torch_lib and torch.is_tensor(images):\n",
    "      images = images.numpy()\n",
    "      # Torch images are in the shape (N, C, H, W) where N is the number of images, C is the number of channels, H is the height, whereas the show methods expect (N, H, W, C) shape...so we transpose switching the last two dimensions\n",
    "      images = np.transpose(images, (0, 2, 3, 1))\n",
    "      if len(images.shape) == 4 and images.shape[3] == 1:\n",
    "         images = np.squeeze(images, axis=3)\n",
    "\n",
    "      if labels is not None and torch.is_tensor(labels):\n",
    "         labels = labels.numpy()\n",
    "\n",
    "   # Create a figure\n",
    "   figure_handle = plt.figure(figsize=(6,8))\n",
    "   n_images = len(images)\n",
    "   n_plot_rows = int(np.floor(np.sqrt(n_images)))\n",
    "   n_plot_columns = int(np.ceil(n_images/n_plot_rows))\n",
    "\n",
    "   plt.ion()\n",
    "\n",
    "   # Plot each dimension against another dimension\n",
    "   for i in range(n_plot_rows * n_plot_columns):\n",
    "      plot_handle = figure_handle.add_subplot(n_plot_rows, n_plot_columns, i + 1)\n",
    "      if len(np.shape(images)) == 3:\n",
    "         plot_handle.imshow(images[i, :, :], cmap='gray')\n",
    "      else:\n",
    "         plot_handle.imshow(images[i, :, :,:])\n",
    "\n",
    "      plot_handle.tick_params(bottom=False, left=False)\n",
    "      plot_handle.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "      titleStr = ''\n",
    "      if labels is not None:\n",
    "         if class_names is not None:\n",
    "            if len(labels.shape) == 1:\n",
    "               titleStr += 'label: %s' % class_names[labels[i]]\n",
    "            else:\n",
    "               titleStr = ''\n",
    "               for j in labels[i]:\n",
    "                  if j<len(class_names):\n",
    "                     titleStr += '%s, ' % class_names[j]\n",
    "               titleStr = titleStr[:-2]\n",
    "         elif isinstance(labels[i],list) and len(labels[i])==3:\n",
    "            titleStr = ''\n",
    "            firstLabel = True\n",
    "            for label in labels[i]:\n",
    "               if firstLabel:\n",
    "                  firstLabel = False\n",
    "               else:\n",
    "                  titleStr += '\\n'\n",
    "               titleStr += '%s (%.2f)' % (label[1],label[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      if predictions is not None:\n",
    "         if labels is not None:\n",
    "            titleStr += '\\n'\n",
    "\n",
    "         titleStr += 'pred: %s' % class_names[predictions[i]]\n",
    "\n",
    "      plot_handle.set_title(titleStr)\n",
    "      plt.pause(0.01)\n",
    "      plt.show()\n",
    "      time.sleep(0.01)\n",
    "\n",
    "   if blocking:\n",
    "      plt.ioff()\n",
    "   else:\n",
    "      time.sleep(1)\n",
    "\n",
    "   plt.show()\n",
    "\n",
    "def show_data_classes(inputs, labels, class_names,mode='pca'):\n",
    "\n",
    "   if len(np.shape(inputs))>2:\n",
    "      n_inputs = len(inputs)\n",
    "      inputs = np.reshape(inputs,(n_inputs,-1))\n",
    "\n",
    "   # Set up PCA compression to 2D\n",
    "   if mode=='pca':\n",
    "      x_2D = PCA(n_components=2).fit_transform(inputs)\n",
    "   elif mode=='tsne':\n",
    "      x_2D = TSNE(n_components=2).fit_transform(inputs)\n",
    "\n",
    "   # Create a new figure\n",
    "   figure_handle = plt.figure()\n",
    "   # Create a subplot\n",
    "   plot_handle = figure_handle.add_subplot(1, 1, 1)\n",
    "   # Do a scatter plot for each class separately (matplotlib automatically does them in different colours)\n",
    "   for k in range(len(class_names)):\n",
    "      # Find all the indexes of points of class k\n",
    "      I = np.where(labels == k)[0]\n",
    "      # Show a scatter plot of dimension i vs. dimension j of the points of class k\n",
    "      plot_handle.scatter(x_2D[I, 0], x_2D[I, 1])\n",
    "\n",
    "   # Show the legend for class colours\n",
    "   if len(class_names) <= 10:\n",
    "      plot_handle.legend(class_names)\n",
    "   # Display the plot title\n",
    "   plot_handle.set_title('Compressed data from %dD to 2D' % inputs.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffe8f88-1159-40c3-ba8f-129750754bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167/450 [===>      ] - ETA: 02:45, loss: 1.8940 - acc: 0.2856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 214\u001b[0m\n\u001b[1;32m    212\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Compute the gradients\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Adjust the weights and biases\u001b[39;00m\n\u001b[1;32m    216\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "__author__ = \"Lech Szymanski\"\n",
    "__organization__ = \"COSC420, University of Otago\"\n",
    "__email__ = \"lech.szymanski@otago.ac.nz\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import pickle, gzip\n",
    "\n",
    "# Specify the device to be used for computations.  Choices are\n",
    "# - 'cpu' - to use the CPU\n",
    "# - 'cuda' to use GPU, \n",
    "# - 'mps' to use Intel MPS (on Apple Silicon)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Change this variable to False if you don't want load the model from file\n",
    "# and train it from scratch (if set to True, but the saved weights file doesn't exist,\n",
    "# the model will be trained from scratch)\n",
    "load_from_file = True\n",
    "\n",
    "# Get the path to the common, data and saved folder\n",
    "path_to_this_scripts_folder = \"./\"#os.path.dirname(os.path.realpath(__file__))\n",
    "path_to_common_folder = os.path.join(path_to_this_scripts_folder, 'common')\n",
    "path_to_data_folder = os.path.join(path_to_this_scripts_folder, 'data')\n",
    "path_to_save_folder = os.path.join(path_to_this_scripts_folder, 'saved')\n",
    "\n",
    "# Add common folder to the system path\n",
    "sys.path.append(path_to_common_folder)\n",
    "\n",
    "# Now we can import show_methods that are found in the common folder\n",
    "# import show_methods\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes): \n",
    "      #Initialise the base class provided by PyTorch\n",
    "      super(CNN, self).__init__()\n",
    "\n",
    "      # Add a convolutional layer, 3x3 window, 64 filters - specify the size of the input as 32x32x3, padding=\"same\"\n",
    "      self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "      # Add a max pooling layer, 2x2 window\n",
    "      self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "      # Add a convolutional layer, 3x3 window, 128 filters, padding=\"same\"\n",
    "      self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "      # Add a max pooling layer, 2x2 window\n",
    "      self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "      # Add a convolutional layer, 3x3 window, 256 filters\n",
    "      self.conv3 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "      # Add a max pooling layer, 2x2 window\n",
    "      self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "      # Flatten the output maps for fully connected layer\n",
    "      self.flatten = torch.nn.Flatten()\n",
    "\n",
    "      # At this point you need to work out the size of the output of the last pooling layer (before flattening).\n",
    "      # You can do this by passing a dummy tensor through the network and printing the \n",
    "      # size of the output to the console.  This will give you the size of the output of the last pooling \n",
    "      # layer.  In this case, with input tensor of 3x32x32, the output of the last pooling layer is 256x4x4, \n",
    "      # which flatten layer will flatten to a total of 256*4*4 = 4096 neurons...and that's the number of\n",
    "      # inputs to the following linear layer.\n",
    "\n",
    "      # Add a fully connected layer of 128 neurons\n",
    "      self.fc1 = torch.nn.Linear(4096, 128)\n",
    "\n",
    "      # Add a fully connected layer of 512 neurons\n",
    "      self.fc2 = torch.nn.Linear(128, 512)\n",
    "\n",
    "      # Add a fully connected layer with number of output neurons the same\n",
    "      # as the number of classes\n",
    "      self.fc3 = torch.nn.Linear(512, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "      # First convolution layer followed by ReLU activation\n",
    "      x = self.conv1(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # Max pooling layer to reduce the size of the feature maps\n",
    "      x = self.pool1(x)\n",
    "\n",
    "      # Second convolution layer followed by ReLU activation\n",
    "      x = self.conv2(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # Max pooling layer to reduce the size of the feature maps\n",
    "      x = self.pool2(x)\n",
    "\n",
    "      # Third convolution layer followed by ReLU activation\n",
    "      x = self.conv3(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # Max pooling layer to reduce the size of the feature maps\n",
    "      x = self.pool3(x)\n",
    "\n",
    "      # Flatten the output of the last pooling layer\n",
    "      x = self.flatten(x)\n",
    "\n",
    "      # Fully connected layer followed by ReLU activation\n",
    "      x = self.fc1(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # Fully connected layer followed by ReLU activation\n",
    "      x = self.fc2(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # Fully connected layer without activation (logits)\n",
    "      x = self.fc3(x)\n",
    "\n",
    "      return x\n",
    "\n",
    "\n",
    "# Load the CIFAR10 training dataset from torchvision (if running first time, it will download the dataset); the transformation at the end converts the images to PyTorch tensors\n",
    "training_set = torchvision.datasets.CIFAR10(root=path_to_data_folder, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Load the CIFAR10 test dataset from torchvision (if running first time, it will download the dataset); the transformation at the end converts the images to PyTorch tensors\n",
    "test_set = torchvision.datasets.CIFAR10(root=path_to_data_folder, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# CIFAR10 is a known benchmark with 50K samples in the training set; we'll split it into\n",
    "# randomly selected 45K training and 5K validation samples\n",
    "training_set, val_set = torch.utils.data.random_split(training_set, [45000, 5000])\n",
    "\n",
    "# Create PyTorch data loaders for training, validation and test data, set mini-batch size to 100, shuffle training data\n",
    "train_data = torch.utils.data.DataLoader(training_set, batch_size=100, shuffle=True)\n",
    "val_data = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=False)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "# Define the class names for the CIFAR10 dataset - this dataset is know and we know the class names for\n",
    "# the 10 classes and the order in which they are stored in the dataset\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create 'saved' folder if it doesn't exist\n",
    "if not os.path.isdir(path_to_save_folder):\n",
    "   os.mkdir(path_to_save_folder)\n",
    "\n",
    "# Specify the names of the save files\n",
    "save_base_name = os.path.join(path_to_save_folder, 'cifar10')\n",
    "saved_weights = save_base_name + '_torch_cnn_net.weights.h5'\n",
    "history_save = save_base_name + '_torch_cnn_net.hist'\n",
    "\n",
    "# Get data sample\n",
    "x_train, y_train = next(iter(train_data))\n",
    "\n",
    "# Show 16 train images with the corresponding labels\n",
    "show_data_images(images=x_train[:16],labels=y_train[:16],class_names=class_names,blocking=False)\n",
    "\n",
    "# Read the number of classes from the class_names list\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Create the CNN with 3 input channels (RGB) and n_classes output neurons.  You don't need to specify the image size - it's\n",
    "# auto adjusted by the network based on the input tensor size.\n",
    "cnn = CNN(in_channels=3, n_classes=n_classes)\n",
    "\n",
    "# Move the model to the device (all computation will be done on the device)\n",
    "cnn.to(device)\n",
    "\n",
    "if load_from_file and os.path.isfile(saved_weights):\n",
    "   # ***************************************************\n",
    "   # * Loading previously trained neural network model *\n",
    "   # ***************************************************\n",
    "\n",
    "   # Load the model from file\n",
    "   print(f'Loading weights from {saved_weights}...')\n",
    "   cnn.load_state_dict(torch.load(saved_weights, weights_only=True))\n",
    "\n",
    "   # Load the training history - since it should have been created right after\n",
    "   # saving the model\n",
    "   if os.path.isfile(history_save):\n",
    "      with gzip.open(history_save) as f:\n",
    "         history = pickle.load(f)\n",
    "   else:\n",
    "      history = []\n",
    "else:\n",
    "   # Create the Adam optimiser object that will be used to adjust the weights and biases\n",
    "   opt = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "   \n",
    "   # Create the cross entropy loss; this loss expects the logits of the output layer\n",
    "   loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "   # Dictionary to store the training history\n",
    "   history = {'loss': [], 'acc':[], 'val_loss': [], 'val_acc':[]}\n",
    "\n",
    "   epochs = 50\n",
    "   for epoch in range(1,epochs+1):\n",
    "\n",
    "      # Reset the loss and accuracy for the epoch\n",
    "      total_loss_training = 0\n",
    "      total_correct_predictions_training = 0\n",
    "      total_sample_count = 0\n",
    "\n",
    "      # This is a complicated logic to show a progress bar with the remaining time - if you need to understand how it works\n",
    "      # you need to familiarise yourself with the tqdm library\n",
    "      print(f'Epoch {epoch}/{epochs}')\n",
    "      batches_loop = tqdm.tqdm(enumerate(train_data), total=len(train_data), bar_format='{n_fmt}/{total_fmt} [{bar}] - ETA: {remaining}{postfix}', ascii=' >=')\n",
    "\n",
    "      for b, (x_batch, y_batch) in batches_loop:         \n",
    "         # Must move the data to the device, because the model is on the device\n",
    "         x_batch = x_batch.to(device)\n",
    "         y_batch = y_batch.to(device)\n",
    "\n",
    "         # Predict the output for the mini-batch\n",
    "         y_pred = cnn(x_batch)\n",
    "         # Compute the loss function\n",
    "         loss_value = loss(y_pred, y_batch)\n",
    "         # Zero out the gradients in the optimiser\n",
    "         opt.zero_grad()\n",
    "         # Compute the gradients\n",
    "         loss_value.backward()\n",
    "         # Adjust the weights and biases\n",
    "         opt.step()\n",
    "         total_loss_training += loss_value.item()\n",
    "         # Compute the accuracy for the mini-batch\n",
    "         y_pred = torch.argmax(y_pred, dim=1)\n",
    "         acc = torch.sum(y_pred == y_batch).item()\n",
    "         total_correct_predictions_training += acc\n",
    "         # Update the total number of samples processed so far\n",
    "         total_sample_count += len(y_batch)\n",
    "         # Show the average (so far) loss and accuracy for the mini-batch in the progress bar\n",
    "         batches_loop.set_postfix_str(f'loss: {total_loss_training/(b+1):.4f} - acc: {total_correct_predictions_training/(total_sample_count):.4f}') \n",
    "\n",
    "      # Note len(training_data) is the number of mini-batches in the training data, \n",
    "      # len(training_set) is the total number of points in the training set.\n",
    "      num_batches_training = len(train_data)\n",
    "      num_samples_training = len(training_set)\n",
    "\n",
    "      # Loss is averaged in each batch, so to get the overall average we divide by the number of batches\n",
    "      average_loss_in_epoch_training = total_loss_training / num_batches_training\n",
    "      # Correct prediction count is for the total number of samples, so to get the accuracy we divide by the number of samples\n",
    "      accuracy_in_epoch_training = total_correct_predictions_training / num_samples_training\n",
    "\n",
    "      # Reset the loss and accuracy for the validation set\n",
    "      total_loss_val = 0\n",
    "      total_correct_predictions_val = 0\n",
    "      for x_batch, y_batch in val_data:\n",
    "         # Must move the data to the device, because the model is on the device\n",
    "         x_batch = x_batch.to(device)\n",
    "         y_batch = y_batch.to(device)\n",
    "\n",
    "         y_pred = cnn(x_batch)\n",
    "         total_loss_val += loss(y_pred, y_batch).item()\n",
    "         y_pred = torch.argmax(y_pred, dim=1)\n",
    "         total_correct_predictions_val += torch.sum(y_pred == y_batch).item()\n",
    "\n",
    "      num_batches_val = len(val_data)\n",
    "      num_samples_val = len(val_set)\n",
    "\n",
    "      # Compute the average loss and accuracy for the validation set\n",
    "      average_loss_in_epoch_val = total_loss_val / num_batches_val\n",
    "      accuracy_in_epoch_val = total_correct_predictions_val / num_samples_val\n",
    "\n",
    "      # Save the loss and accuracy for the epoch\n",
    "      history['loss'].append(average_loss_in_epoch_training)\n",
    "      history['acc'].append(accuracy_in_epoch_training)\n",
    "      history['val_loss'].append(average_loss_in_epoch_val)\n",
    "      history['val_acc'].append(accuracy_in_epoch_val)\n",
    "\n",
    "      # Print the loss and accuracy for the epoch\n",
    "      print(f'loss: {average_loss_in_epoch_training:.4f} - acc: {accuracy_in_epoch_training:.4f} - val_loss: {average_loss_in_epoch_val:.4f} - val_acc: {accuracy_in_epoch_val:.4f}')      \n",
    "   \n",
    "\n",
    "   # Save the model to file\n",
    "   print(\"Saving neural network to %s...\" % saved_weights)\n",
    "   torch.save(cnn.state_dict(), saved_weights)\n",
    "\n",
    "   ## Save training history to file\n",
    "   with gzip.open(history_save, 'w') as f:\n",
    "      pickle.dump(history, f)\n",
    "\n",
    "# Training finished, set the model to evaluation mode\n",
    "cnn.eval()\n",
    "\n",
    "# *********************************************************\n",
    "# * Training history *\n",
    "# *********************************************************\n",
    "\n",
    "# Plot training and validation accuracy over the course of training\n",
    "if history != []:\n",
    "   fh = plt.figure()\n",
    "   ph = fh.add_subplot(111)\n",
    "   ph.plot(history['acc'], label='accuracy')\n",
    "   ph.plot(history['val_acc'], label = 'val_accuracy')\n",
    "   ph.set_xlabel('Epoch')\n",
    "   ph.set_ylabel('Accuracy')\n",
    "   ph.set_ylim([0, 1])\n",
    "   ph.legend(loc='lower right')\n",
    "\n",
    "   print(f'Train accuracy: {history[\"acc\"][-1]:.2f}')\n",
    "   print(f'Valid accuracy: {history[\"val_acc\"][-1]:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "# ************************************************************\n",
    "# * Evaluate the neural network model on test data           *\n",
    "# ************************************************************\n",
    "accuracy_test = 0\n",
    "for x_batch, y_batch in test_data:\n",
    "   # Must move the data to the device, because the model is on the device\n",
    "   x_batch = x_batch.to(device)\n",
    "   y_batch = y_batch.to(device)\n",
    "\n",
    "   y_pred = cnn(x_batch)\n",
    "   y_pred = torch.argmax(y_pred, dim=1)\n",
    "   accuracy_test += torch.sum(y_pred == y_batch).item()\n",
    "\n",
    "accuracy_test /= len(test_set)\n",
    "print(f'Test accuracy : {accuracy_test:.2f}')\n",
    "\n",
    "# Get data sample\n",
    "x_test, y_test = next(iter(test_data))\n",
    "\n",
    "# Move the data to the device\n",
    "x_test = x_test.to(device)\n",
    "# Compute the output of the network for the test data\n",
    "y_pred = cnn(x_test)\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "# Move the data back to the CPU for plotting\n",
    "x_test = x_test.cpu()\n",
    "y_pred = y_pred.cpu()\n",
    "\n",
    "# Show true labels and predictions for 16 test images\n",
    "show_data_images(images=x_test[:16],\n",
    "                              labels=y_test[:16],predictions=y_pred[:16],\n",
    "                              class_names=class_names,blocking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bf0c7-4def-4e98-814d-bbf90b8e6740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
