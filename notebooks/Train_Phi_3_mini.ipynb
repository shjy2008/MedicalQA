{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92f390-b887-4b55-a5b1-7cc77e9e0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda init bash\n",
    "# !source ~/.bashrc\n",
    "# !conda --version\n",
    "!conda activate LLM\n",
    "# !conda env list\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85bb19-0445-4a9f-8577-0a08446240a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c78ebd-41ac-4cd4-af46-9aece9968bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9168249-4fe9-44dc-90e3-f4a02c6a7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "microsoft/Phi-3-mini-4k-instruct\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-instruct\"\n",
    "\n",
    "# model_name = \"KrithikV/MedMobile\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False, torch_dtype=torch.bfloat16)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "print(model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b45fe-ea95-4c26-9460-54926c7efc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch memory usage\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GiB\")\n",
    "print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GiB\")\n",
    "print(f\"Max Reserved: {torch.cuda.max_memory_reserved() / 1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b763f0-2cb4-4983-b0ad-f09ea01e85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer)\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, add_generation_prompt = True, tokenize=False))\n",
    "inputs = tokenizer.apply_chat_template(chat, add_generation_prompt = True, tokenize = True, return_tensors = \"pt\").to(device)\n",
    "print(\"inputs:\", inputs)\n",
    "outputs = model.generate(inputs, max_new_tokens = 512, do_sample = True, temperature = 0.7)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(\"outputs:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dcc7614-824b-43f6-ac6b-3d15da64ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# chat = [\n",
    "#     {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "# ]\n",
    "# print(tokenizer.apply_chat_template(chat, add_generation_prompt = False, tokenize = False, continue_final_message = True))\n",
    "\n",
    "ultraMedical = load_dataset(\"TsinghuaC3I/UltraMedical\")\n",
    "\n",
    "data_list = ultraMedical[\"train\"]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87f6f15-0da8-4e2b-83de-dca33b4638f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chat_format(conversations, is_input):\n",
    "    chat = []\n",
    "    for conversation in conversations:\n",
    "        if conversation[\"from\"] == \"human\":\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                conversation[\"value\"] = conversation[\"value\"].replace(f\"{option_letter}.\", f\"[{option_letter}] :\")\n",
    "            chat.append({\"role\": \"user\", \"content\": conversation[\"value\"]})\n",
    "        elif is_input == False and conversation[\"from\"] == \"gpt\":\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                conversation[\"value\"] = conversation[\"value\"].replace(f\"{option_letter}.\", f\"[{option_letter}].\")\n",
    "            chat.append({\"role\": \"assistant\", \"content\": conversation[\"value\"]})\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9370f45e-450a-4ed8-8eaa-858457e88b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Prepare data from UltraMedical for model to train\n",
    "\n",
    "# TODO: DataLoader\n",
    "\n",
    "first_N_data = data_list.select(range(0, 100))\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# conversations = [{'from': 'human', 'value': 'A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n\\nA. Ampicillin\\nB. Ceftriaxone\\nC. Doxycycline\\nD. Nitrofurantoin'}, {'from': 'gpt', 'value': \"The clinical scenario suggests that the pregnant woman is likely experiencing a urinary tract infection (UTI), given her symptoms of burning upon urination. During pregnancy, the risk of UTIs increases due to physiological changes, and it is important to treat them to prevent complications such as pyelonephritis or preterm labor.\\n\\nWhen considering antibiotic therapy for a UTI in a pregnant woman, it's crucial to choose an agent that is safe for both the mother and the fetus. Let's evaluate the options:\\n\\nA. Ampicillin - This is a penicillin antibiotic that is generally considered safe during pregnancy and can be used to treat UTIs. However, it may not be the first choice due to resistance patterns.\\n\\nB. Ceftriaxone - This is a cephalosporin antibiotic that is also generally safe in pregnancy and effective against a broad range of bacteria. It is typically used for more complicated infections or when other antibiotics are not suitable.\\n\\nC. Doxycycline - This is a tetracycline antibiotic that is contraindicated during pregnancy because it can affect fetal bone growth and discoloration of teeth.\\n\\nD. Nitrofurantoin - This antibiotic is commonly used to treat uncomplicated UTIs and is considered safe during most of the pregnancy. However, it is not recommended at the very end of pregnancy (after 38 weeks) due to the risk of hemolytic anemia in the newborn.\\n\\nGiven that the patient is at 22 weeks gestation and has an uncomplicated UTI, the best choice would be an antibiotic that is safe and commonly used for UTIs in pregnancy.\\n\\nSo, the answer is D. Nitrofurantoin.\"}]\n",
    "\n",
    "# TODO: convert training data to test format:\n",
    "# inputs: <|user|> <question>\\n\\n[A] : xxx\\n[B] : xxx\\n[C] : xxx\\n[D] : xxx\\n<|end|><|assistant|>\n",
    "# outputs: <inputs> <answer> ... the answer is [A].<|end|>\n",
    "\n",
    "\n",
    "def convert_to_tokenized_training_data(data):\n",
    "    #conversations_list = data[\"conversations\"] # [[{\"from\":\"human\", \"value\":xxx}, {\"from\":\"gpt\", \"value\":xxx}], [{\"from\":\"human\", \"value\":xxx}, {\"from\":\"gpt\", \"value\":xxx}], ...]\n",
    "\n",
    "    questions = convert_to_chat_format(data[\"conversations\"], is_input = True) #[convert_to_chat_format(conversations, is_input = True) for conversations in conversations_list]\n",
    "    template_questions = tokenizer.apply_chat_template(questions, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "    questions_and_answers = convert_to_chat_format(data[\"conversations\"], is_input = False) #[convert_to_chat_format(conversations, is_input = False) for conversations in conversations_list]\n",
    "    template_questions_and_answers = tokenizer.apply_chat_template(questions_and_answers, tokenize = False, add_generation_prompt = False)\n",
    "\n",
    "    # print(\"template_questions\", '\\n', template_questions, '\\n------')\n",
    "    # print(\"template_expected_outputs\", '\\n', template_questions_and_answers, '\\n-----')\n",
    "\n",
    "\n",
    "    tokenized_questions = tokenizer(template_questions, padding = False, truncation=True, max_length=MAX_LENGTH)\n",
    "    tokenized_inputs = tokenizer(template_questions_and_answers, padding=False, truncation=True, max_length=MAX_LENGTH)\n",
    "    # print(tokenized_inputs)\n",
    "    # tokenized_inputs[\"labels\"] = [ids[1:] + [tokenizer.pad_token_id] for ids in tokenized_inputs[\"input_ids\"]] # Shifted inputs as labels\n",
    "    # tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"][:]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    # print(\"tokenized_questions\", '\\n', tokenized_questions, '\\n------')\n",
    "    # print(\"tokenized_inputs\", '\\n', tokenized_inputs, '\\n-----')\n",
    "\n",
    "    questions_input_ids = tokenized_questions[\"input_ids\"]\n",
    "    all_content_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "    tokenized_labels = [-100] * len(questions_input_ids) + all_content_input_ids[len(questions_input_ids):]\n",
    "\n",
    "    # print(\"tokenized_labels\", '\\n', tokenized_labels, '\\n-----')\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_labels\n",
    "    \n",
    "    padding_num = MAX_LENGTH - len(tokenized_inputs[\"input_ids\"])\n",
    "    if padding_num > 0:\n",
    "        tokenized_inputs[\"input_ids\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"input_ids\"]\n",
    "        tokenized_inputs[\"attention_mask\"] = [0] * padding_num + tokenized_inputs[\"attention_mask\"]\n",
    "        tokenized_inputs[\"labels\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"labels\"]\n",
    "    \n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    # tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"][:]\n",
    "    # question_start_index = len(tokenized_questions[\"input_ids\"])\n",
    "    # tokenized_inputs[\"labels\"]\n",
    "    \n",
    "    # print(tokenized_inputs)\n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"attention_mask\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    tokenized_inputs = {k: torch.tensor(v) for k, v in tokenized_inputs.items()}\n",
    "\n",
    "    # del tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    # print(tokenized_inputs)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "    \n",
    "\n",
    "# print(first_N_data[0])\n",
    "# tokenized_training_data = convert_to_tokenized_training_data(first_N_data[0])\n",
    "tokenized_training_data = first_N_data.map(convert_to_tokenized_training_data, batched = False)\n",
    "\n",
    "print(\"done\")\n",
    "# print(tokenized_training_data)\n",
    "# print(\"---\")\n",
    "# print(tokenizer.decode(tokenized_training_data[0][\"input_ids\"], skip_special_tokens = False))\n",
    "# print(\"---\")\n",
    "# print(tokenizer.decode(tokenized_training_data[0][\"labels\"], skip_special_tokens = False)) # skip_special_tokens = False will show <|endoftext|><|user|><|end|><|assistant|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a2c131-54f1-4415-9586-977458f961a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 2.69 MiB is free. Process 1499667 has 75.44 GiB memory in use. Including non-PyTorch memory, this process has 3.64 GiB memory in use. Of the allocated memory 3.14 GiB is allocated by PyTorch, and 1.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Documents/fine_tuned_model_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     save_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# TODO, now don't save checkpoints #\"epoch\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# report_to = [\"tensorboard\"],  # Enable logging to TensorBoard\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenized_training_data\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/transformers/trainer.py:612\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    611\u001b[0m ):\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/transformers/trainer.py:899\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 899\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 2.69 MiB is free. Process 1499667 has 75.44 GiB memory in use. Including non-PyTorch memory, this process has 3.64 GiB memory in use. Of the allocated memory 3.14 GiB is allocated by PyTorch, and 1.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./Documents/fine_tuned_model_checkpoints\",\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    per_device_train_batch_size = 1, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 3, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-4, # As specified in the paper: 1e-4\n",
    "    # save_total_limit = 1, # TODO, now keep only the last checkpoint\n",
    "    fp16 = True, # TODO\n",
    "    logging_steps = 1,\n",
    "    logging_dir = \"./Documents/logs\",  # Directory for logs\n",
    "    # report_to = [\"tensorboard\"],  # Enable logging to TensorBoard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_training_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"/home/sheju347/Documents/fine_tuned_model/\", ignore_errors=True)\n",
    "print (\"previous model removed.\")\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"./Documents/fine_tuned_model\"\n",
    "\n",
    "model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d91a1d-fdce-490b-82e8-4c4ddccd08cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e4cce-32b2-4f27-bd61-8968f9dcf6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name = \"./code/Documents/fine_tuned_model\"\n",
    "model_name = \"./Documents/fine_tuned_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "print(model.name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b27255-2510-45ef-b3e8-a715e266c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, add_generation_prompt = False, tokenize = False, continue_final_message = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e255f-5ad4-4e93-ba67-2b0cb78eaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = ultraMedical[\"train\"][0][\"conversations\"]\n",
    "chat = convert_to_chat_format(conversations, is_input = True)\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bac48-6fb6-4d62-9690-07d2c17d88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test apply_chat_template // https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "print(model.name_or_path)\n",
    "\n",
    "def format_choices(choices):\n",
    "    a = zip(list(choices.keys()), choices.values())\n",
    "    final_answers = []\n",
    "    for x,y in a:\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)\n",
    "\n",
    "\n",
    "def run_inference(content, model, tokenizer, max_new_tokens, temperature):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "    # add_generation_prompt indicates the start of a response\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = \"pt\").to(device)\n",
    "    # print(\"inputs:\", tokenizer.apply_chat_template(messages, add_generation_prompt = True, tokenize = False))\n",
    "    outputs = model.generate(inputs, max_new_tokens = max_new_tokens, do_sample = True, temperature = temperature)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text\n",
    "\n",
    "prompt = f'''\n",
    "{{question}} \\n\n",
    "{{choices}}\n",
    "'''\n",
    "\n",
    "# Training data index 0 (GBaker/MedQA-USMLE-4-options)\n",
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\"\n",
    "choices = {\n",
    "\"A\": \"Ampicillin\",\n",
    "\"B\": \"Ceftriaxone\",\n",
    "\"C\": \"Doxycycline\",\n",
    "\"D\": \"Nitrofurantoin\"\n",
    "}\n",
    "\n",
    "# Training data index 1\n",
    "# question = \"A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\"\n",
    "# choices = {\n",
    "# \"A\": \"Placing the infant in a supine position on a firm mattress while sleeping\",\n",
    "# \"B\": \"Keeping the infant covered and maintaining a high room temperature\",\n",
    "# \"C\": \"Application of a device to maintain the sleeping position\",\n",
    "# \"D\": \"Avoiding pacifier use during sleep\"\n",
    "# }\n",
    "\n",
    "# Training data index 1\n",
    "# question = \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\"\n",
    "# choices = {\n",
    "# \"A\": \"Abnormal migration of ventral pancreatic bud\",\n",
    "# \"B\": \"Complete failure of proximal duodenum to recanalize\",\n",
    "# \"C\": \"Abnormal hypertrophy of the pylorus\",\n",
    "# \"D\": \"Failure of lateral body folds to move ventrally and fuse in the midline\"\n",
    "# }\n",
    "\n",
    "# Test data index 0\n",
    "# question = \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\"\n",
    "# choices = {\n",
    "# \"A\": \"Disclose the error to the patient and put it in the operative report\",\n",
    "# \"B\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
    "# \"C\": \"Report the physician to the ethics committee\",\n",
    "# \"D\": \"Refuse to dictate the operative report\"\n",
    "# }\n",
    "\n",
    "formated_choices = format_choices(choices)\n",
    "\n",
    "model_prompt = prompt.format(question = question, choices = formated_choices)\n",
    "\n",
    "# print(model_prompt)\n",
    "\n",
    "output_text = run_inference(model_prompt, model, tokenizer, max_new_tokens = 1024, temperature = 0.7)\n",
    "# output_text = output_text.split(\"<|assistant|>\")[-1]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14392f35-1d3e-4396-bcae-c59d3d709318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FOR TEST\")\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
    "# Choices:\n",
    "# A: Disclose the error to the patient but leave it out of the operative report\n",
    "# B: Disclose the error to the patient and put it in the operative report\n",
    "# C: Tell the attending that he cannot fail to disclose this mistake\n",
    "# D: Report the physician to the ethics committee\n",
    "# E: Refuse to dictate the operative report\n",
    "# Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\n",
    "# The answer is:\n",
    "# \"\"\"\n",
    "\n",
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n\\nA. Ampicillin\\nB. Ceftriaxone\\nC. Doxycycline\\nD. Nitrofurantoin\"\n",
    "# Respond only with letter A, B, C, D, E. Don't need other information.\n",
    "\n",
    "prompt = f\"Give a proper answer to the question:\\nquestion: {question}\\n\\nThe answer is: \"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "\n",
    "print (\"Prompt: \", prompt)\n",
    "print (\"--------------\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\", padding=False).to(device)\n",
    "with torch.no_grad(): # NO gradient calculation for inference\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 1024,\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "        # use_cache=False,\n",
    "        #do_sample = False\n",
    "    )\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "\n",
    "# print(len(answer))\n",
    "# print(\"original decode answer:\", answer)\n",
    "\n",
    "answer = answer.replace(prompt, '').strip()\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n",
    "print (\"--------------\")\n",
    "\n",
    "print (\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f0613-7da9-4d69-ba3c-ad57abc40ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on \"TsinghuaC3I/UltraMedical\" dataset\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"/home/sheju347/Documents/fine_tuned_model/\", ignore_errors=True)\n",
    "print (\"previous model removed.\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "ultraMedical = load_dataset(\"TsinghuaC3I/UltraMedical\")\n",
    "\n",
    "data_list = ultraMedical[\"train\"]\n",
    "print(len(data_list), type(data_list))\n",
    "\n",
    "# print((data_list[0:10]))\n",
    "first_N_data = data_list.select(range(0, 10000)) # TODO\n",
    "\n",
    "print(type(first_N_data))\n",
    "\n",
    "def preprocess_function(data):\n",
    "    print(\"data:\", data)\n",
    "    # input_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: \" for conversation in data[\"conversations\"]] # (context) + question + options\n",
    "    # target_text = [f\"{conversation[1]['value']}\" for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "    # # target_text = [f\"A\" for conversation in data[\"conversations\"]]  # CoT + final answer (TODO)\n",
    "    \n",
    "    input_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: \"\n",
    "                      for conversation in data['conversations']] # (context) + question + options\n",
    "    target_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: D. \\nExplanation: {conversation[1]['value']}<|end>\"\n",
    "                      for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "\n",
    "\n",
    "    # print(\"input:\", input_text)\n",
    "    print(\"target:\", target_text)\n",
    "    \n",
    "    # prompt = f\"Give a proper answer to the question:\\nquestion: {question}\\n\\nThe answer is: \"\n",
    "\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    targets = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    return inputs\n",
    "\n",
    "    # prompt = input_text\n",
    "    # data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # data['labels'] = tokenizer(target_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # return data\n",
    "\n",
    "tokenized_train = first_N_data.map(preprocess_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./Documents/fine_tuned_model_checkpoints\",\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 3, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-4, # As specified in the paper: 1e-4\n",
    "    # save_total_limit = 1, # TODO, now keep only the last checkpoint\n",
    "    fp16 = True, # TODO\n",
    "    logging_steps = 1,\n",
    "    logging_dir = \"./Documents/logs\",  # Directory for logs\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"./Documents/fine_tuned_model\"\n",
    "\n",
    "model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
