{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ee8565-cef9-4b4b-93be-6ce752d84090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /projects/sciences/computing/sheju347/.cache/huggingface\n",
      "HF_HUB_CACHE: /projects/sciences/computing/sheju347/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set env vars BEFORE importing huggingface modules\n",
    "os.environ[\"HF_HOME\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/projects/sciences/computing/sheju347/.cache/huggingface/hub\"\n",
    "\n",
    "# Now import huggingface modules\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(\"HF_HOME:\", constants.HF_HOME)\n",
    "print(\"HF_HUB_CACHE:\", constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92f390-b887-4b55-a5b1-7cc77e9e0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda init bash\n",
    "# !source ~/.bashrc\n",
    "# !conda --version\n",
    "!conda activate LLM\n",
    "# !conda env list\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85bb19-0445-4a9f-8577-0a08446240a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c78ebd-41ac-4cd4-af46-9aece9968bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9168249-4fe9-44dc-90e3-f4a02c6a7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheju347/.local/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947abbaa0e4a42ddb20d7b4ac91d5d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/fine_tuned_model_entire_UltraMedical_batch_4\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-instruct\"\n",
    "\n",
    "# model_name = \"KrithikV/MedMobile\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False, torch_dtype=torch.bfloat16)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "print(model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b45fe-ea95-4c26-9460-54926c7efc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch memory usage\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GiB\")\n",
    "print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GiB\")\n",
    "print(f\"Max Reserved: {torch.cuda.max_memory_reserved() / 1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b763f0-2cb4-4983-b0ad-f09ea01e85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer)\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, add_generation_prompt = True, tokenize=False))\n",
    "inputs = tokenizer.apply_chat_template(chat, add_generation_prompt = True, tokenize = True, return_tensors = \"pt\").to(device)\n",
    "print(\"inputs:\", inputs)\n",
    "outputs = model.generate(inputs, max_new_tokens = 512, do_sample = True, temperature = 0.7)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(\"outputs:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcc7614-824b-43f6-ac6b-3d15da64ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# chat = [\n",
    "#     {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "# ]\n",
    "# print(tokenizer.apply_chat_template(chat, add_generation_prompt = False, tokenize = False, continue_final_message = True))\n",
    "\n",
    "ultraMedical = load_dataset(\"TsinghuaC3I/UltraMedical\")\n",
    "\n",
    "data_list = ultraMedical[\"train\"]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87f6f15-0da8-4e2b-83de-dca33b4638f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chat_format(conversations, is_input):\n",
    "    chat = []\n",
    "    for conversation in conversations:\n",
    "        if conversation[\"from\"] == \"human\":\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                conversation[\"value\"] = conversation[\"value\"].replace(f\"{option_letter}.\", f\"[{option_letter}] :\")\n",
    "            chat.append({\"role\": \"user\", \"content\": conversation[\"value\"]})\n",
    "        elif is_input == False and conversation[\"from\"] == \"gpt\":\n",
    "            for option_letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                conversation[\"value\"] = conversation[\"value\"].replace(f\"{option_letter}.\", f\"[{option_letter}].\")\n",
    "            chat.append({\"role\": \"assistant\", \"content\": conversation[\"value\"]})\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9370f45e-450a-4ed8-8eaa-858457e88b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Prepare data from UltraMedical for model to train\n",
    "\n",
    "# TODO: DataLoader\n",
    "\n",
    "# first_N_data = data_list.select(range(0, 100))\n",
    "first_N_data = data_list\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# conversations = [{'from': 'human', 'value': 'A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n\\nA. Ampicillin\\nB. Ceftriaxone\\nC. Doxycycline\\nD. Nitrofurantoin'}, {'from': 'gpt', 'value': \"The clinical scenario suggests that the pregnant woman is likely experiencing a urinary tract infection (UTI), given her symptoms of burning upon urination. During pregnancy, the risk of UTIs increases due to physiological changes, and it is important to treat them to prevent complications such as pyelonephritis or preterm labor.\\n\\nWhen considering antibiotic therapy for a UTI in a pregnant woman, it's crucial to choose an agent that is safe for both the mother and the fetus. Let's evaluate the options:\\n\\nA. Ampicillin - This is a penicillin antibiotic that is generally considered safe during pregnancy and can be used to treat UTIs. However, it may not be the first choice due to resistance patterns.\\n\\nB. Ceftriaxone - This is a cephalosporin antibiotic that is also generally safe in pregnancy and effective against a broad range of bacteria. It is typically used for more complicated infections or when other antibiotics are not suitable.\\n\\nC. Doxycycline - This is a tetracycline antibiotic that is contraindicated during pregnancy because it can affect fetal bone growth and discoloration of teeth.\\n\\nD. Nitrofurantoin - This antibiotic is commonly used to treat uncomplicated UTIs and is considered safe during most of the pregnancy. However, it is not recommended at the very end of pregnancy (after 38 weeks) due to the risk of hemolytic anemia in the newborn.\\n\\nGiven that the patient is at 22 weeks gestation and has an uncomplicated UTI, the best choice would be an antibiotic that is safe and commonly used for UTIs in pregnancy.\\n\\nSo, the answer is D. Nitrofurantoin.\"}]\n",
    "\n",
    "# TODO: convert training data to test format:\n",
    "# inputs: <|user|> <question>\\n\\n[A] : xxx\\n[B] : xxx\\n[C] : xxx\\n[D] : xxx\\n<|end|><|assistant|>\n",
    "# outputs: <inputs> <answer> ... the answer is [A].<|end|>\n",
    "\n",
    "\n",
    "def convert_to_tokenized_training_data(data):\n",
    "    #conversations_list = data[\"conversations\"] # [[{\"from\":\"human\", \"value\":xxx}, {\"from\":\"gpt\", \"value\":xxx}], [{\"from\":\"human\", \"value\":xxx}, {\"from\":\"gpt\", \"value\":xxx}], ...]\n",
    "\n",
    "    questions = convert_to_chat_format(data[\"conversations\"], is_input = True) #[convert_to_chat_format(conversations, is_input = True) for conversations in conversations_list]\n",
    "    template_questions = tokenizer.apply_chat_template(questions, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "    questions_and_answers = convert_to_chat_format(data[\"conversations\"], is_input = False) #[convert_to_chat_format(conversations, is_input = False) for conversations in conversations_list]\n",
    "    template_questions_and_answers = tokenizer.apply_chat_template(questions_and_answers, tokenize = False, add_generation_prompt = False)\n",
    "\n",
    "    # print(\"template_questions\", '\\n', template_questions, '\\n------')\n",
    "    # print(\"template_expected_outputs\", '\\n', template_questions_and_answers, '\\n-----')\n",
    "\n",
    "\n",
    "    tokenized_questions = tokenizer(template_questions, padding = False, truncation=True, max_length=MAX_LENGTH)\n",
    "    tokenized_inputs = tokenizer(template_questions_and_answers, padding=False, truncation=True, max_length=MAX_LENGTH)\n",
    "    # print(tokenized_inputs)\n",
    "    # tokenized_inputs[\"labels\"] = [ids[1:] + [tokenizer.pad_token_id] for ids in tokenized_inputs[\"input_ids\"]] # Shifted inputs as labels\n",
    "    # tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"][:]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    # print(\"tokenized_questions\", '\\n', tokenized_questions, '\\n------')\n",
    "    # print(\"tokenized_inputs\", '\\n', tokenized_inputs, '\\n-----')\n",
    "\n",
    "    questions_input_ids = tokenized_questions[\"input_ids\"]\n",
    "    all_content_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "    tokenized_labels = [-100] * len(questions_input_ids) + all_content_input_ids[len(questions_input_ids):]\n",
    "\n",
    "    # print(\"tokenized_labels\", '\\n', tokenized_labels, '\\n-----')\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_labels\n",
    "    \n",
    "    padding_num = MAX_LENGTH - len(tokenized_inputs[\"input_ids\"])\n",
    "    if padding_num > 0:\n",
    "        tokenized_inputs[\"input_ids\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"input_ids\"]\n",
    "        tokenized_inputs[\"attention_mask\"] = [0] * padding_num + tokenized_inputs[\"attention_mask\"]\n",
    "        tokenized_inputs[\"labels\"] = [tokenizer.eos_token_id] * padding_num + tokenized_inputs[\"labels\"]\n",
    "    \n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    # tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"][:]\n",
    "    # question_start_index = len(tokenized_questions[\"input_ids\"])\n",
    "    # tokenized_inputs[\"labels\"]\n",
    "    \n",
    "    # print(tokenized_inputs)\n",
    "    # print(len(tokenized_inputs[\"input_ids\"]), len(tokenized_inputs[\"attention_mask\"]), len(tokenized_inputs[\"labels\"]))\n",
    "\n",
    "    tokenized_inputs = {k: torch.tensor(v) for k, v in tokenized_inputs.items()}\n",
    "\n",
    "    # del tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    # print(tokenized_inputs)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "    \n",
    "\n",
    "# print(first_N_data[0])\n",
    "# tokenized_training_data = convert_to_tokenized_training_data(first_N_data[0])\n",
    "tokenized_training_data = first_N_data.map(convert_to_tokenized_training_data, batched = False)\n",
    "\n",
    "print(\"done\")\n",
    "# print(tokenized_training_data)\n",
    "# print(\"---\")\n",
    "# print(tokenizer.decode(tokenized_training_data[6][\"input_ids\"], skip_special_tokens = False))\n",
    "# print(\"---\")\n",
    "# print(tokenizer.decode(tokenized_training_data[0][\"labels\"], skip_special_tokens = False)) # skip_special_tokens = False will show <|endoftext|><|user|><|end|><|assistant|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2c131-54f1-4415-9586-977458f961a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27815' max='102399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 27815/102399 3:29:31 < 9:21:52, 2.21 it/s, Epoch 0.27/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "output_dir = \"/projects/sciences/computing/sheju347/MedicalQA/train/saved_models/base/10-14-UltraMedical-1-more-epoch\"\n",
    "\n",
    "is_bf16_supported = torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 1, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-5, # original model used 1e-4 # As specified in the paper: 1e-4\n",
    "    fp16 = torch.cuda.is_available() and not is_bf16_supported,\n",
    "    bf16 = is_bf16_supported, # Sometimes RuntimeError: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'\n",
    "    # save_total_limit = 1, # TODO, now keep only the last checkpoint\n",
    "    logging_steps = 1000,\n",
    "    logging_dir = output_dir,  # Directory for logs\n",
    "    # report_to = [\"tensorboard\"],  # Enable logging to TensorBoard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_training_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(\"/home/sheju347/Documents/fine_tuned_model/\", ignore_errors=True)\n",
    "# print (\"previous model removed.\")\n",
    "\n",
    "# Save the final model\n",
    "\n",
    "model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d91a1d-fdce-490b-82e8-4c4ddccd08cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e4cce-32b2-4f27-bd61-8968f9dcf6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name = \"./code/Documents/fine_tuned_model\"\n",
    "model_name = \"./Documents/fine_tuned_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "print(model.dtype)\n",
    "print(model.name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b27255-2510-45ef-b3e8-a715e266c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, add_generation_prompt = False, tokenize = False, continue_final_message = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e255f-5ad4-4e93-ba67-2b0cb78eaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = ultraMedical[\"train\"][0][\"conversations\"]\n",
    "chat = convert_to_chat_format(conversations, is_input = True)\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bac48-6fb6-4d62-9690-07d2c17d88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test apply_chat_template // https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "print(model.name_or_path)\n",
    "\n",
    "def format_choices(choices):\n",
    "    a = zip(list(choices.keys()), choices.values())\n",
    "    final_answers = []\n",
    "    for x,y in a:\n",
    "        final_answers.append(f'[{x}] : {y}')\n",
    "    return \"\\n\".join(final_answers)\n",
    "\n",
    "\n",
    "def run_inference(content, model, tokenizer, max_new_tokens, temperature):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "    # add_generation_prompt indicates the start of a response\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = \"pt\").to(device)\n",
    "    # print(\"inputs:\", tokenizer.apply_chat_template(messages, add_generation_prompt = True, tokenize = False))\n",
    "    outputs = model.generate(inputs, max_new_tokens = max_new_tokens, do_sample = True, temperature = temperature)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text\n",
    "\n",
    "prompt = f'''\n",
    "{{question}} \\n\n",
    "{{choices}}\n",
    "'''\n",
    "\n",
    "# Training data index 0 (GBaker/MedQA-USMLE-4-options)\n",
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\"\n",
    "choices = {\n",
    "\"A\": \"Ampicillin\",\n",
    "\"B\": \"Ceftriaxone\",\n",
    "\"C\": \"Doxycycline\",\n",
    "\"D\": \"Nitrofurantoin\"\n",
    "}\n",
    "\n",
    "# Training data index 1\n",
    "# question = \"A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\"\n",
    "# choices = {\n",
    "# \"A\": \"Placing the infant in a supine position on a firm mattress while sleeping\",\n",
    "# \"B\": \"Keeping the infant covered and maintaining a high room temperature\",\n",
    "# \"C\": \"Application of a device to maintain the sleeping position\",\n",
    "# \"D\": \"Avoiding pacifier use during sleep\"\n",
    "# }\n",
    "\n",
    "# Training data index 1\n",
    "# question = \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\"\n",
    "# choices = {\n",
    "# \"A\": \"Abnormal migration of ventral pancreatic bud\",\n",
    "# \"B\": \"Complete failure of proximal duodenum to recanalize\",\n",
    "# \"C\": \"Abnormal hypertrophy of the pylorus\",\n",
    "# \"D\": \"Failure of lateral body folds to move ventrally and fuse in the midline\"\n",
    "# }\n",
    "\n",
    "# Test data index 0\n",
    "# question = \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\"\n",
    "# choices = {\n",
    "# \"A\": \"Disclose the error to the patient and put it in the operative report\",\n",
    "# \"B\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
    "# \"C\": \"Report the physician to the ethics committee\",\n",
    "# \"D\": \"Refuse to dictate the operative report\"\n",
    "# }\n",
    "\n",
    "formated_choices = format_choices(choices)\n",
    "\n",
    "model_prompt = prompt.format(question = question, choices = formated_choices)\n",
    "\n",
    "# print(model_prompt)\n",
    "\n",
    "output_text = run_inference(model_prompt, model, tokenizer, max_new_tokens = 1024, temperature = 0.7)\n",
    "# output_text = output_text.split(\"<|assistant|>\")[-1]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14392f35-1d3e-4396-bcae-c59d3d709318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FOR TEST\")\n",
    "\n",
    "print (\"model: \", model.name_or_path)\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
    "# Choices:\n",
    "# A: Disclose the error to the patient but leave it out of the operative report\n",
    "# B: Disclose the error to the patient and put it in the operative report\n",
    "# C: Tell the attending that he cannot fail to disclose this mistake\n",
    "# D: Report the physician to the ethics committee\n",
    "# E: Refuse to dictate the operative report\n",
    "# Given five answer candidates, A, B, C, D, and E, choose the best answer choice.\n",
    "# The answer is:\n",
    "# \"\"\"\n",
    "\n",
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n\\nA. Ampicillin\\nB. Ceftriaxone\\nC. Doxycycline\\nD. Nitrofurantoin\"\n",
    "# Respond only with letter A, B, C, D, E. Don't need other information.\n",
    "\n",
    "prompt = f\"Give a proper answer to the question:\\nquestion: {question}\\n\\nThe answer is: \"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "\n",
    "print (\"Prompt: \", prompt)\n",
    "print (\"--------------\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\", padding=False).to(device)\n",
    "with torch.no_grad(): # NO gradient calculation for inference\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 1024,\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
    "        # use_cache=False,\n",
    "        #do_sample = False\n",
    "    )\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "\n",
    "# print(len(answer))\n",
    "# print(\"original decode answer:\", answer)\n",
    "\n",
    "answer = answer.replace(prompt, '').strip()\n",
    "\n",
    "finish_time = time.time()\n",
    "elapse_time = finish_time - start_time\n",
    "print(\"elapse_time: \", elapse_time)\n",
    "print (\"--------------\")\n",
    "\n",
    "print (\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f0613-7da9-4d69-ba3c-ad57abc40ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on \"TsinghuaC3I/UltraMedical\" dataset\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"/home/sheju347/Documents/fine_tuned_model/\", ignore_errors=True)\n",
    "print (\"previous model removed.\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "ultraMedical = load_dataset(\"TsinghuaC3I/UltraMedical\")\n",
    "\n",
    "data_list = ultraMedical[\"train\"]\n",
    "print(len(data_list), type(data_list))\n",
    "\n",
    "# print((data_list[0:10]))\n",
    "first_N_data = data_list.select(range(0, 10000)) # TODO\n",
    "\n",
    "print(type(first_N_data))\n",
    "\n",
    "def preprocess_function(data):\n",
    "    print(\"data:\", data)\n",
    "    # input_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: \" for conversation in data[\"conversations\"]] # (context) + question + options\n",
    "    # target_text = [f\"{conversation[1]['value']}\" for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "    # # target_text = [f\"A\" for conversation in data[\"conversations\"]]  # CoT + final answer (TODO)\n",
    "    \n",
    "    input_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: \"\n",
    "                      for conversation in data['conversations']] # (context) + question + options\n",
    "    target_text = [f\"Give a proper answer to the question:\\nquestion: {conversation[0]['value']}\\n\\nThe answer is: D. \\nExplanation: {conversation[1]['value']}<|end>\"\n",
    "                      for conversation in data[\"conversations\"]]  # CoT + final answer\n",
    "\n",
    "\n",
    "    # print(\"input:\", input_text)\n",
    "    print(\"target:\", target_text)\n",
    "    \n",
    "    # prompt = f\"Give a proper answer to the question:\\nquestion: {question}\\n\\nThe answer is: \"\n",
    "\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    targets = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    return inputs\n",
    "\n",
    "    # prompt = input_text\n",
    "    # data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # data['labels'] = tokenizer(target_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # return data\n",
    "\n",
    "tokenized_train = first_N_data.map(preprocess_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./Documents/fine_tuned_model_checkpoints\",\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    num_train_epochs = 3, # As specified in the paper: 3 epochs\n",
    "    learning_rate = 1e-4, # As specified in the paper: 1e-4\n",
    "    # save_total_limit = 1, # TODO, now keep only the last checkpoint\n",
    "    fp16 = True, # TODO\n",
    "    logging_steps = 1,\n",
    "    logging_dir = \"./Documents/logs\",  # Directory for logs\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"./Documents/fine_tuned_model\"\n",
    "\n",
    "model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
ython",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
ython",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
