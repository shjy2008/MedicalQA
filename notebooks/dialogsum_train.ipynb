{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040218dd-63b3-4560-8b31-0ef532ec095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c6423-77ad-4cdd-98c2-78290ac9dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fd5f94-f4c2-4670-a124-fbb526316754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b796cb-773f-4587-b61c-7f61ab5aeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "# import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6792b3-0505-4e65-8a99-add9daa3406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "cuda available: True\n",
      "device: cuda\n",
      "Qwen/Qwen2.5-0.5B-instruct\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False, torch_dtype=torch.bfloat16)\n",
    "\n",
    "print(original_model.config.torch_dtype)\n",
    "print(original_model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(original_model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413facc-9c62-4d73-ba6b-b82f74ec2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b14017-7186-4bbd-b96c-9c6e666e1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2c639-bd64-4cdc-ae8d-773cca58a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"hello there\", padding = \"max_length\", truncation = True, max_length = 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaeee9-4fc1-485e-bacf-a5fc3b912015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59d2332a-625c-48bf-a041-1e089ed60f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "Person#'1 suggests upgrade's because #2s not sure what to in.Person#'2 recommends buy CD drive #1 because #2 need it\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7e4c1-8308-4120-a6ef-e181af1970dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "868c19b2-8af9-46d2-a576-360c6d9b1fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 981.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "\n",
    "    # If use flan-t5, works fine. But if use Qwen0.5B, out of memory when training\n",
    "    # example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # example.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "    # del example[\"id\"]\n",
    "    # del example[\"topic\"]\n",
    "    # del example[\"dialogue\"]\n",
    "    # del example[\"summary\"]\n",
    "    # print(\"example:\", prompt)\n",
    "    # print(\"---\")\n",
    "    \n",
    "    # return example\n",
    "\n",
    "    # TODO the following code is probably wrong!!! even flan-t5 instructed model behaves bad\n",
    "    # TODO: for decoder-only model, labels should be inputs+labels\n",
    "    is_decoder_model = True\n",
    "    if is_decoder_model:\n",
    "        # print(\"examplesummary\", example[\"dialogue\"])\n",
    "        input_texts = []\n",
    "        for i in range(len(prompt)):\n",
    "            summary = example[\"summary\"][i]\n",
    "            input_text = prompt[i] + summary            \n",
    "            input_texts.append(input_text)\n",
    "        # prompt = prompt + example[\"summary\"]\n",
    "        \n",
    "        inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=1024)\n",
    "        \n",
    "        # input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        # attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "        # Create labels: the labels are the same as input, but shifted\n",
    "        # labels = input_ids.clone()\n",
    "        # # Shift labels by one token (this is how language models are trained)\n",
    "        # labels[:-1] = input_ids[1:]\n",
    "\n",
    "        \n",
    "        inputs['labels'] = [ids[1:] + [tokenizer.pad_token_id] for ids in inputs['input_ids']] # labels are shifted version of inputs\n",
    "\n",
    "        inputs = {k: torch.tensor(v) for k, v in inputs.items()} # convert to pyTorch tensor \n",
    "\n",
    "        # inputs: {\"input_ids\": tensor([[xxx, xxx, ...], [xxx, xxx, ...], ...]), \n",
    "        #           \"attention_mask\": tensor([[1, 1, 1, ..., 0, 0], [1, 1, 1, ...0, 0], ...]),\n",
    "        #           \"labels\": tensor([[xxx, xxx, ...], [xxx, xxx, ...], ...])}\n",
    "        # print(inputs)\n",
    "\n",
    "        return inputs\n",
    "    else:\n",
    "        labels = example[\"summary\"]\n",
    "    \n",
    "        # try to use this method to train flan-t5(see if produce irrelevant results), and test Qwen0.5B(not out of memory, see if produce irrelevant results)\n",
    "        inputs = tokenizer(prompt, padding=True, truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "        # print(inputs)\n",
    "        targets = tokenizer(labels, padding=True, truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]  # Set the target tokens as the 'labels'\n",
    "        del inputs[\"attention_mask\"]\n",
    "\n",
    "        # inputs: {\"input_ids\":tensor([[xxx, xxx, ...], [xxx, xxx, ...], ...]), \"labels\":tensor([[xxx, xxx, ...], [xxx, xxx, ...], ...])}\n",
    "        return inputs\n",
    "\n",
    "    # print(inputs)\n",
    "\n",
    "    # print(\"hahaha\", labels)\n",
    "    \n",
    "    # # TODO the following code is probably wrong!!! even flan-t5 instructed model behaves bad\n",
    "    # # TODO: for decoder-only model, labels should be inputs+labels\n",
    "    # is_decoder_model = False\n",
    "    # if is_decoder_model:\n",
    "    #     labels = []\n",
    "    #     for i in range(len(prompt)):\n",
    "    #         summary = example[\"summary\"][i]\n",
    "    #         label = prompt[i] + summary\n",
    "    #         labels.append(label)\n",
    "    # else:\n",
    "    #     labels = example[\"summary\"]\n",
    "    \n",
    "    # # try to use this method to train flan-t5(see if produce irrelevant results), and test Qwen0.5B(not out of memory, see if produce irrelevant results)\n",
    "    # inputs = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "    # targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "    # inputs[\"labels\"] = targets[\"input_ids\"]  # Set the target tokens as the 'labels'\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "print(\"aa\")\n",
    "first_N_data = dataset[\"train\"].select(range(0, 50)) # TODO\n",
    "# print(\"first_N_data\", first_N_data[0])\n",
    "tokenized_datasets = first_N_data.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75101d56-42e0-4d71-93aa-dc1e6209876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b49c55-7da8-4e0e-9d0c-aefb74758e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881db00-a5b5-49bd-82b5-b3c0cf5c2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79381446-5be9-4452-8a3e-d14bc97079fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ERROR: ld.so: object '/usr/lib64/libstdc++.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    }
   ],
   "source": [
    "# output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "output_dir = f'./dialogue-summary-training'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    save_strategy = \"no\", # TODO, now don't save checkpoints #\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    # weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size = 4, # As specified in the paper: batch_size: 32\n",
    "    # max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets#['train'],\n",
    "    # eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f636c3f-4750-4eff-89cf-1805c07944a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.664600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.785700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.026200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.9716713917561066, metrics={'train_runtime': 2.8781, 'train_samples_per_second': 52.118, 'train_steps_per_second': 13.551, 'total_flos': 144626627059200.0, 'train_loss': 1.9716713917561066, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb0c9da6-adc8-4479-9f1a-6c2fdab8c6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dialogue-summary-training/tokenizer_config.json',\n",
       " './dialogue-summary-training/special_tokens_map.json',\n",
       " './dialogue-summary-training/vocab.json',\n",
       " './dialogue-summary-training/merges.txt',\n",
       " './dialogue-summary-training/added_tokens.json',\n",
       " './dialogue-summary-training/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "original_model.to(torch.bfloat16)  # Convert to bfloat16\n",
    "original_model.save_pretrained(output_dir, \n",
    "                        # save_function=torch.save,  # Use standard PyTorch save\n",
    "                        # state_dict=model.state_dict(),  # Only save the model weights\n",
    "                        # safe_serialization=True,  # More efficient serializationsave_optimizer_state=False\n",
    "                     )\n",
    "tokenizer.save_pretrained(output_dir,\n",
    "                         # legacy_format=False  # Use newer, more efficient format\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f6aae-7dee-4104-8f9e-2a533a7819d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('./dialogue-summary-training', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58357b62-7bc4-4ae3-8902-104be918312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForCausalLM.from_pretrained('./dialogue-summary-training', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163bd77-101b-495b-9a54-a4ab1e6cfb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './dialogue-summary-training/checkpoint-500/'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = False)\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = False)\n",
    "\n",
    "print(instruct_model.config.torch_dtype)\n",
    "print(instruct_model.dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "instruct_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e339a26-aeb5-419c-9e56-ee2db326d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_model.name_or_path)\n",
    "print(instruct_model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6085ae7a-0b0f-44f2-bda5-fa6783aeeb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "Qwen/Qwen2.5-0.5B-instruct\n",
      "./dialogue-summary-training\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "Person#1 suggestsPerson#2 a to do by by a.Person#2 agrees #1 wants do.Person#1 suggestsPerson#2 a to by a.Person#2 agrees #1 wants do.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "Person#1 suggestsPerson#2 a to do by way of.Person#2 agrees #1s.\n"
     ]
    }
   ],
   "source": [
    "# index = 200\n",
    "# dialogue = dataset['test'][index]['dialogue']\n",
    "# human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# data = dataset['train'][3]\n",
    "data = dataset[\"test\"][200]\n",
    "dialogue = data['dialogue']\n",
    "human_baseline_summary = data['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "original_model.to(device)\n",
    "instruct_model.to(device)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1), \n",
    "        attention_mask=inputs[\"attention_mask\"],  # Use attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad token ID\n",
    "        )\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(original_model.name_or_path)\n",
    "print(instruct_model.name_or_path)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe620c4-7ab8-4acf-af72-1821cd584189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290947a2-497d-4495-9e33-b372319e8e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
